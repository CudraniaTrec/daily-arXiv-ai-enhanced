<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 58]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Fair intersection of seekable iterators](https://arxiv.org/abs/2510.26016)
*Michael Arntzenius*

Main category: cs.PL

TL;DR: 本文揭示了miniKanren公平搜索策略的实质，即对单个分支工作量进行限定。该机制不仅提高了搜索效率，还可用于以可寻址迭代器实现最坏情况最优连接算法，为函数式语言的关系型计算提供了理论与方法支持。


<details>
  <summary>Details</summary>
Motivation: miniKanren相较于Prolog的主要语义进步在于其实现了完整且高效的搜索策略，能够公平地在不同分支之间交替执行。本文旨在深入探究miniKanren的公平性机制，并将其思想应用于其他领域。

Method: 作者采用了理论分析的方法，提出通过“限定工作量”来实现各分支之间的公平性，并将该思想扩展到使用可寻址迭代器接口实现最坏情况下最优连接操作，适用于在函数式语言中的浅层嵌入。

Result: 证明了通过限定各分支工作量的方式，不仅能保证搜索的公平性，还能优雅且具备可组合性地实现最坏情况最优的连接算法。该方案适用于函数式语言环境。

Conclusion: miniKanren的公平搜索策略核心在于限定单个分支的工作量。这一思想可推广，支持在函数式语言中以可寻址迭代器优雅实现最优连接，具备理论与实际价值。

Abstract: miniKanren's key semantic advance over Prolog is to implement a complete yet
efficient search strategy, fairly interleaving execution between disjuncts.
This fairness is accomplished by bounding how much work is done exploring one
disjunct before switching to the next. We show that the same idea -- fairness
via bounded work -- underlies an elegant compositional approach to implementing
worst-case optimal joins using a seekable iterator interface, suitable for
shallow embedding in functional languages.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Internal Vulnerabilities, External Threats: A Grounded Framework for Enterprise Open Source Risk Governance](https://arxiv.org/abs/2510.25882)
*Wenhao Yang,Minghui Zhou,Daniel Izquierdo Cortázar,Yehui Wang*

Main category: cs.SE

TL;DR: 本论文针对企业开源软件战略深度集成面临的系统性风险（如静默修复、社区冲突、许可证变更）提出技术之外的多维风险治理框架，包含目标清晰化、威胁与漏洞体系分类、缓解能力映射，并经真实案例专家验证，为企业提供构建主动防御与免疫力的系统性工具。


<details>
  <summary>Details</summary>
Motivation: 企业在采用开源软件时，风险管理往往局限于技术层面，无法应对如上游“静默修复”、社区冲突、突然的许可证变更这类系统性威胁，导致治理存在盲区。论文试图解决这种治理真空，推动企业风险管理向全面治理转变。

Method: 通过扎根理论研究，采访15名实践者，开发了一个综合的风险治理框架，并邀请行业专家进行案例回顾检验。

Result: 建立了以“目标-威胁-漏洞-缓解”（OTVM）为逻辑链的决策模型，提出了战略目标矩阵、外部威胁与内部漏洞的双重分类体系，以及能力建设与漏洞映射的实践缓解框架。相关工具在实际案例中被行业专家验证有效。

Conclusion: 论文为企业提供了从被动应对转向主动建设风险治理体系的系统性路径，有助于企业建立更强的组织“免疫系统”，提升面对开源风险的坚韧性。

Abstract: Enterprise engagement with open source has evolved from tactical adoption to
strategic deep integration, exposing them to a complex risk landscape far
beyond mere code. However, traditional risk management, narrowly focused on
technical tools, is structurally inadequate for systemic threats like upstream
"silent fixes", community conflicts, or sudden license changes, creating a
dangerous governance blind spot. To address this governance vacuum and enable
the necessary shift from tactical risk management to holistic risk governance,
we conducted a grounded theory study with 15 practitioners to develop a
holistic risk governance framework. Our study formalizes an analytical
framework built on a foundational risk principle: an uncontrollable External
Threat (e.g., a sudden license change in a key dependency) only becomes a
critical risk when it exploits a controllable Internal Vulnerability (e.g., an
undefined risk appetite for single-vendor projects), which then amplifies the
impact.The framework operationalizes this principle through a clear logical
chain: "Objectives -> Threats -> Vulnerabilities -> Mitigation" (OTVM). This
provides a holistic decision model that transcends mere technical checklists.
Based on this logic, our contributions are: (1) a "Strategic Objectives Matrix"
to clarify goals; (2) a systematic dual taxonomy of External Threats (Ex-Tech,
Ex-Comm, Ex-Eco) and Internal Vulnerabilities (In-Strat, In-Ops, In-Tech); and
(3) an actionable mitigation framework mapping capability-building to these
vulnerabilities. The framework's analytical utility was validated by three
industry experts through retrospective case studies on real-world incidents.
This work provides a novel diagnostic lens and a systematic path for
enterprises to shift from reactive "firefighting" to proactively building an
organizational "immune system".

</details>


### [3] [PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints](https://arxiv.org/abs/2510.25890)
*Tong Ma,Hui Lai,Hui Wang,Zhenhu Tian,Jizhou Wang,Haichao Wu,Yongfan Gao,Chaochao Li,Fengjie Xu,Ling Fang*

Main category: cs.SE

TL;DR: PRISM结合大语言模型与模型驱动工程，自动生成合规、安全且可机检工件，显著减少人工修订，助力自动化合规开发。


<details>
  <summary>Details</summary>
Motivation: 在安全和合规性至关重要的领域，对于生成能够被监管机构接受且可机检的工件和证据存在高度需求。现有的大语言模型在此类任务上面临异构数据结构、复杂约束和合规性的挑战，因此亟需一种统一且自动化的解决方案。

Method: PRISM将大语言模型与模型驱动工程结合，集成三个核心支柱：（1）统一元模型（UMM）用于整合异构结构和法规文本到单一语义空间；（2）集成约束模型（ICM）将结构和语义需求编译为生成时自动机及生成后验证器；（3）约束引导式可验证生成（CVG），通过双层约束（结构约束和语义/逻辑验证）确保自动化生成工件合规并可审计。发生违规时，支持自动修复和生成追溯。

Result: 在汽车软件工程（AUTOSAR）和跨国法律管辖（Brussels I bis）领域评估PRISM，能够生成结构有效、可审计、与现有工具链兼容的工件，显著减少人工修订工作量，为自动化合规工件生成提供了切实路径。

Conclusion: PRISM实现了合规和安全关键领域工件的自动化生成和验证，提升了效率与可靠性，为未来监管就绪自动化系统开发奠定基础。

Abstract: PRISM unifies Large Language Models with Model-Driven Engineering to generate
regulator-ready artifacts and machine-checkable evidence for safety- and
compliance-critical domains. PRISM integrates three pillars: a Unified
Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a
single semantic space; an Integrated Constraint Model (ICM) compiles structural
and semantic requirements into enforcement artifacts including generation-time
automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and
Constraint-Guided Verifiable Generation (CVG) applies these through two-layer
enforcement - structural constraints drive prefix-safe decoding while
semantic/logical validation produces machine-checkable certificates. When
violations occur, PRISM performs audit-guided repair and records generation
traces for compliance review. We evaluate PRISM in automotive software
engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis).
PRISM produces structurally valid, auditable artifacts that integrate with
existing tooling and substantially reduce manual remediation effort, providing
a practical path toward automated artifact generation with built-in assurance.

</details>


### [4] [A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows](https://arxiv.org/abs/2510.25935)
*Antía Dorado,Iván Folgueira,Sofía Martín,Gonzalo Martín,Álvaro Porto,Alejandro Ramos,John Wallace*

Main category: cs.SE

TL;DR: CodeSight通过整合GitHub数据处理、流程挖掘与LSTM时间序列预测，实现对PR解决时间的高效预测，从而增强软件项目管理的预防和决策能力。


<details>
  <summary>Details</summary>
Motivation: 提高软件开发流程对截止日期达成情况的预判能力，帮助项目管理提前识别潜在风险。

Method: 系统收集GitHub上的开发和部署数据，转化为流程挖掘日志，分析Pull Request（PR）活动，并利用LSTM模型预测PR剩余解决时间。

Result: 系统在预测截止日期达成方面表现出高精度和高F1分数，证明了方法的有效性。

Conclusion: 将流程挖掘与机器学习结合，能够有效提升软件项目管理的主动性和预测能力。

Abstract: CodeSight is an end-to-end system designed to anticipate deadline compliance
in software development workflows. It captures development and deployment data
directly from GitHub, transforming it into process mining logs for detailed
analysis. From these logs, the system generates metrics and dashboards that
provide actionable insights into PR activity patterns and workflow efficiency.
Building on this structured representation, CodeSight employs an LSTM model
that predicts remaining PR resolution times based on sequential activity traces
and static features, enabling early identification of potential deadline
breaches. In tests, the system demonstrates high precision and F1 scores in
predicting deadline compliance, illustrating the value of integrating process
mining with machine learning for proactive software project management.

</details>


### [5] [Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation](https://arxiv.org/abs/2510.26130)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 本研究提出并分析了用于评估LLM在类级代码生成能力的新基准。结果显示，LLM在真实类任务上的正确率远低于合成测试，文档和检索增强虽能改善表现但作用有限。主要错误类型揭示模型实际应用中的薄弱环节，为未来工具优化提供针对性建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在函数级代码生成方面已取得进展，但其在真实软件项目中生成正确的类级实现能力尚不清楚。本研究的动机是通过建立来自开源仓库的真实类数据集，评估LLM在实际条件下的泛化能力。

Method: 提出了一个新型基准测试，包括拆分为已见和未见数据的真实世界类，用以评估LLM在不同输入规范、检索增强配置和文档完整度下的表现。通过多个LLM的对比实验，分析其在各种条件下的性能差异，并对错误类型进行细致归类。

Result: LLM在合成基准上正确率达84%-89%，但在真实世界类任务上仅为25%-34%，且在见过与未见代码库之间差异不大。完整docstring能略微提升正确率1%-3%，但多数提升无统计显著性。检索增强在部分文档下最有效，可提升正确率4%-7%。主要错误类型为AttributeError、TypeError和AssertionError（占84%），其中真实场景更突出类型和属性错误。检索增强减少逻辑失败，但可能引入依赖冲突。

Conclusion: 当前大语言模型在类级代码生成方面存在显著局限，特别是在真实项目场景下。新的基准和分析为改进上下文建模、文档处理和检索集成等生产级代码辅助工具提供了重要方向。

Abstract: Large language models (LLMs) have advanced code generation at the function
level, yet their ability to produce correct class-level implementations in
authentic software projects remains poorly understood. This work introduces a
novel benchmark derived from open-source repositories, comprising real-world
classes divided into seen and unseen partitions to evaluate generalization
under practical conditions. The evaluation examines multiple LLMs under varied
input specifications, retrieval-augmented configurations, and documentation
completeness levels.
  Results reveal a stark performance disparity: LLMs achieve 84% to 89%
correctness on established synthetic benchmarks but only 25% to 34% on
real-world class tasks, with negligible differences between familiar and novel
codebases. Comprehensive docstrings yield modest gains of 1% to 3% in
functional accuracy, though statistical significance is rare.
Retrieval-augmented generation proves most effective with partial
documentation, improving correctness by 4% to 7% by supplying concrete
implementation patterns absent from specifications. Error profiling identifies
AttributeError, TypeError, and AssertionError as dominant failure modes (84% of
cases), with synthetic tests overemphasizing assertion issues and real-world
scenarios highlighting type and attribute mismatches. Retrieval augmentation
reduces logical flaws but can introduce dependency conflicts.
  The benchmark and analysis expose critical limitations in current LLM
capabilities for class-level engineering, offering actionable insights for
enhancing context modelling, documentation strategies, and retrieval
integration in production code assistance tools.

</details>


### [6] [CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses](https://arxiv.org/abs/2510.26431)
*Mihály Dobos-Kovács,Levente Bajczi,András Vörös*

Main category: cs.SE

TL;DR: 本文提出了一种新的CHC求解方法，通过复用软件验证工具提升了对部分CHC问题，尤其是带有位向量的复杂问题的求解能力，展示了该方法的可行性和未来潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的CHC求解方法在处理包含位向量和底层语义的问题时效果有限，因此希望借助成熟的软件验证工具来提升性能和扩展应用场景。

Method: 提出了CHCVERIF，一个基于组合（portfolio）的CHC求解器，并采用软件验证的方法来处理CHC问题。

Result: 该方法在处理线性整数算术问题时表现一般，在位向量相关基准测试上取得了一定的成效。

Conclusion: 研究证明了使用软件验证工具作为CHC求解后端的可行性和潜力，尤其是在组合式方法的支持下。

Abstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate
representations for a variety of verification tasks, including safety checking,
invariant synthesis, and interprocedural analysis. This paper introduces
CHCVERIF, a portfolio-based CHC solver that adopts a software verification
approach for solving CHCs. This approach enables us to reuse mature software
verification tools to tackle CHC benchmarks, particularly those involving
bitvectors and low-level semantics. Our evaluation shows that while the method
enjoys only moderate success with linear integer arithmetic, it achieves modest
success on bitvector benchmarks. Moreover, our results demonstrate the
viability and potential of using software verification tools as backends for
CHC solving, particularly when supported by a carefully constructed portfolio.

</details>


### [7] [Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests](https://arxiv.org/abs/2510.26171)
*Hasnain Iqbal,Zerina Begum,Kazi Sakib*

Main category: cs.SE

TL;DR: 本文提出通过分析共享静态字段优先判别顺序依赖型测试（OD tests），实验证明该方法能显著减少测试执行和无效重跑次数，提升检测效率。


<details>
  <summary>Details</summary>
Motivation: 自动化软件测试中的flaky tests（不稳定测试）会导致测试结果不可靠，影响CI流程。不稳定测试中，order-dependent (OD)测试尤为常见，其结果依赖于测试执行顺序。现有OD测试检测与修复技术需反复多次执行全部测试，增加了开销，急需降低无效重跑次数。

Method: 该论文提出一种基于分析测试类中共享静态字段的方法，提前判别哪些测试可能为OD test，对这些测试进行优先级排序，以便于减少测试用例无效重跑次数。

Result: 在27个项目模块上实验证明，该方法在23个案例中成功优先定位所有OD tests，平均减少65.92%的测试执行，和72.19%的无关重跑。

Conclusion: 通过静态字段分析能有效识别并优先检测OD tests，大幅优化OD测试检测的效率并降低测试成本。

Abstract: Flaky tests can make automated software testing unreliable due to their
unpredictable behavior. These tests can pass or fail on the same code base on
multiple runs. However, flaky tests often do not refer to any fault, even
though they can cause the continuous integration (CI) pipeline to fail. A
common type of flaky test is the order-dependent (OD) test. The outcome of an
OD test depends on the order in which it is run with respect to other test
cases. Several studies have explored the detection and repair of OD tests.
However, their methods require re-runs of tests multiple times, that are not
related to the order dependence. Hence, prioritizing potential OD tests is
necessary to reduce the re-runs. In this paper, we propose a method to
prioritize potential order-dependent tests. By analyzing shared static fields
in test classes, we identify tests that are more likely to be order-dependent.
In our experiment on 27 project modules, our method successfully prioritized
all OD tests in 23 cases, reducing test executions by an average of 65.92% and
unnecessary re-runs by 72.19%. These results demonstrate that our approach
significantly improves the efficiency of OD test detection by lowering
execution costs.

</details>


### [8] [The "4W+1H" of Software Supply Chain Security Checklist for Critical Infrastructure](https://arxiv.org/abs/2510.26174)
*Liming Dong,Sung Une Lee,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 该文回顾并分析现有供应链安全框架在关键基础设施领域的适用性，发现存在明显短板与脱节，提出十类核心实践和一套多层次安全检查清单，以促进更精准和综合的安全防护。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击日益频繁和复杂，严重威胁关键基础设施部门及国家安全，但现有安全实践碎片化，且大多未针对CI（关键基础设施）领域进行定制，难以满足其独特需求。

Method: 进行多声道文献综述，涵盖国际框架、澳大利亚监管资料及学术研究，系统梳理和分析与关键基础设施相关的软件供应链安全实践，采用4W+1H分析法（是什么、何时、谁、怎么做、哪里实现）。

Result: 发现现有框架鲜有专为CI领域定制，并归纳出十类核心安全实践，横跨生命周期各阶段、角色和实施深度，并分析了各自的适用性。同时，提出一份结构化、分层的80问题清单，帮助利益相关者评估和提升安全水平。

Conclusion: 当前软件供应链安全指南与关键基础设施领域的实际需求存在脱节，急需集成化、上下文相关的安全方法，保护关键基础设施免受演变中的供应链威胁。

Abstract: The increasing frequency and sophistication of software supply chain attacks
pose severe risks to critical infrastructure sectors, threatening national
security, economic stability, and public safety. Despite growing awareness,
existing security practices remain fragmented and insufficient, with most
frameworks narrowly focused on isolated life cycle stages or lacking alignment
with the specific needs of critical infrastructure (CI) sectors. In this paper,
we conducted a multivocal literature review across international frameworks,
Australian regulatory sources, and academic studies to identify and analyze
security practices across the software supply chain, especially specific CI
sector. Our analysis found that few existing frameworks are explicitly tailored
to CI domains. We systematically leveraged identified software supply chain
security frameworks, using a "4W+1H" analytical approach, we synthesized ten
core categories (what) of software supply chain security practices, mapped them
across life-cycle phases (when), stakeholder roles (who), and implementation
levels (how), and examined their coverage across existing frameworks (where).
Building on these insights, the paper culminates in structured, multi-layered
checklist of 80 questions designed to relevant stakeholders evaluate and
enhance their software supply chain security. Our findings reveal gaps between
framework guidance and sector-specific needs, highlight the need for
integrated, context-aware approaches to safeguard critical infrastructure from
evolving software supply chain risks.

</details>


### [9] [A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI](https://arxiv.org/abs/2510.26275)
*Domenico Amalfitano,Andreas Metzger,Marco Autili,Tommaso Fulcini,Tobias Hey,Jan Keim,Patrizio Pelliccione,Vincenzo Scotti,Anne Koziolek,Raffaela Mirandola,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文系统分析了生成式AI对软件工程实践的全面影响，总结了其增强形式、面临的挑战、研究机遇，并提出了未来研究方向和对2030年软件工程的预测，为行业和学术界提供参考。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）正在快速改变软件工程（SE）的实践，影响SE流程的执行以及软件系统的开发、运维和演化。由于GenAI对行业变革的深远影响，亟需系统性地探索其对软件工程领域的具体影响和未来发展路径。

Method: 采用设计科学研究方法，分三轮逐步整合多种证据来源，包括学术讨论、快搜文献评述及同行反馈，运用McLuhan's tetrads作为系统化分析工具，梳理GenAI对SE流程和产品的变革。

Result: 明确了GenAI在SE中的四种基本增强形式，并系统梳理相关的研究挑战与机遇，最终归纳出未来研究方向，并根据发现提出了对2030年软件工程的十项预测。

Conclusion: 通过严谨的多轮过程和跨团队、同行验证，本文为分析GenAI如何影响SE流程、方法和工具，以及未来研究框架提供了透明且可复现的基础。

Abstract: Generative AI (GenAI) is rapidly transforming software engineering (SE)
practices, influencing how SE processes are executed, as well as how software
systems are developed, operated, and evolved. This paper applies design science
research to build a roadmap for GenAI-augmented SE. The process consists of
three cycles that incrementally integrate multiple sources of evidence,
including collaborative discussions from the FSE 2025 "Software Engineering
2030" workshop, rapid literature reviews, and external feedback sessions
involving peers. McLuhan's tetrads were used as a conceptual instrument to
systematically capture the transforming effects of GenAI on SE processes and
software products.The resulting roadmap identifies four fundamental forms of
GenAI augmentation in SE and systematically characterizes their related
research challenges and opportunities. These insights are then consolidated
into a set of future research directions. By grounding the roadmap in a
rigorous multi-cycle process and cross-validating it among independent author
teams and peers, the study provides a transparent and reproducible foundation
for analyzing how GenAI affects SE processes, methods and tools, and for
framing future research within this rapidly evolving area. Based on these
findings, the article finally makes ten predictions for SE in the year 2030.

</details>


### [10] [Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search](https://arxiv.org/abs/2510.26287)
*Guochang Li,Yuchen Liu,Zhen Qin,Yunkun Wang,Jianping Zhong,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: 本文提出无需蒸馏和外部监督的RepoSearch-R1框架，显著提升了代码仓库问答任务的答案完整性和训练效率，解决了企业级应用中的数据合规难题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在执行仓库级软件工程任务时，难以高效利用工具和根据环境反馈作出决策，且基于训练的方法依赖昂贵的大模型蒸馏，企业环境下有合规风险。

Method: 提出了RepoSearch-R1，一种基于蒙特卡洛树搜索（MCTS）的代理式强化学习框架，实现了无监督自训练，生成多样且高质量的推理轨迹，且无需模型蒸馏或外部监督。并基于此构建了专用于仓库问答任务的RepoQA-Agent。

Result: 在仓库问答任务上，RepoSearch-R1在答案完整性上相较于无检索方法提升16.0%，相较于迭代检索方法提升19.5%，相比通用代理式强化学习提升33%的训练效率。在冷启动训练中，兼顾了数据合规性和答案多样性、完整性。

Conclusion: RepoSearch-R1无需外部监督和模型蒸馏就能有效提升仓库级推理任务的表现，为企业安全数据合规和实际应用提供了新方法。

Abstract: Repository-level software engineering tasks require large language models
(LLMs) to efficiently navigate and extract information from complex codebases
through multi-turn tool interactions. Existing approaches face significant
limitations: training-free, in-context learning methods struggle to guide
agents effectively in tool utilization and decision-making based on
environmental feedback, while training-based approaches typically rely on
costly distillation from larger LLMs, introducing data compliance concerns in
enterprise environments. To address these challenges, we introduce
RepoSearch-R1, a novel agentic reinforcement learning framework driven by
Monte-carlo Tree Search (MCTS). This approach allows agents to generate
diverse, high-quality reasoning trajectories via self-training without
requiring model distillation or external supervision. Based on RepoSearch-R1,
we construct a RepoQA-Agent specifically designed for repository
question-answering tasks. Comprehensive evaluation on repository
question-answering tasks demonstrates that RepoSearch-R1 achieves substantial
improvements of answer completeness: 16.0% enhancement over no-retrieval
methods, 19.5% improvement over iterative retrieval methods, and 33% increase
in training efficiency compared to general agentic reinforcement learning
approaches. Our cold-start training methodology eliminates data compliance
concerns while maintaining robust exploration diversity and answer completeness
across repository-level reasoning tasks.

</details>


### [11] [Environmental Impact of CI/CD Pipelines](https://arxiv.org/abs/2510.26413)
*Nuno Saavedra,Alexandra Mendes,João F. Ferreira*

Main category: cs.SE

TL;DR: 通过大规模数据分析，本研究首次量化了GitHub Actions CI/CD流程的碳排放和水消耗，并提出多项实际的减缓措施，呼吁开发者关注和优化其工具链的环境足迹。


<details>
  <summary>Details</summary>
Motivation: CI/CD流程已被广泛应用于软件开发，但其碳排放和水足迹等环境影响一直不为开发者所知。随着云计算带来的环境压力增大，理解CI/CD服务，特别是GitHub Actions的环境影响日益重要。

Method: 基于Cloud Carbon Footprint框架的方法，利用目前文献报道中最大规模的数据集，分析了GitHub Actions开源仓库（免费且标准runner无限使用）的222万次工作流运行数据，量化了碳排放和水足迹。

Result: 2024年GitHub Actions生态系统碳排放估算值在150.5至994.9 MTCO2e之间，最可能情况为456.9 MTCO2e。水足迹介于1,989.6至37,664.5千升，最可能值为5,738.2千升。

Conclusion: GitHub Actions在CI/CD应用中带来了不可忽视的环境足迹。减少无用计算、选择低排放地区（如法国、英国）部署runner、调整定时运行以适应绿色电力时段、缩减仓库体积等措施可以有效降低环境影响。

Abstract: CI/CD pipelines are widely used in software development, yet their
environmental impact, particularly carbon and water footprints (CWF), remains
largely unknown to developers, as CI service providers typically do not
disclose such information. With the growing environmental impact of cloud
computing, understanding the CWF of CI/CD services has become increasingly
important.
  This work investigates the CWF of using GitHub Actions, focusing on
open-source repositories where usage is free and unlimited for standard
runners. We build upon a methodology from the Cloud Carbon Footprint framework
and we use the largest dataset of workflow runs reported in the literature to
date, comprising over 2.2 million workflow runs from more than 18,000
repositories.
  Our analysis reveals that the GitHub Actions ecosystem results in a
substantial CWF. Our estimates for the carbon footprint in 2024 range from
150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most
pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5
kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon
footprint and 5,738.2 kiloliters for water footprint. To provide perspective,
the carbon footprint in the most likely scenario is equivalent to the carbon
captured by 7,615 urban trees in a year, and the water footprint is comparable
to the water consumed by an average American family over 5,053 years.
  We explore strategies to mitigate this impact, primarily by reducing wasted
computational resources. Key recommendations include deploying runners in
regions whose energy production has a low environmental impact such as France
and the United Kingdom, implementing stricter deactivation policies for
scheduled runs and aligning their execution with periods when the regional
energy mix is more environmentally favorable, and reducing the size of
repositories.

</details>


### [12] [Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis](https://arxiv.org/abs/2510.26423)
*Dong Huang,Mingzhe Du,Jie M. Zhang,Zheng Lin,Meng Luo,Qianru Zhang,See-Kiong Ng*

Main category: cs.SE

TL;DR: 本文提出多智能体系统Nexus，实现自动生成高质量测试oracle，相比主流方法能显著提升准确率、bug检测与程序修复效果，推动自动化软件测试发展。


<details>
  <summary>Details</summary>
Motivation: 在软件工程中，非回归测试的测试oracle生成一直是个难题。传统方法难以自动、准确地判断被测函数是否符合预期行为，因此亟需新的自动化、高效解决方案。

Method: 本文提出了Nexus，一个多智能体框架。Nexus集成了四个风格迥异的专家智能体，通过协作评审、验证和自我迭代，生成高质量的测试oracle。具体流程包括：专家团协同完善初始oracle，系统生成候选函数实现并在安全沙盒中运行oracle验证其有效性，对于验证失败的oracle，通过自动化自我修正机制，定位并纠正错误后再验。

Result: 在七组基准测试中，Nexus在oracle准确率上显著优于现有方法。例如，在LiveCodeBench数据集，GPT-4.1-Mini的oracle准确率通过Nexus从46.30%提升至57.73%。同时下游任务表现也显著提升，包括bug检测率、人类评测任务成功率以及自动程序修复率。

Conclusion: Nexus有效解决并优化了测试oracle的自动生成问题，带来更高准确率和更好下游表现，为非回归测试和软件质量保障带来重要进展。

Abstract: Test oracle generation in non-regression testing is a longstanding challenge
in software engineering, where the goal is to produce oracles that can
accurately determine whether a function under test (FUT) behaves as intended
for a given input. In this paper, we introduce Nexus, a novel multi-agent
framework to address this challenge. Nexus generates test oracles by leveraging
a diverse set of specialized agents that synthesize test oracles through a
structured process of deliberation, validation, and iterative self-refinement.
During the deliberation phase, a panel of four specialist agents, each
embodying a distinct testing philosophy, collaboratively critiques and refines
an initial set of test oracles. Then, in the validation phase, Nexus generates
a plausible candidate implementation of the FUT and executes the proposed
oracles against it in a secure sandbox. For any oracle that fails this
execution-based check, Nexus activates an automated selfrefinement loop, using
the specific runtime error to debug and correct the oracle before
re-validation. Our extensive evaluation on seven diverse benchmarks
demonstrates that Nexus consistently and substantially outperforms
state-of-theart baselines. For instance, Nexus improves the test-level oracle
accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The
improved accuracy also significantly enhances downstream tasks: the bug
detection rate of GPT4.1-Mini generated test oracles on HumanEval increases
from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of
automated program repair improves from 35.23% to 69.32%.

</details>


### [13] [SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning](https://arxiv.org/abs/2510.26457)
*Fang Liu,Simiao Liu,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 该论文针对自动化代码评审在安全问题领域的不足，提出了SecureReviewer方法，通过专属数据集和安全感知微调提升大模型的安全评审能力，并结合RAG知识增强和SecureBLEU新评价指标，在安全问题检测和评论质量上超过现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码评审主要关注通用代码问题，对于安全相关问题的识别与解决研究不足。此外，数据稀缺和评价指标欠缺也制约了安全代码评审方法的有效性。

Method: 提出SecureReviewer方法，通过构建专门的数据集，采用安全感知的微调策略提升大模型对代码安全问题的识别和修复能力。为减少大模型幻觉与提升输出可靠性，融合RAG技术以引入安全领域知识。并设计了SecureBLEU评价指标以专门评估安全相关评论的有效性。

Result: 实验结果显示，SecureReviewer在安全问题检测准确率以及评论质量和实用性上均优于当前主流方法。

Conclusion: SecureReviewer有效提升了大模型在代码安全问题识别和修复方面的能力，同时提出了更为合理的安全代码评审评价体系。

Abstract: Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.

</details>


### [14] [Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study](https://arxiv.org/abs/2510.26480)
*Sivajeet Chand,Melih Kilic,Roland Würsching,Sushant Kumar Pandey,Alexander Pretschner*

Main category: cs.SE

TL;DR: 本文系统评估了五款开源LLM在Python代码自动Extract Method重构任务上的性能，并提出RCI递归批评与改进的提示方法，相较一次性提示取得更高的正确性和代码质量。主流模型表现优异，开发者认可度高，并建立开放基准促进自动化重构领域发展。


<details>
  <summary>Details</summary>
Motivation: Extract Method重构对代码可读性和可维护性具有重要作用，但传统方法需大量人工介入。新一代开源、高效LLM为自动化此类高级任务带来新契机，因此需系统检验LLM在EMR自动化中的效果及最佳实践。

Method: 对5种开源LLM（参数量3B至8B）在Python代码EMR任务自动化上进行系统评测。通过自动化指标（如测试通过率、代码行数、圈复杂度）评价功能正确性和代码质量，并将一次性提示与RCI递归批评与改进两种提示方法对比。同时开展开发者问卷调查，对RCI生成的重构代码进行人工评估。

Result: RCI提示方法在测试通过率和重构质量上显著优于一次性提示。Deepseek-Coder-RCI和Qwen2.5-Coder-RCI的TPP分别为0.829和0.808，代码行数和圈复杂度均有大幅下降。开发者对RCI生成的重构代码接受率超70%，Qwen2.5-Coder各评价维度最高；相比之下，原始代码在可读性、可维护性方面评分偏低。传统指标虽有参考价值，但与人工评价存在偏差。开放基准数据集促进后续研究。

Conclusion: RCI递归批评与改进的提示策略在自动Extract Method重构任务中优于传统的一次性提示，所生成的重构代码在功能正确性和代码质量上均优于原始代码。优秀模型如Deepseek-Coder-RCI和Qwen2.5-Coder-RCI获得了较高的测试通过率和代码质量改进，且得到开发者较高认可。

Abstract: Automating the Extract Method refactoring (EMR) remains challenging and
largely manual despite its importance in improving code readability and
maintainability. Recent advances in open-source, resource-efficient Large
Language Models (LLMs) offer promising new approaches for automating such
high-level tasks. In this work, we critically evaluate five state-of-the-art
open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python
code. We systematically assess functional correctness and code quality using
automated metrics and investigate the impact of prompting strategies by
comparing one-shot prompting to a Recursive criticism and improvement (RCI)
approach. RCI-based prompting consistently outperforms one-shot prompting in
test pass rates and refactoring quality. The best-performing models,
Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)
scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from
12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453
and 3.294, respectively. A developer survey on RCI-generated refactorings shows
over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation
criteria. In contrast, the original code scored below neutral, particularly in
readability and maintainability, underscoring the benefits of automated
refactoring guided by quality prompts. While traditional metrics like CC and
LOC provide useful signals, they often diverge from human judgments,
emphasizing the need for human-in-the-loop evaluation. Our open-source
benchmark offers a foundation for future research on automated refactoring with
LLMs.

</details>


### [15] [Envisioning Future Interactive Web Development: Editing Webpage with Natural Language](https://arxiv.org/abs/2510.26516)
*Truong Hai Dang,Jingyu Xiao,Yintong Huo*

Main category: cs.SE

TL;DR: 本文提出并开源了自动化生成网页编辑微调数据集Instruct4Edit的方法，实现了更高效、准确的基于自然语言的网页编辑。微调后的小型开源模型可媲美专有系统，推动相关领域发展。


<details>
  <summary>Details</summary>
Motivation: Web应用的迭代通常需要手动、耗时的代码修改。虽然大语言模型（LLMs）能生成UI代码，但其根据新设计需求自动编辑现有代码的能力有限，主要原因在于缺乏大规模高质量的微调数据集，因此难以满足用户实际意图。

Method: 本文提出了一种新颖的自动化数据生成流水线，利用LLM合成名为Instruct4Edit的高质量网页编辑微调数据集。该方法包括生成多样化的编辑指令、自动实现对应的代码修改，并通过视觉方式验证代码改动的正确性。

Result: 通过对模型在Instruct4Edit数据集上的微调，实验表明模型在将用户意图转化为结构合理、视觉准确的代码改动方面有持续改进，微调后的开源小模型也能达到接近甚至媲美专有系统的性能。

Conclusion: 本文为基于自然语言的网页编辑提供了可扩展、透明的技术基础，并公开所有数据、代码和模型，以便复现和进一步研究。

Abstract: The evolution of web applications relies on iterative code modifications, a
process that is traditionally manual and time-consuming. While Large Language
Models (LLMs) can generate UI code, their ability to edit existing code from
new design requirements (e.g., "center the logo") remains a challenge. This is
largely due to the absence of large-scale, high-quality tuning data to align
model performance with human expectations. In this paper, we introduce a novel,
automated data generation pipeline that uses LLMs to synthesize a high-quality
fine-tuning dataset for web editing, named Instruct4Edit. Our approach
generates diverse instructions, applies the corresponding code modifications,
and performs visual verification to ensure correctness. By fine-tuning models
on Instruct4Edit, we demonstrate consistent improvement in translating human
intent into precise, structurally coherent, and visually accurate code changes.
This work provides a scalable and transparent foundation for natural language
based web editing, demonstrating that fine-tuning smaller open-source models
can achieve competitive performance with proprietary systems. We release all
data, code implementations, and model checkpoints for reproduction.

</details>


### [16] [Reflecting on Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models](https://arxiv.org/abs/2510.26538)
*David Williams,Max Hort,Maria Kechagia,Aldeida Aleti,Justyna Petke,Federica Sarro*

Main category: cs.SE

TL;DR: 本文分析了LLM在软件工程领域应用中出现的新问题，总结了当前的实践与不足，并提出了改进建议，呼吁社区共同关注和提升相关研究规范。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在软件工程（SE）研究中的广泛应用，出现了基准测试严谨性、数据污染、可复现性和可持续性等新挑战。该文希望促使研究社区重视和解决这些问题。

Method: 通过对ICSE会议中基于LLM的软件工程研究进行结构化分析和回顾，总结当前实践中好的做法及存在的问题。

Result: 研究系统梳理了现有LLM-SE研究的优点与不足，发现鼓舞人心的实践与持续存在的短板。最后提出了相关改进建议。

Conclusion: 建议提升基准测试的严谨性、提高实验可复现性，并关注LLM-SE领域的经济与环境成本，以促进领域健康发展。

Abstract: Software Engineering (SE) research involving the use of Large Language Models
(LLMs) has introduced several new challenges related to rigour in benchmarking,
contamination, replicability, and sustainability. In this paper, we invite the
research community to reflect on how these challenges are addressed in SE. Our
results provide a structured overview of current LLM-based SE research at ICSE,
highlighting both encouraging practices and persistent shortcomings. We
conclude with recommendations to strengthen benchmarking rigour, improve
replicability, and address the financial and environmental costs of LLM-based
SE.

</details>


### [17] ["Show Me You Comply... Without Showing Me Anything": Zero-Knowledge Software Auditing for AI-Enabled Systems](https://arxiv.org/abs/2510.26576)
*Filippo Scaramuzza,Renato Cordeiro Ferreira,Tomaz Maia Suller,Giovanni Quattrocchi,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 针对AI审计中因黑箱模型导致验证与隐私保护冲突的问题，本文提出零知识证明驱动的ZKMLOps框架，实现了可验证且安全的合规性证明方案，并在金融审计场景下进行了实证评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统的广泛应用使其可验证性和透明度要求大幅提高，尤其在关键领域和法规推动下，传统验证方法难以兼顾审计和隐私保护，因此亟需新方法来解决合规性证明与机密信息保护的冲突。

Method: 提出并实现了将零知识证明（ZKP）与传统软件工程流程集成的 ZKMLOps 框架，提供模块化、可复用的合规验证机制。通过金融风险审计场景案例，结合主流ZKP协议，对ML模型不同复杂度下的性能进行了实证评估。

Result: ZKMLOps 实现了在不暴露核心资产（如数据和模型）的前提下生成符合法规要求的可靠证明，已在金融风险审计领域展示出较强的实用性和性能可行性。

Conclusion: ZKMLOps 框架能够在机器学习生命周期中，通过零知识证明（ZKP）实现可验证且无需泄露敏感信息的合规性证明，有效缓解了AI审计中可验证性与隐私保护之间的矛盾。

Abstract: The increasing exploitation of Artificial Intelligence (AI) enabled systems
in critical domains has made trustworthiness concerns a paramount showstopper,
requiring verifiable accountability, often by regulation (e.g., the EU AI Act).
Classical software verification and validation techniques, such as procedural
audits, formal methods, or model documentation, are the mechanisms used to
achieve this. However, these methods are either expensive or heavily manual and
ill-suited for the opaque, "black box" nature of most AI models. An intractable
conflict emerges: high auditability and verifiability are required by law, but
such transparency conflicts with the need to protect assets being audited-e.g.,
confidential data and proprietary models-leading to weakened accountability. To
address this challenge, this paper introduces ZKMLOps, a novel MLOps
verification framework that operationalizes Zero-Knowledge Proofs
(ZKPs)-cryptographic protocols allowing a prover to convince a verifier that a
statement is true without revealing additional information-within
Machine-Learning Operations lifecycles. By integrating ZKPs with established
software engineering patterns, ZKMLOps provides a modular and repeatable
process for generating verifiable cryptographic proof of compliance. We
evaluate the framework's practicality through a study of regulatory compliance
in financial risk auditing and assess feasibility through an empirical
evaluation of top ZKP protocols, analyzing performance trade-offs for ML models
of increasing complexity.

</details>


### [18] [Online and Interactive Bayesian Inference Debugging](https://arxiv.org/abs/2510.26579)
*Nathanael Nussbaumer,Markus Böck,Jürgen Cito*

Main category: cs.SE

TL;DR: 本文提出了一种集成于开发环境的、能够在线和交互式调试贝叶斯推断的新工具，经验证大幅降低了调试难度和所需时间。


<details>
  <summary>Details</summary>
Motivation: 概率编程（Probabilistic programming）能够使贝叶斯模型的构建和推断自动化，但推断的调试很困难，需要耗费大量时间和具备深厚知识，因此亟需简化贝叶斯推断调试过程的工具或方法。

Method: 提出了一种新的Bayesian推断调试工具，并直接集成进开发环境，实现在线和交互式调试。通过与18位有经验的参与者开展研究评估了工具的效果。

Result: 新工具显著减少了调试贝叶斯推断的时间和难度。实验结果表明，该方法可以显著提升调试效率，降低调试门槛。

Conclusion: 所提出的调试工具及框架有效缓解了贝叶斯推断在概率编程中的调试难题，提升了该领域建模与推断工作的可行性和易用性。

Abstract: Probabilistic programming is a rapidly developing programming paradigm which
enables the formulation of Bayesian models as programs and the automation of
posterior inference. It facilitates the development of models and conducting
Bayesian inference, which makes these techniques available to practitioners
from multiple fields. Nevertheless, probabilistic programming is notoriously
difficult as identifying and repairing issues with inference requires a lot of
time and deep knowledge. Through this work, we introduce a novel approach to
debugging Bayesian inference that reduces time and required knowledge
significantly. We discuss several requirements a Bayesian inference debugging
framework has to fulfill, and propose a new tool that meets these key
requirements directly within the development environment. We evaluate our
results in a study with 18 experienced participants and show that our approach
to online and interactive debugging of Bayesian inference significantly reduces
time and difficulty on inference debugging tasks.

</details>


### [19] [Stitch: Step-by-step LLM Guided Tutoring for Scratch](https://arxiv.org/abs/2510.26634)
*Yuan Si,Kyle Qi,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: Stitch系统用分步互动辅导代替直接给答案，通过对比关键差异并解释原因，显著提升初学者在Scratch上的编程学习效果，优于传统自动化反馈工具。


<details>
  <summary>Details</summary>
Motivation: 现有的基于积木的编程环境（如Scratch）常用于编程教育，但初学者容易遇到语义错误。传统调试工具直接展示正确代码，虽然能修复错误，但削弱了学生的问题解决能力。提升学习效果，需要更好的交互式辅导方式。

Method: 提出了Stitch系统：通过Diff-Analyze模块，将学生项目与参考实现进行对比，识别关键差异，并利用大型语言模型解释每一处修改的重要性。学生通过自定义渲染引擎逐步检查、理解并应用部分修正，反复迭代直到达成目标功能。

Result: 通过实验，将Stitch与当前最先进的自动反馈工具对比，发现Stitch采用互动、循序渐进的辅导方式显著提高了学习效果，优于直接给答案和现有自动反馈系统。

Conclusion: 单纯展示正确代码并不可取，交互式分步辅导显著提升积木编程教育效果，为界定有效反馈提出新证据。

Abstract: Block-based environments such as Scratch are increasingly popular in
programming education. While block syntax reduces surface errors, semantic bugs
remain common and challenging for novices to resolve. Existing debugging
workflows typically show the correct program directly to learners, a strategy
that may fix errors but undermines the development of problem-solving skills.
  We present Stitch, an interactive tutoring system that replaces "showing the
answer" with step-by-step scaffolding. The system's Diff-Analyze module
contrasts a student's project with a reference implementation, identifies the
most critical differences, and uses a large language model to explain why these
changes matter. Learners inspect highlighted blocks through a custom rendering
engine, understand the explanations, and selectively apply partial fixes. This
iterative process continues until the intended functionality is achieved.
  We evaluate Stitch in an empirical study, comparing it against a
state-of-the-art automated feedback generation tool for Scratch. Our key
insight is that simply presenting the correct program is pedagogically
ineffective. In contrast, our interactive, step-by-step guided system promotes
a more effective learning experience. More broadly, what constitutes effective
feedback in block-based programming remains an open question. Our evaluation
provides new evidence that step-by-step tutoring significantly enhances
learning outcomes, outperforming both direct-answer approaches and current
automated feedback generation tools.

</details>


### [20] [Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study](https://arxiv.org/abs/2510.26676)
*Samiha Shimmi,Nicholas M. Synovic,Mona Rahimi,George K. Thiruvathukal*

Main category: cs.SE

TL;DR: 本文通过分析过程指标和代码变更，发现漏洞再引入通常源自问题管理低效与团队响应不足，强调结合过程与代码指标有助于发现风险开发活动并提升软件安全。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞在被修复后仍旧存在或再次出现，揭示了代码演化与社会技术因素之间的复杂关系。现有研究主要关注源代码指标，但过程指标有望揭示导致漏洞产生的模式，但相关研究不足。

Method: 本文不再局限于文件级预测，而是在提交级分析安全修复，关注漏洞随时间演变和再次出现的更长变化序列。通过对ImageMagick项目进行案例分析，将纵向过程指标（如bus factor、问题密度、问题变质）与漏洞再引入活动相关联。

Result: 在ImageMagick项目中，76次漏洞再引入往往伴随着问题变质增加和问题密度波动，反映出短期内的问题管理效率低下和团队响应能力不足。

Conclusion: 过程指标与代码修改共同作用于理解和缓解漏洞再引入。漏洞再引入不是单一行为造成的，而是累积开发活动和社会技术条件共同导致的。结合过程和代码指标能够预测风险修复并增强软件安全性。

Abstract: Software vulnerabilities often persist or re-emerge even after being fixed,
revealing the complex interplay between code evolution and socio-technical
factors. While source code metrics provide useful indicators of
vulnerabilities, software engineering process metrics can uncover patterns that
lead to their introduction. Yet few studies have explored whether process
metrics can reveal risky development activities over time -- insights that are
essential for anticipating and mitigating software vulnerabilities. This work
highlights the critical role of process metrics along with code changes in
understanding and mitigating vulnerability reintroduction. We move beyond
file-level prediction and instead analyze security fixes at the commit level,
focusing not only on whether a single fix introduces a vulnerability but also
on the longer sequences of changes through which vulnerabilities evolve and
re-emerge. Our approach emphasizes that reintroduction is rarely the result of
one isolated action, but emerges from cumulative development activities and
socio-technical conditions. To support this analysis, we conducted a case study
on the ImageMagick project by correlating longitudinal process metrics such as
bus factor, issue density, and issue spoilage with vulnerability reintroduction
activities, encompassing 76 instances of reintroduced vulnerabilities. Our
findings show that reintroductions often align with increased issue spoilage
and fluctuating issue density, reflecting short-term inefficiencies in issue
management and team responsiveness. These observations provide a foundation for
broader studies that combine process and code metrics to predict risky fixes
and strengthen software security.

</details>


### [21] [Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment](https://arxiv.org/abs/2510.26699)
*Aylton Almeida,Laerte Xavier,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 本文评估了LLM驱动的GitHub Copilot Agent在Python SQLAlchemy库自动迁移中的表现。结果显示API迁移准确率高，但应用功能维护能力不足，测试通过率较低，表明自动化代码迁移工具还存在显著改进空间。


<details>
  <summary>Details</summary>
Motivation: 软件系统的持续更新对于避免技术债务、安全漏洞和系统僵化至关重要。然而，库和框架的更新过程通常耗时且易出错。近年来，大型语言模型（LLMs）和智能编码系统的进步为自动化维护任务带来了新机遇。本文聚焦于提升这一过程自动化和准确性。

Method: 作者选取知名Python库SQLAlchemy，在10个真实客户应用的代码库上进行版本迁移实验。迁移工作由GitHub Copilot Agent Mode自动完成，该模式可自主规划并执行多步骤迁移流程。为评估自动迁移的有效性，提出了Migration Coverage新指标，用以量化API使用点的正确迁移比例。

Result: 实验结果发现，此LLM agent在API功能迁移和用法迁移方面表现极佳（迁移覆盖率中位数达到100%），但不足之处在于未能很好地保持应用功能稳定，导致测试通过率偏低（中位数仅39.75%）。

Conclusion: 尽管大语言模型代理在代码API迁移自动化方面有显著进展，但应用功能整体维护仍存在挑战，自动化工具距离成熟应用还需进一步完善评估和修正机制。

Abstract: Keeping software systems up to date is essential to avoid technical debt,
security vulnerabilities, and the rigidity typical of legacy systems. However,
updating libraries and frameworks remains a time consuming and error-prone
process. Recent advances in Large Language Models (LLMs) and agentic coding
systems offer new opportunities for automating such maintenance tasks. In this
paper, we evaluate the update of a well-known Python library, SQLAlchemy,
across a dataset of ten client applications. For this task, we use the Github's
Copilot Agent Mode, an autonomous AI systema capable of planning and executing
multi-step migration workflows. To assess the effectiveness of the automated
migration, we also introduce Migration Coverage, a metric that quantifies the
proportion of API usage points correctly migrated. The results of our study
show that the LLM agent was capable of migrating functionalities and API usages
between SQLAlchemy versions (migration coverage: 100%, median), but failed to
maintain the application functionality, leading to a low test-pass rate
(39.75%, median).

</details>


### [22] [Optimized Log Parsing with Syntactic Modifications](https://arxiv.org/abs/2510.26793)
*Nafid Enan,Gias Uddin*

Main category: cs.SE

TL;DR: 本文系统比较了多种日志解析技术，发现两阶段架构具备准确率优势，并提出的新模块SynLog+，能大幅提升主流解析器表现，推荐实际部署应用。


<details>
  <summary>Details</summary>
Motivation: 日志分析对软件开发和维护至关重要，但日志解析器种类繁多、性能差异明显，因此需要系统评估各种解析器的特点和表现。

Method: 本研究对比了基于语法和语义的日志解析器，以及单阶段和两阶段解析架构，并对其性能进行了实证评测。

Result: 语义型方法在模板识别方面表现更好，语法型方法则在效率和分组准确率上更优（快10到1000倍），但模板识别略逊；两阶段架构对准确率有持续提升。提出的SynLog+模块（作为两阶段架构的第二阶段）可使语法和语义解析器的准确率分别平均提升236%和20%，且几乎不增加运行时开销。

Conclusion: 两阶段架构和结合先进模板识别（如SynLog+）可明显提升日志解析器的整体准确率和效率，推荐将SynLog+集成到现有解析流程中以优化日志分析效果。

Abstract: Logs provide valuable insights into system runtime and assist in software
development and maintenance. Log parsing, which converts semi-structured log
data into structured log data, is often the first step in automated log
analysis. Given the wide range of log parsers utilizing diverse techniques, it
is essential to evaluate them to understand their characteristics and
performance. In this paper, we conduct a comprehensive empirical study
comparing syntax- and semantic-based log parsers, as well as single-phase and
two-phase parsing architectures. Our experiments reveal that semantic-based
methods perform better at identifying the correct templates and syntax-based
log parsers are 10 to 1,000 times more efficient and provide better grouping
accuracy although they fall short in accurate template identification.
Moreover, two-phase architecture consistently improves accuracy compared to
single-phase architecture. Based on the findings of this study, we propose
SynLog+, a template identification module that acts as the second phase in a
two-phase log parsing architecture. SynLog+ improves the parsing accuracy of
syntax-based and semantic-based log parsers by 236\% and 20\% on average,
respectively, with virtually no additional runtime cost.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Finding Regular Herbrand Models for CHCs using Answer Set Programming](https://arxiv.org/abs/2510.26428)
*Gregoire Maire,Thomas Genet*

Main category: cs.LO

TL;DR: 本文提出一种将带ADT的CHCs编码为ASP问题并结合Clingo求解的可满足性判定方法，有效寻找模型或反例，具备半完备性和自动化优点。


<details>
  <summary>Details</summary>
Motivation: 当前已有的方法依赖CVC4等工具找到有限模型后构造自动机，存在局限性。本文旨在寻求一种新的、可扩展的方法来处理带ADT的CHCs的可满足性问题。

Method: 将带ADT的CHCs转化为ASP问题，并用Clingo工具求解，通过此过程构造识别Herbrand模型的树自动机，从而判定CHCs的可满足性。

Result: 实现了CHCs到ASP的自动翻译，并结合Clingo工具，可半完备地判定带ADT的CHCs的可满足性，成功构建正规Herbrand模型的树自动机或发现反例。

Conclusion: 本文提出的方法通过将带有代数数据类型（ADT）的受限霍恩子句（CHCs）编码为ASP问题，实现了半完备的可满足性判定。方法能发现Herbrand模型的正规性，或在不可满足时给出反例。

Abstract: We are interested in proving satisfiability of Constrained Horn Clauses
(CHCs) over Algebraic Data Types (ADTs). We propose to prove satisfiability by
building a tree automaton recognizing the Herbrand model of the CHCs. If such
an automaton exists then the model is said to be regular, i.e., the Herbrand
model is a regular set of atoms. Kostyukov et al. have shown how to derive an
automaton when CVC4 finds a finite model of the CHCs. We propose an alternative
way to build the automaton using an encoding into a SAT problem using Clingo,
an Answer Set Programming (ASP) tool. We implemented a translation of CHCs with
ADTs into an ASP problem. Combined with Clingo, we obtain a semi-complete
satisfiability checker: it finds a tree automaton if a regular Herbrand model
exists or finds a counter-example if the problem is unsatisfiable.

</details>


### [24] [Semantic Properties of Computations Defined by Elementary Inference Systems](https://arxiv.org/abs/2510.26429)
*Salvador Lucas*

Main category: cs.LO

TL;DR: 该论文提出了用任意可满足模型代替规范模型的方法，解决了用基本推理系统分析编程语言及重写系统语义属性时规范模型不可计算的问题，实现了相关属性的实际验证。


<details>
  <summary>Details</summary>
Motivation: 论文旨在研究如何利用Elementary Inference Systems（基本推理系统）来定义集合、关系和计算，并且分析这些系统下对象的性质。作者关注于解决当前语义属性验证中“规范模型”通常不可计算的难题。

Method: 作者采用了将Elementary Formal Systems（Smullyan提出的基本形式系统）与Gentzen推理规则相结合的方法，形成Elementary Inference Systems。通过给予这些系统一个一阶理论Th(I)，并将对象属性表示为一阶句子F，然后通过引入任意模型A的可满足性来替代规范模型M的验证过程。

Result: 作者证明了可以通过Th(I)的任意模型A的可满足性来(反)证系统定义对象的语义属性，无需实际计算规范模型。并将该方法应用于编程语言和可由基本推理系统描述的计算系统（如重写系统）的性质分析。

Conclusion: 通过推广验证过程，作者提供了一种新的方式，用于分析和证明基于Elementary Inference Systems描述的编程系统的语义属性，克服了规范模型不可计算的现实限制。

Abstract: We consider sets/relations/computations defined by *Elementary Inference
Systems* I, which are obtained from Smullyan's *elementary formal systems*
using Gentzen's notation for inference rules, and proof trees for atoms
P(t_1,...,t_n), where predicate P represents the considered
set/relation/computation. A first-order theory Th(I), actually a set of
definite Horn clauses, is given to I. Properties of objects defined by I are
expressed as first-order sentences F, which are proved true or false by
*satisfaction* M |= F of F in a *canonical* model M of Th(I). For this reason,
we call F a *semantic property* of I. Since canonical models are, in general,
incomputable, we show how to (dis)prove semantic properties by satisfiability
in an *arbitrary* model A of Th(I). We apply these ideas to the analysis of
properties of programming languages and systems whose computations can be
described by means of an elementary inference system. In particular,
rewriting-based systems.

</details>


### [25] [Theta as a Horn Solver](https://arxiv.org/abs/2510.26430)
*Levente Bajczi,Milán Mondok,Vince Molnár*

Main category: cs.LO

TL;DR: Theta 框架虽然在 2025 年 CHC-COMP 竞赛表现受配置影响，但其独特的 CHC-CFA 转换分析方法在修正配置后表现优异，本文详细剖析了其核心技术、优缺点及实际性能。


<details>
  <summary>Details</summary>
Motivation: Theta 参加了自 2023 年以来的 CHC-COMP 竞赛，但其具体验证技术、设计权衡与局限性在受限霍恩子句（CHCs）背景下鲜有深入探讨。本文旨在填补这一空白。

Method: 详细介绍 Theta 所采用的算法（将受限霍恩子句转换为控制流自动机进行分析）；并对比其它 CHC 求解器；复现并分析了在 2025 年 CHC-COMP 竞赛中因配置问题导致的性能表现，并在修正配置后重新运行并报告性能。

Result: 文中揭示了Theta独有的特性、在 CHC-COMP 基准测试上的优势与劣势。修正配置后，Theta 的实际能力得以准确反映，性能结果显著优于受影响时的竞赛成绩。

Conclusion: Theta 具有独特的算法创新和一定的实力，经过正确配置后能展现出更真实和优异的验证性能。对于理解受限霍恩子句工具的设计权衡有重要价值。

Abstract: Theta is a verification framework that has participated in the CHC-COMP
competition since 2023. While its core approach -- based on transforming
constrained Horn clauses (CHCs) into control-flow automata (CFAs) for analysis
-- has remained mostly unchanged, Theta's verification techniques, design
trade-offs, and limitations have remained mostly unexplored in the context of
CHCs. This paper fills that gap: we provide a detailed description of the
algorithms employed by Theta, highlighting the unique features that distinguish
it from other CHC solvers. We also analyze the strengths and weaknesses of the
tool in the context of CHC-COMP benchmarks. Notably, in the 2025 edition of the
competition, Theta's performance was impacted by a configuration issue, leading
to suboptimal results. To provide a clearer picture of Theta's actual
capabilities, we re-execute the tool on the competition benchmarks under
corrected settings and report on the resulting performance.

</details>


### [26] [Bridge and Bound: A Logic-Based Framework for Abstracting (Preliminary Report)](https://arxiv.org/abs/2510.26654)
*Andrzej Szalas*

Main category: cs.LO

TL;DR: 该论文提出了一个基于经典逻辑的新抽象框架，涵盖必要与充分条件，支持分层和近似抽象，提升了复杂系统建模与推理能力，并兼具计算效率。


<details>
  <summary>Details</summary>
Motivation: 抽象在人类认知、科学知识和模型发展中扮演核心角色，但处理不完美或不完整信息时，现有抽象理论难以满足实际需求。因此，提出更系统且逻辑化的抽象建模框架成为必要。

Method: 该论文提出了一个基于经典逻辑的抽象建模框架，能够在转换源表示为抽象表示时，不仅保留必要条件，还涵盖充分条件。框架进一步定义了近似抽象，并研究其最紧致和精确的形式，扩展为分层抽象以实现复杂系统的分级简化，详细分析了相关推理任务的计算复杂度。

Result: 该方法能对抽象过程进行更全面的逻辑建模，包括必要与充分条件，还支持层次化、近似抽象，并明确相关推理的计算复杂性。

Conclusion: 新提出的逻辑框架增强了抽象建模的能力，适用于处理复杂信息结构且具备良好的计算性质，为抽象理论和实际应用奠定坚实基础。

Abstract: At its core, abstraction is the process of generalizing from specific
instances to broader concepts or models, with the primary objective of reducing
complexity while preserving properties essential to the intended purpose. It is
a fundamental, often implicit, principle that structures the understanding,
communication, and development of both scientific knowledge and everyday
beliefs. Studies on abstraction have evolved from its origins in Ancient Greek
philosophy through methodological approaches in psychological and philosophical
theories to computational frameworks.
  Formally, abstraction can be understood as the transformation of a source
representation into an abstract representation that discards certain details
while retaining desirable features. In real-world modeling and reasoning,
abstraction is crucial, particularly when managing imperfect or incomplete
information that calls for approximate representations. This paper introduces a
novel logic-based framework for modeling abstraction processes that goes beyond
the traditional entailment of necessary conditions to encompass sufficient
conditions as well. We define approximate abstractions, study their tightest
and exact forms, and extend the approach to layered abstractions, enabling
hierarchical simplification of complex systems and models. The computational
complexity of the related reasoning tasks is also discussed.
  For clarity, our framework is developed within classical logic, chosen for
its simplicity, expressiveness, and computational friendliness.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 本文提出StreetMath基准，评估多种大语言模型在真实世界近似算术中的推理能力。发现LLMs更倾向于精确计算，对近似任务耗费较多计算资源，并未像人类在街头数学中那样快速、简化推理。实验还揭示了模型对精确与近似算术的神经元分离，相关数据与基准已开源。


<details>
  <summary>Details</summary>
Motivation: 虽然目前有大量文献研究大语言模型（LLM）在精确算术上的表现，但对其在非精确、快速数学推理能力的研究远少于对自回归架构的关注，尤其是在非自回归解码器模型上。本文关注这一研究空白。

Method: 提出StreetMath基准，用于评估模型在真实世界近似情景下的数学估算能力，对多种不同架构的LLM进行广泛测试，并应用机制可解释性技术分析模型内部计算状态。

Result: 发现LLMs普遍在需要近似的任务中仍尝试精确计算或调用外部工具，虽然在早期层或步骤有时能得到正确答案，但整体在近似任务中消耗更多token。进一步实验证明，精确与近似算术操作依赖的神经元组件大致分离。

Conclusion: LLMs在“街头数学”情境下并不具备人类认知心理中表现出的认知吝啬特性。相关基准与数据已开源。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [28] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: SymCode通过将数学问题代码化生成并引入符号引擎验证，显著提高LLM在复杂数学推理任务的准确率和透明性，增强了AI在正式领域的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂数学推理任务中表现欠佳，生成的文本答案难以验证且常常存在算术错误。目前主流的提示策略（如思维链）仍然依赖于不可靠的文本生成，缺乏确定性的验证机制。

Method: 提出了一种新颖的神经符号框架SymCode，将数学问题求解重新定义为使用SymPy库进行可验证代码生成的任务，提供了一种机制让模型生成可被验证的程序性解答。

Result: 在MATH-500和OlympiadBench等高难度基准测试上，SymCode的准确率相比基线方法提升最高达到13.6个百分点，且在令牌效率上更优。模型的错误类型也从晦涩的逻辑错误变为可追踪的编程错误，透明性更高。

Conclusion: 通过将LLM推理与确定性符号引擎结合，SymCode提升了AI在形式化领域的准确性与可信度，是通向可验证、高可靠AI的重要一步。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [29] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本研究提出了一种结合意见词强度与模糊逻辑的情感分析方法，可以细致分类并量化评论中的情感，从而为产品等实体按特定方面和情感强度进行排名，提高评估的精准性。


<details>
  <summary>Details</summary>
Motivation: 情感分析领域通常仅关注评论的极性，而忽略了情感表达的强度（如强烈负面、弱正面等）。作者认为考虑情感强度能够更细致地评估实体（如产品、服务等）的表现和受欢迎程度。

Method: 提出结合词性（副词、形容词、名词和动词）与模糊逻辑算法，通过对评论和查询进行颗粒度分类（非常弱、弱、中等、强、非常强），同时利用句法依赖解析来确定与目标方面相关的词，从而为每个方面计算实体得分。

Result: 能够自动根据评论中不同方面的情感倾向及其强度，对产品或服务等实体进行排名，具体体现为细颗粒度的评价分数。

Conclusion: 通过融合评论意见词的情感强度和句法关系，所提出方法能够更细致且准确地评估实体在用户评论中的表现和受偏好程度。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [30] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 该论文提出了评测大语言模型量子编程能力的QCoder Benchmark，并发现主流LLMs在该任务中表现有限，推理型模型表现优异，远超人类编程者。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在自动编程代码生成方面应用广泛，但在需要与硬件设备交互的领域（如量子编程）仍然研究不足，现有人类编写Python代码在量子计算机中执行的能力有限。

Method: 提出QCoder Benchmark评测框架，用于评估LLMs在量子编程任务中的表现。该框架利用量子模拟器环境提供特定于领域的反馈指标（如电路深度、执行时间、错误分类），并引入真实编程竞赛的人类代码作为对比。

Result: 实验发现，如GPT-4o这样先进的模型在该评测中的准确率仅为18.97%，而基于推理的模型o3准确率高达78%，也超过了人类代码的平均成功率（39.98%）。

Conclusion: QCoder Benchmark框架揭示了量子编程代码生成的高难度，部分推理型模型已显著超越人类平均表现，数据集和API已开放以推动该领域研究。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [31] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 本文提出了面向韩文的大规模目标无关型立场检测数据集LASTIST，详细说明了数据收集和构建流程，并用先进模型进行了实验，为韩文及低资源语言中的立场检测研究提供了重要支持。


<details>
  <summary>Details</summary>
Motivation: 目前立场检测主要集中在目标依赖型任务，并且主流数据集都基于英文，导致在韩国语等低资源语言上难以开发相关模型。尤其是对于新兴领域如立场检测，缺乏合适的数据集。

Method: 提出了一个大规模的目标无关型立场检测数据集LASTIST。数据集由韩国主要政党发布的新闻稿收集，包含563,299条标注的韩文句子。文中详细介绍了数据集的收集和构建方法，并使用最新深度学习模型进行了训练和测试。

Result: 成功构建了适用于立场检测各种任务（包括目标无关型和历时立场演化检测）的大规模韩文数据集LASTIST，并发布于开源平台，促进了低资源语言领域立场检测研究。

Conclusion: LASTIST弥补了韩文立场检测领域的数据资源空缺，将推动目标无关型立场检测和相关深度学习模型在低资源环境下的发展。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [32] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 本文提出zFLoRA低秩适配器，显著降低LLM推理延迟，实验证明在多硬件平台上几乎无延迟且性能优异，适合多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在部署任务特定的适配器时，虽然适配器参数数量较少，却会导致推理时的显著计算开销，甚至高达基线模型的2.5倍。因此，降低适配器在实际部署中的推理延迟成为重要需求。

Method: 本文提出了一种新的零延迟低秩适配器方法，称为zFLoRA。该方法通过融合低秩适配器实现近乎零延迟或极低延迟，无需显著增加模型推理开销。

Result: 在1B、3B和7B规模的LLM上进行实验，zFLoRA在18个常识推理、数学推理和摘要对话等任务中，对比LoRA和全量微调，取得了更优或持平的性能。在实际硬件（NPU和GPU）上的延迟测试显示，zFLoRA仅带来零或极小的延迟。

Conclusion: zFLoRA方法有效解决了低秩适配器推理时延迟过高的问题，可在多种硬件平台上实现近乎零延迟的适配器部署，同时保持下游任务的高性能。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [33] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文针对机械可解释性中的电路发现任务，提出基于自助法、比值筛选和整数线性规划的三项新方法，在多个任务和模型上效果优于以往，提升了发现电路的真实性与性能。方法与代码已公开。


<details>
  <summary>Details</summary>
Motivation: 机械可解释性中的一个主要挑战是电路发现，即确定模型的哪些部分执行了特定任务。当前方法在稳定性和性能上存在不足。

Method: 基于Mechanistic Interpretability Benchmark（MIB），提出三点改进：1) 采用自助法（bootstrapping）筛选具有一致归因分数的边；2) 使用基于比值的简单选择策略，优先考虑高正分边，兼顾性能和忠实度；3) 用整数线性规划替代贪心算法进行边选择。

Result: 提出的方法能发现更符合原理的电路，并在多个MIB任务和不同模型上优于现有方法。

Conclusion: 文中提出的三项技术改进显著提升了电路发现的忠实度和整体性能，推动了机械可解释性领域的发展。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [34] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 提出LISTEN框架，依靠大语言模型，用自然语言指令辅助多目标决策。其中LISTEN-U适合参数化场景，LISTEN-T适合复杂偏好。实验涵盖实际任务并证明方法有效，显著减少专家认知负担。


<details>
  <summary>Details</summary>
Motivation: 多目标决策面临专家隐性偏好难以形式化表达的问题，人工选择极易造成认知负担。希望借助LLM，简化偏好表达、提高选择效率。

Method: 提出LISTEN系统，利用大语言模型（LLM）作为零样本偏好判别器，仅依靠专家的高层次优先级描述。包含两个算法：LISTEN-U（通过LLM精炼参数化效用函数）、LISTEN-T（非参数化方法，通过锦标赛方式在小批量中筛选方案）。

Result: 在航班预订、购物、考试排班等不同实际任务中，LISTEN-U在偏好参数化时效果更佳（通过新颖的协同度指标衡量），LISTEN-T在偏好表达更复杂时更具鲁棒性。

Conclusion: LISTEN框架通过自然语言描述专家偏好，有效简化了多目标选择过程。LISTEN-U和LISTEN-T两种算法分别适用于偏好参数化明确和非参数化、更为复杂场景。整体而言，在实际任务中展现了优异表现。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [35] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: 针对长文本预训练数据筛选问题，提出LongFilter方法，有效选取需要长距离依赖的数据，使模型能力和评测结果显著提升。


<details>
  <summary>Details</summary>
Motivation: 长文本数据在提升语言模型推理、代码生成和文档摘要等高级能力上具有重要作用，但现有大量长文本数据并不具备有意义的远距离依赖，大部分内容只需局部上下文即可预测，导致训练效率低下。因此，迫切需要筛选适合长文本预训练的数据。

Method: 提出了LongFilter框架，通过对比模型在长文本和短文本环境下的预测，衡量延长上下文所带来的信息增益，从而筛选出确实需要远距离依赖的数据样本。

Result: 在LLaMA-3-8B语言模型上，将上下文长度从8K扩展到64K，LongFilter高效筛选出高质量数据，并在HELMET、LongBench和RULER等基准测试上取得了显著改进。

Conclusion: LongFilter能够有效提升长文本预训练效率和模型性能，通过数据筛选让模型充分利用长距离依赖关系，为长文本语言模型的发展提供了重要工具。

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [36] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: LLM在人格设定影响下，会引入意识形态偏见，造成审核标准在不同政治立场上不一致，对AI中立性提出挑战。


<details>
  <summary>Details</summary>
Motivation: 内容审核系统依赖LLM，确保公平与中立至关重要，探究人格设定（persona adoption）对LLM分类一致性与公平性的影响，尤其是在不同意识形态下。

Method: 通过对不同架构、规模和内容模态的LLM进行实验，在采用不同意识形态倾向的人格设定下，分析模型对有害内容分类的表现和一致性，包括政治倾向任务测试和一致性分析。

Result: 人格设定对整体准确率影响有限，但会导致模型在不同意识形态下对有害内容的判断倾向发生变化。大模型更容易与自身倾向一致的人格设定产生一致性，导致意识形态组间分歧加大；在政治相关任务中，模型倾向于维护自身立场并淡化对立观点的有害性。

Conclusion: 人格设定会引入细微的意识形态偏见，可能影响LLM在内容审核任务中的公平性和中立性。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [37] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: 本文提出了CLEAR实体增强检索方法显著提升了EHR语义问答的效果和计算效率，特别适合处理超长临床笔记，并为同类系统评测提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中的临床文档以base64编码存储，导致语义问答困难，同时传统向量数据库方法易忽略细致的临床关系。作者旨在解决EHR中高效精准语义检索和问答的问题。

Method: 提出新的CLEAR（Clinical Entity Augmented Retrieval）方法，结合实体感知的检索方式。开发并使用一个临床笔记问答评测平台，将CLEAR与零样本大上下文推理及传统分块检索增强生成方法进行比较，在包含10,000至65,000 tokens的12份实际EHR临床笔记上进行测试。

Result: CLEAR检索方法在相对于广域上下文处理减少78%token消耗的同时，达到了58.3%的胜率，平均语义相似度0.878，且对超过65,000 token的长文档胜率高达75%。

Conclusion: 实体感知检索方法能够同时提升临床自然语言处理问答的效率与准确性。评测平台也为临床问答系统提供了可复用、透明的基准框架，适用于高要求的语义精度与计算效率场景。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [38] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 本文系统性综述了大语言模型数据高效后训练方法，归纳分类了典型方案，指明挑战并提出未来研究方向，为提升大模型数据利用效率奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的后训练对于激发模型的任务泛化能力和特定领域能力至关重要。然而，当前该过程面临数据成本高昂、人工标注困难及数据规模带来的收益递减等问题，因此实现高效的数据利用成为核心研究问题。

Method: 作者首次以数据为中心，对数据高效的LLM后训练方法进行了系统性综述，并提出了包括数据选择、数据质量提升、合成数据生成、数据蒸馏与压缩、自我进化数据生态系统在内的方法分类。随后总结了每个类别中的代表性方法，并展望了未来的研究方向。

Result: 本文系统梳理了数据高效LLM后训练的主要方法类别，总结了代表性研究，并提出了当前尚未解决的问题以及未来的研究路径。

Conclusion: 本综述为大模型的数据高效后训练提供了完整的研究脉络和方法归纳，明确了该领域研究的挑战与未来前景，希望促进更高效地利用数据来激发大模型潜力。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [39] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 本文系统评估了LLM工具在语义注释任务中的作用，结果显示人工+LLM混合方式既快又好，纯自动化虽然速度最快但质量较差。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型可用来加速或替代人工创建语言资源，但其在注释任务上的实际性能和影响仍缺乏全面评估，尤其是在不同“视角化”NLP任务中。

Method: 通过对三种方式（人工、自动化、半自动化）进行语义角色标注实验，对比注释时间、覆盖率和多样性，评价LLM工具在FrameNet风格的语义注释任务中的表现。

Result: 半自动化注释兼具效率和质量，能提升语义框架多样性，且覆盖率与纯人工相似。自动化方式虽然速度快，但在质量和多样性等指标上明显逊色。

Conclusion: 半自动化的注释方式（结合LLM工具和人工）在注释覆盖率上与人工方式相当，但能提高语义框架的多样性，而纯自动化方式在大多数评估指标上表现较差，仅在注释时间上有优势。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [40] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

TL;DR: 该工作提出RECAP方法，通过反馈回路和越狱机制，从LLM输出中有效抽取模型可能记住的训练数据，并在新基准上表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 在无法直接检查大型语言模型（LLM）训练数据的情况下，如何确定模型是否“见过”某些内容？作者认为最有力的证据是模型能否自由复现目标内容。

Method: 提出RECAP，这是一个用于从LLM输出中引导和验证记忆训练数据的管道流程。它采用反馈驱动循环：初步抽取尝试由第二个语言模型评估，后者与参考段落比对，发现差异，并通过纠错提示反馈到目标模型，指导后续生成。同时，为突破模型对敏感内容的拒绝，还引入了“越狱”模块。

Result: 在EchoTrace基准（涵盖30多本完整书籍）上评估，RECAP显著优于单次抽取方式。例如在GPT-4.1上的ROUGE-L得分对版权文本抽取从0.38提升到0.47，增幅近24%。

Conclusion: RECAP能更高效、准确地从LLM输出中挖掘和验证模型记忆的训练数据，突破现有抽取方式的局限。

Abstract: If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [41] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 多语言大模型预训练时，只要确保各语言语料量充足，并合理混合，模型不仅不会性能降低，还能提升多语种表现。英语作为枢纽语言能泛化模型；未见多语言性带来的性能诅咒。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型预训练中，如何权衡语言覆盖度和模型性能一直有争议，尤其是所谓多语言性带来的"性能诅咒"问题亟待实证探讨。

Method: 训练1.1B和3B参数规模的LLM，分别采用涵盖25到400种语言的多样化语料混合，分析不同语言数量和组合对模型性能的影响。

Result: 1. 英语与多语言数据混合并不会损害任一语言组的性能，只要语料规模足够。2. 将英语设为pivot语言利于全体语言泛化，选本族内高资源语言作pivot并不会带来更好族内表现。3. 未观察到随训练语言增多带来的显著性能下滑（无明显"多语言性诅咒"）。

Conclusion: 只要合理平衡数据，多语言语料可提升模型能力，即使在低资源语种下也无性能损失。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [42] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文发现，机器翻译在文化敏感领域有明显的语义标签漂移，尤其是在使用LLMs时，文化因素显著影响标签的准确性，调用文化知识可能加剧漂移。建议MT系统在低资源语言数据生成时必须关注文化适应性，否则可能导致误解甚至冲突。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译（MT）常用于低资源语言的数据扩充，但过去关于MT语义标签漂移的研究很少关注文化因素。本文提出，源语言与目标语言的文化差异可能导致翻译过程中语义标签的漂移或改变。

Method: 本文通过一系列实验，涵盖文化敏感和中性领域，比较传统MT工具与现代大型语言模型（LLMs）在语义标签漂移方面的表现，并分析影响标签保持的主要因素。

Result: 主要发现：(1) MT系统（包括LLMs）在翻译过程中特别是文化敏感领域会造成语义标签漂移；(2) LLMs对文化知识有编码能力，利用这些能力会加剧标签漂移；(3) 源语言与目标语言的文化相似性或差异性是标签保真的关键决定因素。

Conclusion: 忽视文化因素会降低标签保真度，风险包括下游应用中的误解和潜在的文化冲突。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [43] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文针对Amazon AWS的AI加速器Trainium，设计了高效的矩阵乘法方法，通过核融合和数据缓存策略，显著提升了LLM推理性能，对比官方实现，最高加速可达2.49倍。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用的扩展，对AI加速器的高性能和成本有效性需求日益增加。Amazon AWS开发的Trainium专为大规模语言模型（LLM）训练和推理设计，但其独特的异构架构（如脉动阵列和数据布局要求）使得高效利用十分具有挑战性。本文的目的是解决这些架构约束，提升LLM推理中的关键计算核——矩阵乘法（matmul）的性能。

Method: 本文针对Trainium的架构特性设计了高性能矩阵乘法实现，采用了核函数融合（kernel fusion）和新颖的数据缓存策略，以减少在软件管理内存中的数据移动、最大化SRAM带宽，并避免昂贵的矩阵转置操作。

Result: 经过在9个数据集和4个主流LLM上的评估，提出的系统在矩阵乘法核层面平均加速1.35倍，最高达2.22倍；在端到端的LLM推理场景下平均加速1.66倍，最高达2.49倍，大幅超越AWS官方实现。

Conclusion: 定制化的核融合和缓存优化等技术能显著提升Trainium加速器在LLM推理中的matmul性能，显著优于现有方法。本文为AI推理系统在特定硬件上的性能优化提供了有效解决方案。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [44] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文针对LLM编码阶段自注意力计算瓶颈，提出注意力图复用加速框架AttnCache，可在保持准确率的情况下显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前很多实际应用如分类、问答、推荐、文本嵌入仅依赖于LLM的prefill阶段，主要进行输入编码，不需要自回归式的解码。此阶段的性能瓶颈在于自注意力机制的计算复杂度高，尤其随序列长度呈二次增长。因此亟需加速自注意力计算。

Method: 提出AttnCache框架，通过构建注意力图缓存数据库，并用高效缓存与相似性检索技术，在推理过程中识别并复用已缓存的注意力图，从而减少自注意力的计算开销。该方法基于发现语义不同的句子往往可以产生相似的跨层注意力图。

Result: 实验表明，AttnCache在CPU上实现1.2倍整体加速、2倍注意力加速；在GPU上实现1.6倍整体加速、3倍注意力加速，且几乎不会降低准确率。

Conclusion: AttnCache能够通过复用相似的注意力图显著加速LLM的prefill阶段推理，在保障精度的前提下有效缓解自注意力计算的性能瓶颈。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [45] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: SRL是一种新训练方法，通过引入逐步推理和奖励机制，显著提升开源小模型的复杂推理和泛化能力，对比现有方法优势明确。


<details>
  <summary>Details</summary>
Motivation: 目前小型开源大模型在需要多步推理的问题上表现不佳。RLVR由于正确解很难采样而失败，SFT则容易出现过拟合且模仿僵化。作者希望找到能让模型更好学习复杂推理问题的方法。

Method: 提出了一种新的训练框架——监督式强化学习（SRL），将问题求解转化为生成一系列逻辑“动作”。SRL鼓励模型在每步动作前生成内部推理独白，并根据模型动作与专家动作的相似度给予更平滑的逐步奖励。此外，先用SRL预训练再通过RLVR微调，可以获得最佳性能。

Result: SRL能够让小模型学习以往SFT和RLVR无法解决的复杂问题，且在推理、软件工程等任务中表现出强大的泛化能力，是一种稳健、通用的推理型大模型训练框架。

Conclusion: SRL为小型大模型带来了复杂推理能力，相较于传统SFT或RLVR有更强的表现，是发展推理型LLM的新方向。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [46] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: PORTool结合RL优化LLM工具调用路径，通过分步奖励增强模型探索性，显著提高了动态环境下的准确率和工具调用效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）通过静态数据集进行训练，在工具调用和多步推理中表现有限，难以适应动态变化的工具调用环境，探索解决方案上的能力不足。

Method: 提出了一种名为PORTool的强化学习（RL）方法，对工具使用型LLM进行训练。该方法通过生成多个工具调用路径（rollouts）形成树状结构，并对每个步骤根据其产生正确答案和成功调用工具的能力分配奖励。不同路径分叉处的步骤奖励有所不同，最终利用这些分步奖励进行优势计算，优化LLM的工具调用能力。

Result: 实验覆盖17种工具，涉及时效性和非时效性主题，系统性消融实验验证了分步奖励机制的必要性和设计稳健性。与其他训练方法对比，PORTool显著提升了最终准确率和工具调用步骤数量。

Conclusion: PORTool通过强化学习鼓励工具使用型LLM探索多样化的解题路径，在动态工具调用环境中显著提升了模型准确率和效率。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [47] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 多语言大模型在跨语言对齐时会牺牲文化本地化能力，作者提出框架和方法，可以分离并同时优化知识迁移与文化回复，改善性能权衡。


<details>
  <summary>Details</summary>
Motivation: 跨语言对齐（CLA）用于多语言表示的对齐，使大型语言模型能够在不同语言间顺畅转移知识，但这种表示收敛可能导致“文化消解”，即模型失去基于语言产生文化化回复的能力。

Method: 引入transfer-localization plane评估框架，同时量化知识转移和文化消解；分析最新CLA方法，并提炼模型内部在不同层可以分别优化知识和文化目标；提出Surgical Steering方法，实现推理时针对不同层分离知识转移和文化本地化目标，通过激活引导达到更好的平衡。

Result: 最新CLA方法在提升知识迁移的同时，显著损害了文化本地化能力。Surgical Steering方法通过层级激活引导，有效克服此权衡，改善了知识转移与文化化之间的平衡。

Conclusion: 当前跨语言对齐带来知识转移的提升但造成文化消解；通过层级分离和推理时引导，可兼顾知识迁移与文化本地化，为多语言大模型的应用提供新方法。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [48] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

TL;DR: 本研究用NLP自动筛查影像报告发现偶发甲状腺结节（ITFs）较为常见，且显著增加后续甲状腺癌诊断，凸显其在癌症过度诊断中的作用，亟需标准化报告和有针对性的随访以减少过度医疗。


<details>
  <summary>Details</summary>
Motivation: 影像学检查为非甲状腺相关病症频繁发现偶发甲状腺结节（ITFs），但其流行率、特征及临床后果尚不明确。

Method: 利用基于Transformer的自然语言处理（NLP）技术，回顾性分析超过11万份成人影像报告，自动识别ITFs并提取结节特征，结合多模态和多身体部位影像资料，统计随访中进一步检查和治疗情况。

Result: 在115,683名患者中，约7.8%发现ITF（其中92.9%为结节），女性、年龄较大、高BMI及肿瘤/内科开单患者发现率更高。较胸部CT，颈部CT、PET、核医学扫描更容易检出ITF。结节特征记录率低，仅44%报告大小，其他特征低于15%。ITF患者后续甲状腺结节诊断、生检、切除及癌症发生率均显著提高，大部分癌症为乳头状，且ITF后发现的癌症体积更大。

Conclusion: 偶发甲状腺结节常见，导致后续小而低风险癌症的过度诊断。需规范报告并优化随访策略，避免无谓医疗干预。

Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [49] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 本文提出一种‘一题多解’训练方式，并引入RPD指标用于推理路径区分。实验表明，该方法能显著提升模型输出的多样性和推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在测试时通过Test-Time Scaling（TTS）提升推理能力，但输出缺乏多样性。常见的“一题一解”（1P1S）训练方式导致推理路径单一，限制了模型输出的丰富性。

Method: 提出“一题多解”（1PNS）训练范式，为模型提供多样合理推理路径。此外，引入了Reasoning Path Divergence（RPD）指标，能够量化长推理链之间的语义差异，支持构建高多样性训练样本集。基于RPD方法，对Qwen3-4B-Base进行微调。

Result: 采用RPD筛选的1PNS训练后，输出更丰富，并在pass@16指标上比1P1S基线平均提升2.80%，在AIME24数据集上提升4.99%。

Conclusion: 1PNS训练结合RPD方法能丰富大模型推理多样性，进一步提升TTS效果。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [50] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 本研究用大语言模型自动为劝说技巧数据集标注话语关系，生成了银标数据集，发现某些话语关系与劝说技巧高度相关，这对揭露网络宣传及提升交流认知有实用意义。


<details>
  <summary>Details</summary>
Motivation: 目前尚无同时标注了劝说技巧（PTs）和话语关系（DRs）的数据集，而理解它们之间的关联对识别网络宣传和误导信息以及提升交流有效性有重要价值。

Method: 以SemEval 2023 Task 3的19种劝说技巧数据为基础，设计提示词，引导四种大语言模型（LLMs）对数据进行PDTB 3.0二级话语关系（22类）自动标注。共创建了40种分类器，并利用多种多数投票方式生成了5个银标数据集。

Result: 银标数据集规模从204到1,281例不等。统计分析发现六种话语关系（因果、目的、对比、因果+信念、让步、条件）在劝说文本中至关重要，尤其与特定劝说技巧（夸张/缩小、重复、激起怀疑、渲染性用语）高度相关。

Conclusion: 基于LLMs和提示工程构建的双标签银标数据集揭示了话语关系和劝说技巧的关键联系，有助于理解劝说机制并发展检测网络宣传和虚假信息的方法。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [51] [MossNet: Mixture of State-Space Experts is a Multi-Head Attention](https://arxiv.org/abs/2510.26182)
*Shikhar Tuli,James Seale Smith,Haris Jeelani,Chi-Heng Lin,Abhishek Patel,Vasili Ramanishka,Yen-Chang Hsu,Hongxia Jin*

Main category: cs.CL

TL;DR: MossNet创新性地用专家混合机制实现多头注意力，相较现有模型，在准确性、扩展性和运行效率上表现更好。


<details>
  <summary>Details</summary>
Motivation: 目前SSM/GRM模型通常只模拟单一注意力头，限制了模型表现力。希望提出一种能实现多头注意力且高效的新架构。

Method: 提出MossNet，一种基于状态空间专家混合（MoE）的模型架构，在MLP和SSM两个模块都应用专家混合机制，实现线性多头注意力。

Result: 在语言建模和下游任务上，MossNet在同等模型规模和数据量下优于Transformer和SSM架构。更大规模的MossNet也表现出良好扩展性与高性能。在真实设备上测试显示其速度和资源使用优于同等体量模型。

Conclusion: MossNet是一种高效且性能优异的新型循环LLM架构，是未来研究的有前景方向。

Abstract: Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.

</details>


### [52] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 这项工作提出了一种新的微调方法（SDM），能让语言模型更自信、更少拒绝生成，提升整体任务表现。


<details>
  <summary>Details</summary>
Motivation: 提高语言模型在任务指令跟随时的准确性与输出置信度，减少模型因低置信或不确定性产生的拒绝生成现象，提高统计效率。

Method: 将现有的预训练Transformer解码模型，经过有监督微调，使用最后一层的SDM激活层进行二分类训练，同时在训练时引入对比输入编码和在线生成的硬负样本，提高模型的判别能力。

Result: 经过SDM微调后的语言模型在与强有监督基线比较时表现出更少的拒绝生成行为，说明其统计效率有所提升。

Conclusion: SDM语言模型通过引入SDM激活层进行微调，提高了生成内容在高概率区域的比例，并减少模型在遇到不确定内容时的拒绝生成（abstentions）。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [53] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 本研究提出RCScore，通过多指令风格评估LLM，发现模型表现受指令形式显著影响，并用自洽性一致性指标CRS衡量模型可靠性，为真实场景下模型评测和部署提供新视角和方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的大型语言模型（LLM）评估通常只采用单一的指令模板，忽视了指令风格对模型响应的敏感性，而这一点对于实际应用至关重要。本文希望通过多维度的方法揭示和度量指令风格变化对模型表现的影响。

Method: 提出了RCScore评估框架，通过将基准测试题目系统性地转换为多种指令风格，量化并分析指令表达对模型输出的影响。同时引入Cross-Response Similarity (CRS) 方法，用RCScore指标度量模型应答的一致性。

Result: 实验覆盖10个LLM和4个推理基准，发现指令风格可使模型准确率波动高达16.7个百分点。CRS方法与任务准确率高度相关，自洽性可作为模型可靠性的代理指标。还发现确定性解码能产生更稳定的输出，大模型在风格一致性方面表现更好。

Conclusion: RCScore为评估模型应对不同指令风格的鲁棒性提供了系统性方法，强调指令风格敏感性在现实应用与模型可靠性中的重要性。

Abstract: Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [54] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 文章针对扩散语言模型在实际控制中易出现的“更新遗忘”问题，提出Token Timestep Allocation（TTA）方法，通过智能推断阶段排序策略，显著提升了模型的文本可控性、流畅度和准确性，并降低毒性，为稳定高效的扩散文本生成提供了关键解决方案。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）虽然能够实现细粒度的文本生成调整，但在实际控制方面表现不佳，尤其面临“更新遗忘”问题：即统一且忽略上下文的更新导致时间步之间的令牌波动，破坏之前的语义编辑，进而影响文本的连贯性和流畅性。作者旨在解决这一根本性缺陷，从而提升扩散语言模型的可控性。

Method: 提出了Token Timestep Allocation（TTA）方法，通过为每个令牌定制时间步进度表，实现令牌的软性语义排序：重要令牌提早冻结，易变令牌进一步迭代。TTA可以作为固定策略或根据任务需求的自适应策略，并且仅需在推断阶段运行，兼容各类扩散语言模型和多样监督信号。

Result: TTA在情感控制任务中提升准确率超过20%，困惑度减少近一半，且所需生成步数不到原方法的五分之一。在文本去毒任务中，最大毒性从14.5降至12.2，困惑度从32.0降至26.0。总体上，有效提升了生成文本的可控性与流畅性。

Conclusion: 通过基于时间步的令牌排序分配，有效缓解了DLM中的“更新遗忘”问题，是实现高稳定性与高可控性文本生成的核心手段。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [55] [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202)
*Rajiv Movva,Smitha Milli,Sewon Min,Emma Pierson*

Main category: cs.CL

TL;DR: 本论文提出WIMHF方法，利用稀疏自编码器自动解释人类反馈的特征，实现偏好分析与数据整顿，在安全性和个性化方面取得显著提升，为理解反馈数据和优化模型提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 人类反馈可以以不可预测和不理想的方式影响语言模型，但实践者对反馈数据实际编码的内容没有清晰理解，而自动提取相关特征又很有挑战性。

Method: 提出了WIMHF方法（What's In My Human Feedback?），利用稀疏自编码器解释反馈数据，可以自动发现并分析反馈中的特征，而无需预设假设。

Result: 在7个数据集上，WIMHF发现了少量可被人解释的特征，这些特征能解释大部分偏好预测信号，并揭示了不同数据集和人群多样化的偏好，如Reddit用户偏好非正式和幽默，HH-RLHF和PRISM则反之。同时也发现LMArena用户有不安全偏好，如倾向支持有毒内容。通过特征进行数据整顿，Arena集安全性提升37%，且不会损害总体性能。个性化特征权重也可提升偏好预测。

Conclusion: WIMHF为理解和优化人类反馈数据提供了一种以人为中心的分析方法，助力更安全且个性化的语言模型训练。

Abstract: Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.

</details>


### [56] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出GlobalQA基准评估RAG全球性任务，发现传统方法效果极差。为此，作者设计了GlobalRAG框架，通过多技术融合显著提升模型在全局任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前RAG基准主要关注局部检索与理解，但实际应用常需要对整个文档集合进行全局信息汇总和分析，现有方法在这类任务表现极差。

Method: 提出GlobalRAG多工具协作框架，包括块级检索、LLM智能过滤噪声文档，以及符号计算聚合模块。

Result: GlobalRAG在Qwen2.5-14B模型上取得6.63 F1分数，远高于传统方法的1.51 F1。

Conclusion: GlobalRAG在全球性检索增强任务上显著优于现有方法，能够有效提升对全局信息处理的能力。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [57] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本研究提出将语用理论作为提示词融入语言模型，以加强其理解隐含意义的能力。实验表明，无论详细介绍理论还是仅提及其名称，都能显著提升模型的语用推理表现，最高提升9.6%。


<details>
  <summary>Details</summary>
Motivation: 语言模型在理解隐含意义方面的能力需要提升。人类的交流高度依赖对隐含意义的把握，因此希望语言模型也具备这种能力。

Method: 将语用理论（如Grice语用学和关联理论）的概述作为提示，指导语言模型进行逐步推理以理解隐含意义。

Result: 引入语用理论提示的方法在语用推理任务上比仅用中间推理（0-shot Chain-of-Thought）基线高出最多9.6%；仅在提示中提及语用理论名称也可使大模型性能提升约1-3%。

Conclusion: 将语用理论引入提示可以提高语言模型理解隐含意义的能力。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [58] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 本论文发现预训练语言模型难以辨识借词与本土词，表现较差且有向借词倾斜的趋势。这揭示当前NLP系统在少数语言处理和识别方面的不足，对语言保护工具开发有重要意义。


<details>
  <summary>Details</summary>
Motivation: 语言之间词汇的借用与融合在语言历史中非常常见，特别是在双语社区，主导语言对少数语言的词汇施加影响。了解预训练语言模型是否具有识别借词的能力，对于保护少数语言具有重要意义。

Method: 评估多种预训练语言模型（包括大型语言模型），测试其在10种语言中区分借词和本土词的能力，通过明确指示和上下文信息进行实验。

Result: 即使给予明确的指导和背景，各模型在区分借词与本土词方面表现不佳。此外，结果进一步印证了现代NLP系统倾向于借词而非本土词的偏见。

Conclusion: 预训练语言模型在识别借词与本土词方面能力有限，存在向借词偏移的系统性问题。这对少数语言NLP工具的开发和语言保护工作至关重要。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [59] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文系统评测了五种知识蒸馏方法在多语言视觉-语言模型压缩下的效果，发现不同蒸馏设计在模型性能和稳定性上存在明显权衡。部分方法能保持跨语言鲁棒性，但有的配置可能牺牲任务间的一致性，凸显仅关注总准确率不够全面。


<details>
  <summary>Details</summary>
Motivation: 多模态视觉-语言模型在不同语言上的表现不均衡，且模型缩小后问题更为严重。虽然知识蒸馏可将大模型知识迁移到小模型，但在多语言场景下的研究较少，因此有必要系统分析知识蒸馏对多语言VLM稳定性的影响。

Method: 对五种知识蒸馏方法进行控制实验，在CLIP和SigLIP2模型上，分别评估模型压缩后在同域检索和异域视觉问答任务中的表现。重点分析了蒸馏方法对跨语言表示一致性和下游任务稳定性的影响。

Result: 部分蒸馏配置在大幅压缩模型的情况下，能够提升多语言检索性能，但有些配置会导致不同任务间表现不稳定，准确率无法反映这些底层权衡。

Conclusion: 有些知识蒸馏配置能够在模型尺寸减半的情况下，仍然保持甚至提升多语言检索的鲁棒性，但其他配置则无法维护跨任务的稳定性，暴露出对蒸馏设计敏感的权衡问题，单纯看准确率无法揭示这些问题。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [60] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: 该论文提出利用LLM内部神经元激活信号的无监督解码方法NAD，不依赖外部输出，自主筛选高质量答案。实验显示NAD能大幅节省计算资源且保持高质量，与或优于现有主流方法，为模型推理任务提供了新的高效思路。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型推理增强方法主要通过sample-evaluate-ensemble解码器提升推理能力，但大多仅依赖模型外部的输出信号（如token概率、自评估等）来评分候选答案，这些信号在后训练阶段可能校准不佳。

Method: 作者分析了模型的内部神经元激活行为，发现正确答案与错误答案在神经元激活上的差异，于是提出Neuron Agreement Decoding (NAD)，一种不需要监督、仅基于内部神经元激活信号的最佳N选法：通过激活稀疏性和神经元激活一致性来选择答案，无需参考文本输出。

Result: NAD能在生成初期（前32个token）预测正确性，并支持激进的早停。在可验证答案的数学和科学基准上，NAD表现与多数投票相当；在开放式编码测试中则持续优于传统方法，还能极大减少生成token（减少99%），几乎不损失生成质量。

Conclusion: NAD展现了使用模型内部信号引导无监督集成解码的有效性和高效性，为提升LLM无标签任务提供了新的实用工具。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [61] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 本文揭示LLMs在内部对数字有准确和通用的表征，尽管生成输出时易出错。通过通用探针分析模型层级，定位误差来源，为后续优化模型架构和探针技术奠定基础。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在嵌入数字时表现出一致性和准确性，但在生成输出时却容易出错。因此，作者希望解释嵌入与实际输出之间的矛盾，并深入理解模型对数字的操作机制。

Method: 研究者通过对语言模型处理数字的方式进行量化，设计通用探针追踪误差产生的具体层级，从而分析模型内部的数字表征机制。

Result: 发现不同语言模型在隐藏状态和各种输入语境下对数字有系统且准确的表征，并能通过通用探针定位模型处理数字时的特定错误层级。同时，结果为改进模型架构和探针技术提供了理论基础。

Conclusion: 不同的大型语言模型（LLMs）虽然在生成数字相关输出时会出现错误，但它们在内部隐藏状态和多种输入语境下都能学到高度准确且可互换的数字表示。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [62] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: 本研究评估了ChatGPT Atlas在浏览器游戏中的表现。结果显示，其逻辑推理能力突出，但在动态和实时交互方面尚有限，适合静态分析类任务，对复杂交互尚存挑战。


<details>
  <summary>Details</summary>
Motivation: ChatGPT Atlas已具备网页交互和信息检索能力，但其在动态、交互性强的环境中的实际性能尚未被充分研究。本文旨在填补这一研究空白。

Method: 利用网页浏览器小游戏作为评测场景，通过游戏内得分等量化指标对不同任务类型下的Atlas表现进行早期评估。

Result: Atlas在数独等需要逻辑分析的游戏表现出色，解题速度显著超过人类基线；但在T-Rex Runner、Flappy Bird等需要实时反应与运动控制的游戏中表现较差，常常无法顺利通过最初障碍。

Conclusion: Atlas在逻辑推理型任务（如数独）中的表现优于人类，但在强调实时性和操作精确度的动态交互任务中存在明显不足。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [63] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: SCRIBE提出了一种低资源环境下，能够本地运行的小型反馈生成模型，通过工具增强和自我反思推理流程，性能接近甚至超越大型模型，非常适合教育中的隐私敏感应用。


<details>
  <summary>Details</summary>
Motivation: 在教育环境中，语言模型可以为学生提供个性化反馈，但其实际部署面临隐私、计算资源有限和响应质量三大挑战。为解决这些问题，需要能够本地运行、开源且可靠的小型模型来生成高质量反馈。

Method: 提出了SCRIBE框架，通过多步推理和工具增强（multi-hop, tool-augmented reasoning）生成有效反馈。SCRIBE整合领域工具和自我反思推理流程，实现迭代推理、工具使用和错误恢复，并通过两阶段LoRA微调，在GPT-4o生成的合成数据上对3B和8B模型进行训练。

Result: 8B-SCRIBE模型在相关性和可操作性等关键维度上，展现出与更大模型（如GPT-4o和Llama-3.3 70B）相当甚至更优的表现。在108名学生的用户调查中，模型也被认为在主观感受上不亚于这些大型模型。

Conclusion: SCRIBE框架能够在受限资源和隐私敏感的教育场景下，有效应用于学生反馈生成，满足高质量、可用性和隐私保护需求。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [64] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: 本文提出ACER方法，通过自动生成体系化课程和定制预训练，显著提升LLMs在经济学、心理学等专业任务上的表现，同时保持其在通用任务上的优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在通用任务上表现优异，但在经济学、心理学等需要深入、规范理解的专业领域表现不佳。弥补这一不足、提升LLMs专业领域能力成为研究动机。

Method: 提出ACER（Automated Curriculum-Enhanced Regimen）方法，通过自动生成教材式课程体系，包括目录和由Bloom认知分类指导的问题-答案对；再用合成语料进行持续预训练，采用交错课程调度，在内容和认知维度上系统提升模型能力。

Result: 在Llama 3.2模型（1B、3B）上的实验表明，ACER显著提升模型在MMLU专业子集上的表现，微观经济等困难领域准确率提升5个百分点，所有目标领域平均提升3个百分点。ACER还能防止遗忘，促进知识迁移，非目标领域也提升0.7点；此外在ARC、GPQA等知识密集型任务上提升2点，同时保持通用推理能力的稳定。

Conclusion: ACER是一种可扩展、有效的训练方案，能够显著缩小LLMs在专业领域的能力差距，同时保持其通用任务性能。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [65] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 本文提出MisSynth流程，通过检索增强生成合成谬论样本对大语言模型微调，显著提升模型识别科学健康错误信息的能力，尤其在标注数据和计算资源有限情况下效果尤为突出。


<details>
  <summary>Details</summary>
Motivation: 健康相关的错误信息非常普遍且可能造成伤害，尤其是当这些错误信息曲解或误用科学发现时，更难以识别。作者希望提升大型语言模型对谬论（fallacious arguments）的识别能力，改善对健康错误信息的检测。

Method: 作者提出了名为MisSynth的流程，利用检索增强生成（RAG）手段生成合成的谬论样本，并用这些样本对大型语言模型进行微调。以MISSCI数据集和评估框架为基础，通过少量标注数据与合成数据结合训练模型。

Result: 微调后的模型在准确率上相比原始基线模型有显著提升。例如，LLaMA 3.1 8B模型在MISSCI测试集上的F1分数提升了35%以上，表明引入合成谬论数据极大增强了模型对科学错误信息的零样本分类能力。

Conclusion: 合成数据生成结合轻量级微调技术，能够在有限标注资源和计算资源下，显著提升大模型在科学健康错误信息识别任务上的表现。MisSynth流程有效增强了模型识别谬论能力，相关代码和数据集已公开。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [66] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 本文提出一种无需先验知识的自动团队组建方法，通过分析模型对话构建关系图并聚类，自动发现协同能力强的LLM团队，实验显示组队效果优越且接近人工组队水平。


<details>
  <summary>Details</summary>
Motivation: 单一大语言模型（LLM）难以发挥多模型协同的优势，但组建高效的多模型团队存在重大挑战，主要因模型内部特征不透明，难以优化团队协作。

Method: 提出一种以交互为核心的自动化团队组建框架，无需知道模型架构、训练数据或任务表现。方法包括：构建模型间语义一致性关系的“语言模型图”，再用社区检测划分模型群组。

Result: 实验验证该方法能发现具备潜在功能一致性的模型团队。针对特定话题进行任务引导时，自动组成的团队在下游任务上优于随机分组，也能达到与手动组队类似的准确率。

Conclusion: 本研究提供了一种自动组建协作型多代理LLM团队的新方法，促进了模型间协作设计的自动化探索。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [67] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 本文探索了预训练语言模型和大语言模型在科学文本中的话语关系分类任务，发现利用话语结构定义的上下文可以提高分类效果，并分析了哪些话语关系类型最需上下文，为AI辅助科学论据验证奠定基础。


<details>
  <summary>Details</summary>
Motivation: 科学工作流程中越来越多地使用生成式人工智能（AI）方法，推动了利用话语层面信息来寻找AI生成科学论据的支持证据的需求。作为实现这一目标的第一步，需研究科学写作中的话语结构推断任务。

Method: 采用了预训练语言模型（PLM）与大语言模型（LLM）两种方法，针对科学出版物进行话语关系分类（DRC）实验，并考察了话语结构所定义的上下文对任务的辅助作用，同时分析了哪些科学话语关系类型最能受益于上下文信息。

Result: 实验结果表明，话语结构所定义的上下文通常对话语关系分类任务有益。此外，论文分析了不同科学话语关系类型对上下文的依赖差异。

Conclusion: 上下文信息，尤其是科学写作的话语结构，有助于提升话语关系分类的准确性，为未来基于AI自动识别和验证科学论据提供了支持。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [68] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 本文构建了涵盖知识和培养能力的中文教育大模型基准OmniEduBench，并在多款主流LLM上测试表现，发现其在实际教育应用中存在显著不足，特别是培养维度与人类水平仍有大幅差距。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）及其基准主要侧重于知识层面，忽视了培养能力的评估，这对于实际教育场景至关重要。尤其在中文教育场景下，缺乏多样性的综合基准。

Method: 提出并构建了OmniEduBench——一个包含24,602对高质量问答、覆盖知识与培养两大维度和61个科目、11种题型的综合性中文教育基准。对11款主流开源与闭源LLM进行评测。

Result: 知识维度上，只有Gemini-2.5 Pro准确率超过60%；培养维度上，最佳模型QWQ距离人类智能还差近30%。显示现有LLM在教育领域存在明显短板。

Conclusion: OmniEduBench为中文教育LLM评测建立了全面、严密的标准。结果显示LLMs在知识和培养能力方面均有显著提升空间，教育应用仍面临重大挑战。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [69] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: 提出SSLC压缩方法，将剪枝和低秩近似结合用于大型语言模型，无需额外训练即可实现高效压缩，显著提升模型推理速度且性能不下降，在主流模型上均取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在语言理解和生成方面表现突出，但因带宽与计算需求巨大，限制了其广泛应用。现有的剪枝与低秩近似方法各自有效，但两者结合用于LLMs鲜有研究。

Method: 提出了SSLC（Synergistic Sparse and Low-Rank Compression）方法，将剪枝和低秩近似统合为一个优化问题，通过迭代算法进行求解，无需额外训练步骤。理论与实验基于LLaMA与Qwen2.5模型。

Result: SSLC方法在LLaMA和Qwen2.5（7B-70B参数规模）模型上均优于单独压缩方法，达到最新效果。尤其对Qwen2.5压缩50%，性能无损，推理速度提升至少1.63倍。

Conclusion: SSLC方法兼具剪枝与低秩优势，为LLM高效部署提供了实用、优异的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [70] [Bayesian Network Fusion of Large Language Models for Sentiment Analysis](https://arxiv.org/abs/2510.26484)
*Rasoul Amirzadeh,Dhananjay Thiruvady,Fatemeh Shiri*

Main category: cs.CL

TL;DR: BNLF通过贝叶斯网络并融合三类LLM模型的输出，在金融情感分析任务上取得了显著的准确率提升，解决了现有LLM透明性和一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 针对当前大型语言模型（LLMs）在领域专用任务中的透明性缺失、可解释性差、微调成本高、提示工程需求大、跨领域结果不一致及高计算资源消耗等问题，作者提出新的解决方法。

Method: 提出了Bayesian network LLM fusion (BNLF)框架，通过概率机制融合FinBERT、RoBERTa和BERTweet三种LLM的情感分析预测。BNLF在贝叶斯网络中将多个LLM的情感预测建模为概率节点，实现晚期融合。

Result: 在三个具有不同语言和语境特点的人工标注金融语料库上评估，BNLF在准确率上比各基线LLM提升约6%，表现出对数据集变化的鲁棒性以及概率融合在可解释情感分类上的有效性。

Conclusion: BNLF框架有效提升了情感分析的准确率，并增强了模型的可解释性与鲁棒性，是应对多组LLM模型融合难题的可行方案。

Abstract: Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.

</details>


### [71] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 集成多个开源中大型LLM模型，对脑出血分诊AI的回顾性评估更准确可靠，优于单独LLM。


<details>
  <summary>Details</summary>
Motivation: 探索通过LLM模型集成能否为像脑出血AI分诊工具这类临床AI模型的可靠性评估，提供比单一LLM更好的结果。

Method: 处理了来自14家医院共29,766份无对比剂头部CT检查数据，通过商业化脑出血AI检测工具进行分析，再用8个开源LLM和内部GPT-4o，使用多轮提示，对放射科报告进行是否脑出血评估。共人工复核了1,726个示例，对各模型及其集成结果与GPT-4o进行性能比较，并测试了三种理想集成方式。

Result: Llama3.3:70b和GPT-4o性能最佳（AUC=0.78），Llama3.3:70b在F1、召回、精确度、特异性、MCC等指标表现突出。模型集成（如Full-9 Ensemble、Top-3 Ensemble等）MCC优于单独GPT-4o，无统计显著差异。

Conclusion: 集合多个中大型开源LLM模型相比单一LLM能够更一致、更可靠地对临床AI分诊工具进行回顾性评估。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


### [72] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 本文通过消融实验论证，CORE-KG框架中的指代消解和结构化提示两大关键组件极大提升了从复杂法律文档自动构建知识图谱的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 人类偷渡网络不断适应环境，分析难度提升。虽然法律案件文档蕴含关键信息，但因内容非结构化、词汇密集且指代不明，给自动知识图谱构建带来重大挑战。现有基于大型语言模型的方法虽优于传统模板，但仍易生成噪声、重复节点等问题。本文旨在系统量化CORE-KG框架中关键技术对提升知识图谱质量的作用。

Method: 采用系统性消融实验，对CORE-KG框架中的类型感知指代消解模块和领域引导结构化提示词分别进行移除，定量分析两者对节点重复和噪声的影响。

Result: 移除指代消解模块导致节点重复增加28.32%，噪声节点增加4.32%；移除结构化提示词则节点重复增加4.34%，噪声节点激增73.33%。

Conclusion: 类型感知指代消解和结构化领域提示在提升法律文本自动知识图谱质量方面均至关重要。结论为设计鲁棒的LLM知识抽取流程提供了定量指导。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [73] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: 本文提出DIVRIT系统，用视觉语言模型将希伯来语元音还原任务转化为零样本分类问题，显著提升准确率和泛化能力，显示出这一方法在希伯来语自动标注中的广阔应用前景。


<details>
  <summary>Details</summary>
Motivation: 希伯来语在没有元音符号（diacritics）时，词义和发音存在高度歧义，因此还原元音符号对准确发音和消除歧义非常重要。面对这一挑战，现有的机器学习方法虽取得进展，但仍有创新空间。

Method: 提出DIVRIT系统，将希伯来语的元音符号还原任务建模为零样本分类问题。即在每个未加注元音的词汇周围上下文条件下，从动态生成的候选集选择最合适的元音符号模式。创新之处在于运用Hebrew Visual Language Model，把未加注元音的文本当作图片处理，使元音信息嵌入输入向量表示中，无须复杂的显式语言分析。

Result: 在多种配置下系统表现优异，能在无需复杂语言学分析的情况下实现有效的元音还原。在oracle设定下（正确形式在候选中），DIVRIT达到很高准确率。通过模型结构和训练方法优化，系统泛化能力也大幅提升。

Conclusion: 视觉表示方法对提高希伯来语自动元音标注准确率和通用性有很大潜力，是一种值得关注的新思路。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [74] [The Structure of Relation Decoding Linear Operators in Large Language Models](https://arxiv.org/abs/2510.26543)
*Miranda Anna Christ,Adrián Csiszárik,Gergely Becsó,Dániel Varga*

Main category: cs.CL

TL;DR: 本文系统分析transformer模型中线性关系解码器，发现多个解码器可被高度压缩，并主要提取粗粒度属性而非精细关系，从而解释其泛化与压缩特性。


<details>
  <summary>Details</summary>
Motivation: 此前研究表明，transformer语言模型中的线性算子可解码特定的关系事实，但是否能处理多种关系以及这些算子的组织结构尚不明确。

Method: 将以往只分析单一关系的研究扩展到多种关系，系统性地分析多个关系解码器的组织方式。利用三阶张量网络对这些解码器进行压缩，并设计交叉评测协议：将每个线性解码算子应用于其他关系的主体。

Result: 发现多个关系解码算子可通过简单三阶张量网络高度压缩且解码精度几乎无损。交叉评测结果揭示，这些线性映射并不是在编码具体关系，而是在提取重复出现的粗粒度语义属性。

Conclusion: transformer语言模型中的线性关系解码更偏向于属性而非具体关系，可解释其压缩性和在语义相近关系间的泛化能力有限。

Abstract: This paper investigates the structure of linear operators introduced in
Hernandez et al. [2023] that decode specific relational facts in transformer
language models. We extend their single-relation findings to a collection of
relations and systematically chart their organization. We show that such
collections of relation decoders can be highly compressed by simple order-3
tensor networks without significant loss in decoding accuracy. To explain this
surprising redundancy, we develop a cross-evaluation protocol, in which we
apply each linear decoder operator to the subjects of every other relation. Our
results reveal that these linear maps do not encode distinct relations, but
extract recurring, coarse-grained semantic properties (e.g., country of capital
city and country of food are both in the country-of-X property). This
property-centric structure clarifies both the operators' compressibility and
highlights why they generalize only to new relations that are semantically
close. Our findings thus interpret linear relational decoding in transformer
language models as primarily property-based, rather than relation-specific.

</details>


### [75] [InfoFlow: Reinforcing Search Agent Via Reward Density Optimization](https://arxiv.org/abs/2510.26575)
*Kun Luo,Hongjin Qian,Zheng Liu,Ziyi Xia,Shitao Xiao,Siqi Bao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本文针对强化学习中的奖励稀疏问题，提出InfoFlow框架，通过任务拆分、纠错提示、双智能体协作三方面系统提升奖励密度。实验证明其在多个基准测试上优于强基线，轻量LLM性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）在深度搜索场景下，因奖励稀疏，导致智能体探索成本高而收益少，严重影响应用效果。该问题在实际任务中普遍存在，亟需提升单位探索成本带来的奖励密度。

Method: 本文提出了InfoFlow系统，系统性解决奖励密度优化问题。具体方法包括：1）将长距离任务拆分为若干子问题，分配过程奖励，使学习信号更加密集；2）在探索失败或停滞时，注入纠错提示，提高成功概率；3）采用双智能体架构，其中一个agent负责合成搜索历史，压缩研究者感知轨迹，从而减轻深度探索负担。

Result: 在多个智能搜索基准测试中，InfoFlow显著优于现有强基线系统，并帮助轻量级大语言模型（LLMs）达到接近高级专有LLMs的性能。

Conclusion: InfoFlow有效缓解了奖励密度低导致的探索成本高、学习效率低的问题，为RLVR在深度搜索场景中的应用提供了高效可行的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.

</details>


### [76] [Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models](https://arxiv.org/abs/2510.26577)
*Yinrong Hong,Zhiquan Tan,Kai Hu*

Main category: cs.CL

TL;DR: 本文提出动态树形解码CAST，将GPU配置和batch size等系统因素纳入优化推理速度，实验表明性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型因为自回归设计和模型体量大，面临推理延迟难题。虽然已有投机解码方法，但普遍忽视如GPU设备、批大小等系统变量对推理速度的影响。

Method: 提出了一种新的动态树形解码方法CAST，该方法动态调整树结构，综合考虑推理成本（如GPU配置和batch size），提升推理效率。

Result: 在六个不同任务、六种LLM上广泛实验，CAST解码速度最高比传统方法快5.2倍，对比最新技术快5%-20%。

Conclusion: 引入系统变量分析的动态树形解码CAST，显著提升了LLM的推理速度，优于现有投机解码技术。

Abstract: Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.

</details>


### [77] [SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding](https://arxiv.org/abs/2510.26615)
*Yiqiao Jin,Rachneet Kaur,Zhen Zeng,Sumitra Ganesh,Srijan Kumar*

Main category: cs.CL

TL;DR: SlideAgent 是针对多页、多模态复杂文档理解提出的新方法，采用多级代理推理结构，有效提升了文档内容的精准理解与推理能力，实验中显著优于现有专有及开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在文档理解方面表现出色，但对于包含复杂视觉元素和跨页信息的多页可视化文档（如手册、宣传册、演示文稿和海报）仍面临挑战，特别是在细粒度推理方面。

Method: 提出了一种名为 SlideAgent 的新型代理框架，专注于多模态、多页和多布局文档的理解，尤其适用于幻灯片文档。SlideAgent 采用多代理协作，将推理划分为全局、页面和元素三个层次，并构建与查询无关的结构化表示，实现对文档主题和细节的全面捕捉。推理时，SlideAgent 按需激活不同代理进行多层推理，并将各层结果整合为上下文感知的答案。

Result: 实验结果显示，SlideAgent 在复杂多页视觉文档理解任务上，较现有专有模型与开源模型均取得了显著提升，整体性能分别领先 7.9 和 9.8。

Conclusion: SlideAgent 能有效提升多页、多模态文档中复杂视觉与文本元素的理解与推理能力，超越当前主流模型。

Abstract: Multi-page visual documents such as manuals, brochures, presentations, and
posters convey key information through layout, colors, icons, and cross-slide
references. While large language models (LLMs) offer opportunities in document
understanding, current systems struggle with complex, multi-page visual
documents, particularly in fine-grained reasoning over elements and pages. We
introduce SlideAgent, a versatile agentic framework for understanding
multi-modal, multi-page, and multi-layout documents, especially slide decks.
SlideAgent employs specialized agents and decomposes reasoning into three
specialized levels-global, page, and element-to construct a structured,
query-agnostic representation that captures both overarching themes and
detailed visual or textual cues. During inference, SlideAgent selectively
activates specialized agents for multi-level reasoning and integrates their
outputs into coherent, context-aware answers. Extensive experiments show that
SlideAgent achieves significant improvement over both proprietary (+7.9
overall) and open-source models (+9.8 overall).

</details>


### [78] [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622)
*Biao Zhang,Yong Cheng,Siamak Shakeri,Xinyi Wang,Min Ma,Orhan Firat*

Main category: cs.CL

TL;DR: 本文系统性比较了编码器-解码器与仅解码器大语言模型扩展性与性能，发现编码器-解码器模型同样适合大规模预训练且推理更高效，下游任务有优势，有望成为高效强大LLM的新选择。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型（LLM）研究从编码器-解码器架构快速转向仅解码器架构，但缺乏从扩展性（scaling）的角度进行严格的对比分析，担心编码器-解码器的潜力被忽视。

Method: 研究重访编码器-解码器大语言模型（RedLLM），并结合仅解码器架构的最新配方，进行RedLLM（用prefix LM预训练）与DecLLM（用causal LM预训练）在不同规模下的全面对比，预训练和微调都使用RedPajama V1及FLAN。比较扩展性、性能和推理效率。

Result: RedLLM表现出了优越的扩展性和强劲性能，预训练阶段虽然DecLLM总体计算更高效，但RedLLM在扩展和上下文长度推断方面表现持平。微调后，RedLLM在多个下游任务上表现同样或更好，并在推理效率上大幅领先。

Conclusion: 编码器-解码器LLM（RedLLM）在扩展性、性能和推理效率上均展现出巨大潜力，值得进一步关注和研究，为发展更强大 eficiente大语言模型指明新方向。

Abstract: Recent large language model (LLM) research has undergone an architectural
shift from encoder-decoder modeling to nowadays the dominant decoder-only
modeling. This rapid transition, however, comes without a rigorous comparative
analysis especially \textit{from the scaling perspective}, raising concerns
that the potential of encoder-decoder models may have been overlooked. To fill
this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent
recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison
between RedLLM, pretrained with prefix language modeling (LM), and DecLLM,
pretrained with causal LM, at different model scales, ranging from $\sim$150M
to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for
instruction tuning, our experiments show that RedLLM produces compelling
scaling properties and surprisingly strong performance. While DecLLM is overall
more compute-optimal during pretraining, RedLLM demonstrates comparable scaling
and context length extrapolation capabilities. After instruction tuning, RedLLM
achieves comparable and even better results on various downstream tasks while
enjoying substantially better inference efficiency. We hope our findings could
inspire more efforts on re-examining RedLLM, unlocking its potential for
developing powerful and efficient LLMs.

</details>


### [79] [Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models](https://arxiv.org/abs/2510.26683)
*Mingchen Tu,Zhiqiang Liu,Juan Li,Liangyurui Liu,Junjie Wang,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: Evontree利用少量本体规则，无需大量领域数据，提升LLMs在医疗领域表现，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗等数据敏感领域，缺乏高质量、领域专属的训练语料，导致大型语言模型（LLM）难以实现专业化适应。现有领域专家通过本体规则总结领域知识，如何利用这些本体规则提升LLMs在特定领域的表现成为难题。

Method: 提出Evontree框架，无需大量外部数据，通过少量高质量本体规则，系统性地从原始模型中提取、验证并增强领域知识。具体流程为：提取领域本体、利用两条核心本体规则检测不一致、通过自蒸馏微调强化知识。

Result: 在医学问答基准上，对比Llama3-8B-Instruct和Med42-v2，Evontree持续优于原始模型和主流有监督基线，准确率提升最高达到3.7%。

Conclusion: Evontree有效、高效且具鲁棒性，适合低资源环境下进行LLM领域适应，显著提升医学等敏感领域LLM的专用能力。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.

</details>


### [80] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/abs/2510.26692)
*Kimi Team,Yu Zhang,Zongyu Lin,Xingcheng Yao,Jiaxi Hu,Fanqing Meng,Chengyin Liu,Xin Men,Songlin Yang,Zhiyuan Li,Wentao Li,Enzhe Lu,Weizhou Liu,Yanru Chen,Weixin Xu,Longhui Yu,Yejie Wang,Yu Fan,Longguang Zhong,Enming Yuan,Dehao Zhang,Yizhi Zhang,T. Y. Liu,Haiming Wang,Shengjun Fang,Weiran He,Shaowei Liu,Yiwei Li,Jianlin Su,Jiezhong Qiu,Bo Pang,Junjie Yan,Zhejun Jiang,Weixiao Huang,Bohong Yin,Jiacheng You,Chu Wei,Zhengtao Wang,Chao Hong,Yutian Chen,Guanduo Chen,Yucheng Wang,Huabin Zheng,Feng Wang,Yibo Liu,Mengnan Dong,Zheng Zhang,Siyuan Pan,Wenhao Wu,Yuhao Wu,Longyu Guan,Jiawen Tao,Guohong Fu,Xinran Xu,Yuzhi Wang,Guokun Lai,Yuxin Wu,Xinyu Zhou,Zhilin Yang,Yulun Du*

Main category: cs.CL

TL;DR: 提出了Kimi Linear线性注意力架构，实现了比传统全注意力更高效和性能更佳，尤其在长上下文和高吞吐任务上表现突出。相比原本模型，资源用量更省，速度更快，且已开源相关代码和模型。


<details>
  <summary>Details</summary>
Motivation: 当前的全注意力机制在不同任务（如短长上下文和RL扩展等场景）计算资源消耗大，性能受限。因此需要一种高效的替代架构，能兼容更大上下文长度并提升硬件效率。

Method: 提出Kimi Linear架构，并设计了Kimi Delta Attention（KDA）模块，结合更精细的门控机制和有限状态RNN记忆。采用基于分块算法的特制Diagonal-Plus-Low-Rank（DPLR）转移矩阵，提高硬件效率并简化计算。此外模型混合了KDA和Multi-Head Latent Attention，并进行了大规模预训练。

Result: Kimi Linear在公平对比测试中，性能全面超越现有全注意力模型，包括短上下文、长上下文和强化学习扩展任务。KV缓存减少最高75%，在百万级上下文下解码速度提升达6倍。模型可无缝替换现有全注意力架构，并支持更长输入输出长度任务。

Conclusion: Kimi Linear架构可成为全注意力机制的高效替代方案，兼具更优秀的性能与硬件效率，显著推动长序列建模和大模型推理。相关内核和模型已开源，有助于推进线性注意力研究和应用。

Abstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.

</details>


### [81] [The End of Manual Decoding: Towards Truly End-to-End Language Models](https://arxiv.org/abs/2510.26697)
*Zhichao Wang,Dongyang Ma,Xinting Huang,Deng Cai,Tian Lan,Jiahao Xu,Haitao Mi,Xiaoying Tang,Yan Wang*

Main category: cs.CL

TL;DR: AutoDeco通过让LLM自我调节解码参数，实现端到端生成，在性能和可控性上优于现有解码方式，并可根据自然语言指令动态调整生成策略。


<details>
  <summary>Details</summary>
Motivation: 目前大多数大语言模型（LLM）在生成过程中依赖于非可微分的解码过程，需要手动调参（如temperature和top-p），这不够自动化且难以实现真正的端到端方法。

Method: 提出了一种新架构AutoDeco，在标准Transformer上增加轻量级模块，使模型在每个生成步骤动态预测context-specific的temperature和top-p参数，并与下一个token的logits一起输出，从而使解码变为可参数化、可自我调节的过程。

Result: AutoDeco在8个基准测试中显著优于默认解码策略，并接近由“测试集调参”的oracle上界。同时，模型展现出理解自然语言指令（如“生成低随机性内容”）并据此调整解码参数的能力，实现了可交互、可引导的解码。

Conclusion: AutoDeco实现了真正端到端的LLM生成过程，开辟了可控与交互式高效解码的新范式。

Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly "end-to-end" generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
"hacking the test set"-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
"generate with low randomness") and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.

</details>


### [82] [Value Drifts: Tracing Value Alignment During LLM Post-Training](https://arxiv.org/abs/2510.26707)
*Mehar Bhatia,Shravan Nayak,Gaurav Kamath,Marius Mosbach,Karolina Stańczak,Vered Shwartz,Siva Reddy*

Main category: cs.CL

TL;DR: 本论文系统分析了LLMs在后训练过程中的价值对齐形成机制，发现SFT阶段对价值建立至关重要，偏好优化算法和数据的选择也会影响最终对齐效果，为模型和数据的选择提供参考。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在社会中变得越来越重要，如何让它们不仅具备通识知识，还能更好地契合人类价值体系，成为关键问题。此前研究主要关注评价模型最终的价值对齐情况，忽略了模型在训练过程中是如何学习人类价值的。

Method: 论文对模型在后训练（post-training）过程中价值对齐的形成阶段与方式进行了深入分析。具体实验对象包括不同规模的Llama-3和Qwen-3模型，以及主流的监督微调（SFT）与偏好优化的数据集和算法。通过测量训练中价值变化程度和时间，分别考察了后训练算法与数据集对模型价值漂移的影响。同时利用可控的偏好合成数据集，比较了不同偏好优化算法在固定数据下的价值对齐结果差异。

Result: 实验发现，监督微调阶段基本决定了模型的价值取向，而后续的偏好优化过程很少重新调整这些价值。此外，即使在相同的偏好数据下，不同的偏好优化算法也会导致价值对齐结果的变化。

Conclusion: 研究揭示了LLMs在后训练过程中价值习得的机制，并提出了可操作性建议，可用于优化模型后训练数据的选取及算法选择，从而提升模型向人类价值的对齐效果。

Abstract: As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.

</details>


### [83] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench提出了一套更高难度的数学推理基准，显著提升了对顶级LLM评测的挑战性。现有模型在该基准上的表现低于预期，凸显模型推理能力仍有较大提升空间。该基准将推动相关领域的进一步研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在高水平数学推理上的评估已近表现饱和，现有竞赛题库难以进一步区分模型能力。因此需要更严苛、更具挑战性的评测基准来推动顶级模型的发展。

Method: 提出了AMO-Bench，这是一套包含50道由专家设计、符合国际奥林匹克数学竞赛难度标准的原创高难度题目。所有题目均为全新且只要求最终答案，以便自动化、可靠评测。通过26个LLM模型进行实证测试。

Result: 在AMO-Bench上，最佳模型准确率仅为52.4%，大多数模型低于40%。分析发现，在提升测试时算力的情况下，模型性能有增长趋势。

Conclusion: AMO-Bench有效揭示了当前顶级LLM在高阶数学推理上的不足，为促进未来模型推理能力提升提供了新的基准和方向。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


### [84] [Gistify! Codebase-Level Understanding via Runtime Execution](https://arxiv.org/abs/2510.26790)
*Hyunji Lee,Minseon Kim,Chinmay Singh,Matheus Pereira,Atharv Sonwane,Isadora White,Elias Stengel-Eskin,Mohit Bansal,Zhengyan Shi,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan,Lucas Caccia*

Main category: cs.CL

TL;DR: 本文提出Gistify任务，考察编码大模型能否从完整代码库中提取并复现指定功能，结果显示当前模型难以胜任，尤其在处理复杂执行流程时表现有限，为未来提升AI对大型代码库理解能力提供了新评测标准。


<details>
  <summary>Details</summary>
Motivation: 随着编码智能体在大型代码库中的应用增多，迫切需要对这些模型进行更具挑战性的自动化评估，以反映真实应用场景。现有评测多针对单一解题或补全任务，未能覆盖对代码整体结构和执行流程理解的能力。本文提出新评测任务以弥补此空白。

Method: 本文提出Gistify任务：向编码大模型（LLM）提供完整代码库和特定入口命令，要求其生成包含必要组件的最小自包含文件，能正确复现入口命令的输出。评估模型重构代码功能与还原执行流程能力。

Result: 实验发现，当前最先进的编码大模型在Gistify任务上表现不佳，尤其是处理需要较长执行追踪的大型任务时，表现尤为欠佳。这说明这些模型对代码库结构和执行流程的理解仍有限。

Conclusion: Gistify任务为评估编码大模型在理解和重现大型代码库功能方面提供了新的挑战性基准，揭示了现有模型的局限性，并为进一步提升模型理解能力指明了方向。

Abstract: As coding agents are increasingly deployed in large codebases, the need to
automatically design challenging, codebase-level evaluation is central. We
propose Gistify, a task where a coding LLM must create a single, minimal,
self-contained file that can reproduce a specific functionality of a codebase.
The coding LLM is given full access to a codebase along with a specific
entrypoint (e.g., a python command), and the generated file must replicate the
output of the same command ran under the full codebase, while containing only
the essential components necessary to execute the provided command. Success on
Gistify requires both structural understanding of the codebase, accurate
modeling of its execution flow as well as the ability to produce potentially
large code patches. Our findings show that current state-of-the-art models
struggle to reliably solve Gistify tasks, especially ones with long executions
traces.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [85] [On the number of non-degenerate canalizing Boolean functions](https://arxiv.org/abs/2510.26556)
*Claus Kadelka*

Main category: cs.DM

TL;DR: 本文推导出递归公式，精确计数具有导向性和变量重要性的布尔函数，为复杂系统的鲁棒性和层次结构分析提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统（尤其是基因调控网络）中，canalization（导向性）和degeneracy（退化性）影响系统的稳定性和动态特性。两者分别体现变量的主导性和冗余性，对功能输出产生层次性和抗扰性。但各自的组合及具体计数方式尚不完全明晰。

Method: 推导递归公式，对具备特定数量重要变量且具有预设导向性属性的布尔函数进行计数；特别计算所有变量重要且至少一个变量有导向性的非退化性导向布尔函数数目。该方法扩展了早期关于导向和嵌套导向布尔函数的计数结果。

Result: 成功得到上述递归公式，精确定量了具特定导向性属性的布尔函数数量，尤其确定了非退化性导向布尔函数的数目。

Conclusion: 本文为随机布尔函数中导向性出现的频率，以及在生物网络模型中导向性显著过度表达、增强鲁棒性和调控分工的现象，提供了严谨的定量基础。

Abstract: Canalization is a key organizing principle in complex systems, particularly
in gene regulatory networks. It describes how certain input variables exert
dominant control over a function's output, thereby imposing hierarchical
structure and conferring robustness to perturbations. Degeneracy, in contrast,
captures redundancy among input variables and reflects the complete dominance
of some variables by others. Both properties influence the stability and
dynamics of discrete dynamical systems, yet their combinatorial underpinnings
remain incompletely understood. Here, we derive recursive formulas for counting
Boolean functions with prescribed numbers of essential variables and given
canalizing properties. In particular, we determine the number of non-degenerate
canalizing Boolean functions -- that is, functions for which all variables are
essential and at least one variable is canalizing. Our approach extends earlier
enumeration results on canalizing and nested canalizing functions. It provides
a rigorous foundation for quantifying how frequently canalization occurs among
random Boolean functions and for assessing its pronounced over-representation
in biological network models, where it contributes to both robustness and to
the emergence of distinct regulatory roles.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [86] [Unambiguous Acceptance of Thin Coalgebras](https://arxiv.org/abs/2510.26371)
*Anton Chernev,Corina Cîrstea,Helle Hvid Hansen,Clemens Kupke*

Main category: cs.FL

TL;DR: 本文将不歧义自动机构造从thin trees扩展到了analytic functors下的thin coalgebras，并通过余代数框架提供理论澄清，推导出相关语言的自动机判别方式。


<details>
  <summary>Details</summary>
Motivation: 在反应式系统验证中，不歧义自动机因其扩展了确定性自动机的类且保留了一些有益性质而应用广泛。传统上，这类自动机的构造主要针对thin trees。本研究希望将相关构造推广到更广泛的结构（如analytic functors下的thin coalgebras），并通过余代数（coalgebraic）理论提供清晰及参数化的理解。

Method: 采用coalgebraic理论，形式化并推广了不歧义自动机构造，建立了自动机与coherent algebras之间的连接，完成了理论刻画。

Result: 研究实现了将不歧义自动机构造推广至analytic functors下的thin coalgebras；通过构建，建立了自动机接受thin coalgebras语言与通过coherent algebras进行语言识别的联系，并由此得出由有限coherent algebra所识别的语言的自动机理论刻画。

Conclusion: 通过提供一个余代数框架下的参数化建模方法，本文促进了不歧义自动机的泛化构造，不仅覆盖了更大类结构，也加深了与coherent algebra的关系，从而为相关语言识别给出了自动机理论的刻画工具。

Abstract: Automata admitting at most one accepting run per structure, known as
unambiguous automata, find applications in verification of reactive systems as
they extend the class of deterministic automata whilst maintaining some of
their desirable properties. In this paper, we generalise a classical
construction of unambiguous automata from thin trees to thin coalgebras for
analytic functors. This achieves two goals: extending the existing construction
to a larger class of structures, and providing conceptual clarity and
parametricity to the construction by formalising it in the coalgebraic
framework. As part of the construction, we link automaton acceptance of
languages of thin coalgebras to language recognition via so-called coherent
algebras, which were previously introduced for studying thin coalgebras. This
link also allows us to establish an automata-theoretic characterisation of
languages recognised by finite coherent algebras.

</details>
