<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 64]
- [cs.DM](#cs.DM) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: 本文提出了Prompt Decorators，通过声明式、可组合的语法简化和标准化大语言模型的提示设计，实现行为的可控、透明与标准化，具有良好的扩展性和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）在推理、表达和决策支持中应用广泛，但用户很难一致地控制其推理和输出风格。而传统的提示工程依赖冗长的自然语言指令，导致复现性差、可扩展性低、难以理解和模块化。

Method: 提出Prompt Decorators，一种声明式和可组合的语法，通过精简的控制标记（如+++Reasoning、+++Tone(style=formal)等）来调控LLM行为。该方法将20个核心装饰器归为两个功能类别，并定义统一的语法、作用域和处理流程，实现可预测、可审计的行为组合。

Result: Prompt Decorators能够分离任务意图和执行行为，实现复用和可解释的提示设计接口。案例实验显示，采用该方法能提高推理透明度、减少提示复杂性，并在不同领域标准化模型行为。

Conclusion: 该方法提升了兼容性、一致性，并为可扩展AI系统的声明式接口开发提供了新思路。

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


### [2] [A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification](https://arxiv.org/abs/2510.19853)
*Assaf Marron,David Harel*

Main category: cs.PL

TL;DR: 本文提出算法规范realm的概念，系统归纳实现算法所需的基础知识，并探讨自动化生成及标准化执行的可能性，对方法论和验证具有启发意义。


<details>
  <summary>Details</summary>
Motivation: 在算法规范主要用自然语言或伪代码表达时，如何确保其能够被人或机器清晰、明确地执行，是一个基础但尚未系统化的问题。该论文旨在定位执行代理需要具备的知识，以独立于任何系统实现下执行算法规范。

Method: 作者提出用一个名为“realm”的文档来归纳算法规范所需的所有知识，包括规范语言的语法和语义、涉及实体的领域知识、实体间关系、因果规则及操作说明，提出了生成realm的系统性分析流程并探讨部分自动化方案（如利用大语言模型和已有文档复用）。

Result: 论文初步描述了realm的内容和生成方式，并指出该描述能促进算法规范在不同系统中的方法化实现及实现形式化验证。另外分析了“执行忠实度”与算法“正确性”的区分，探讨了无参考解释时，如何判断执行代理的行为是否符合输入规范。

Conclusion: 该文提出并定义了算法规范realm的概念，为算法执行所需预备知识的汇总提供了新路径，有助于规范实施与验证方法论的发展。与此同时，指出对“执行忠实度”的评估为未来进一步研究方向。

Abstract: An algorithm specification in natural language or pseudocode is expected to
be clear and explicit enough to enable mechanical execution. In this position
paper we contribute an initial characterization of the knowledge that an
executing agent, human or machine, should possess in order to be able to carry
out the instructions of a given algorithm specification as a stand-alone
entity, independent of any system implementation. We argue that, for that
algorithm specification, such prerequisite knowledge, whether unique or shared
with other specifications, can be summarized in a document of practical size.
We term this document the realm of the algorithm specification. The generation
of such a realm is itself a systematic analytical process, significant parts of
which can be automated with the help of large language models and the reuse of
existing documents. The algorithm-specification's realm would consist of
specification language syntax and semantics, domain knowledge restricted to the
referenced entities, inter-entity relationships, relevant underlying
cause-and-effect rules, and detailed instructions and means for carrying out
certain operations. Such characterization of the realm can contribute to
methodological implementation of the algorithm specification in diverse systems
and to its formalization for mechanical verification. The paper also touches
upon the question of assessing execution faithfulness, which is distinct from
correctness: in the absence of a reference interpretation of natural language
or pseudocode specification with a given vocabulary, how can we determine if an
observed agent's execution indeed complies with the input specification.

</details>


### [3] [Deconstructed Proto-Quipper: A Rational Reconstruction](https://arxiv.org/abs/2510.20018)
*Ryan Kavanagh,Chuta Sano,Brigitte Pientka*

Main category: cs.PL

TL;DR: 本文提出 Proto-Quipper-A，通过线性λ-演算和附加对偶逻辑简化 Proto-Quipper 的语义，证明了其归约归一性，为未来 Proto-Quipper 语言的理论和应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: Proto-Quipper 家族旨在为 Quipper 量子编程语言提供形式化基础，但其操作语义复杂，难以通过标准编程语言技术推理和机械化。需要简化并形式化用于静态电路生成的编程模型。

Method: 提出 Proto-Quipper-A，一种基于线性λ-演算的静态电路生成语言，通过附加对偶逻辑的基础，将量子电路语言与线性/非线性函数语言整合，同时简化语义，引入简单的逐值求值归约语义，并利用标准的逻辑关系方法证明归约归一性。

Result: Proto-Quipper-A 拥有简单的归约语义，并被证明是归约归一的。文中展示了如何利用标准逻辑关系证明系统的归一性，无需现有复杂的线性逻辑关系。

Conclusion: Proto-Quipper-A 简化了 Proto-Quipper 的语义与推理难度，成为 Proto-Quipper 语言良好的理论基础，并证明其在静态电路生成上的可行性和可机械化性。

Abstract: The Proto-Quipper family of programming languages aims to provide a formal
foundation for the Quipper quantum programming language. Unfortunately,
Proto-Quipper languages have complex operational semantics: they are inherently
effectful, and they rely on set-theoretic operations and fresh name generation
to manipulate quantum circuits. This makes them difficult to reason about using
standard programming language techniques and, ultimately, to mechanize. We
introduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages
for static circuit generation. It uses a linear $\lambda$-calculus to describe
quantum circuits with normal forms that closely correspond to box-and-wire
circuit diagrams. Adjoint-logical foundations integrate this circuit language
with a linear/non-linear functional language and let us reconstruct
Proto-Quipper's circuit programming abstractions using more primitive
adjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value
reduction semantics, and to illustrate its tractability as a foundation for
Proto-Quipper languages, we show that it is normalizing. We show how to use
standard logical relations to prove normalization of linear and substructural
systems, thereby avoiding the inherent complexity of existing linear logical
relations.

</details>


### [4] [Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism](https://arxiv.org/abs/2510.20532)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 本文提出一种新的类型与效应系统推断算法，兼具高表现力和直观性，并验证了其实用性和理论正确性，为该领域推广提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 类型与效应系统虽然有助于程序员组织数据和计算效应，但由于其复杂性和现有推断算法在表现力、直观性和可判定性之间的权衡，尚未被广泛采用。本文旨在解决类型与效应系统的推断与实用难题，推动其在编程语言中的应用。

Method: 提出了一种适用于具有子类型、具表现力的高阶多态和直观集合集合语义的类型与效应系统的效应推断算法。通过把效应约束转换为命题逻辑公式，延迟高阶多态体作用域问题的求解。此外，算法的正确性和完备性在Rocq证明助理中进行了形式化证明，并在真实编程语言中实现验证。

Result: 本文算法在理论（通过形式化证明）及实际（真实编程语言实现）层面均被证明有效，能够为复杂的类型与效应系统实现表现力强且直观的效应推断。

Conclusion: 通过提出的新效应推断算法，克服了类型与效应系统引入实际编程语言中的阻碍，为类型与效应系统的进一步应用和研究打下坚实基础。本文兼顾了推断的表现力、直观性及判定性，为推广类型与效应系统创造了条件。

Abstract: Type-and-effect systems help the programmer to organize data and
computational effects in a program. While for traditional type systems
expressive variants with sophisticated inference algorithms have been developed
and widely used in programming languages, type-and-effect systems did not yet
gain widespread adoption. One reason for this is that type-and-effect systems
are more complex and the existing inference algorithms make compromises between
expressiveness, intuitiveness, and decidability. In this work, we present an
effect inference algorithm for a type-and-effect system with subtyping,
expressive higher-rank polymorphism, and intuitive set-like semantics of
effects. In order to deal with scoping issues of higher-rank polymorphism, we
delay solving of effect constraints by transforming them into formulae of
propositional logic. We prove soundness and completeness of our algorithm with
respect to a declarative type-and-effect system. All the presented results have
been formalized in the Rocq proof assistant, and the algorithm has been
successfully implemented in a realistic programming language.

</details>


### [5] [Compiling the Mimosa programming language to RTOS tasks](https://arxiv.org/abs/2510.20547)
*Nikolaus Huber,Susanne Graf,Philipp Rümmer,Wang Yi*

Main category: cs.PL

TL;DR: 本文提出了针对Mimosa嵌入式编程语言的编译方案，通过形式化适配Lustre编译方法和实时系统，实现了高效支持时间触发进程及其通信。


<details>
  <summary>Details</summary>
Motivation: 为了解决嵌入式系统中时间触发进程的编程与编译问题，作者提出了一种基于MIMOS计算模型的Mimosa语言编译方案。该方案可以更好地支持进程间通过FIFO队列通信的需求。

Method: 形式化地将Lustre编译方案调整适配到Mimosa语言的语义，并展示如何将协调层映射到实时操作系统的基本原语。

Result: 成功实现了Mimosa语言中时间触发和FIFO通信进程的编译，并能够将协调逻辑映射到实时操作系统，证明了方案的可行性。

Conclusion: 该方案为Mimosa语言开发嵌入式系统软件提供了高效的编译和运行机制，提升了对时间触发式进程以及通信的支持。

Abstract: This paper introduces a compilation scheme for programs written in the Mimosa
programming language, which builds upon the MIMOS model of computation. Mimosa
describes embedded systems software as a collection of time-triggered processes
which communicate through FIFO queues. We formally describe an adaptation of
the Lustre compilation scheme to the semantics of Mimosa and show how the
coordination layer can be mapped to real-time operating system primitives.

</details>


### [6] [SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications](https://arxiv.org/abs/2510.20688)
*Oliver Braunsdorf,Tim Lange,Konrad Hohentanner,Julian Horsch,Johannes Kinder*

Main category: cs.PL

TL;DR: SafeFFI通过只在unsafe与safe代码边界进行内存安全检测，显著提升了Rust程序的性能，大幅减少检测数量，同时仍能有效发现所有内存安全问题。


<details>
  <summary>Details</summary>
Motivation: Rust与C/C++库互操作需要unsafe代码，会引入内存安全问题，传统检测方案效率低下，急需优化检测手段以兼顾安全和性能。

Method: 提出一种优化内存安全检测的系统SafeFFI，在Rust二进制文件中将检测集中于unsafe与safe代码的边界处，不需要全程序分析。

Result: SafeFFI在性能上优于现有方法，编译时间开销显著降低（2.64x vs 8.83x），在已知漏洞代码和常用Rust库中减少了98%的检测，同时没有遗漏任何内存安全违规情况。

Conclusion: SafeFFI 能有效减少不必要的内存安全检测，同时保持对所有空间和时间内存安全违规的正确检测。

Abstract: Unsafe Rust code is necessary for interoperability with C/C++ libraries and
implementing low-level data structures, but it can cause memory safety
violations in otherwise memory-safe Rust programs. Sanitizers can catch such
memory errors at runtime, but introduce many unnecessary checks even for memory
accesses guaranteed safe by the Rust type system. We introduce SafeFFI, a
system for optimizing memory safety instrumentation in Rust binaries such that
checks occur at the boundary between unsafe and safe code, handing over the
enforcement of memory safety from the sanitizer to the Rust type system. Unlike
previous approaches, our design avoids expensive whole-program analysis and
adds much less compile-time overhead (2.64x compared to over 8.83x). On a
collection of popular Rust crates and known vulnerable Rust code, SafeFFI
achieves superior performance compared to state-of-the-art systems, reducing
sanitizer checks by up to 98%, while maintaining correctness and flagging all
spatial and temporal memory safety violations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: LLMs能较好生成访问控制策略，但易出现许可过宽的问题。与符号方法结合，可有效分析和总结现有策略，对提升云安全管理有现实价值。


<details>
  <summary>Details</summary>
Motivation: 手写云服务访问控制策略复杂且易出错，现有规则难以分析其实际效果，借助LLM自动生成和解读提升安全和效率。

Method: 评估了多种LLM在访问控制策略合成和摘要方面的能力，提出了基于语义的请求摘要方法，将LLM与符号推理结合用于策略分析。

Result: 非推理型LLM生成策略与参考规范等价概率为45.8%，推理型为93.7%。自动生成仍有挑战，但辅助分析现有策略结合符号方法效果显著。

Conclusion: LLMs在接入控制策略自动生成方面存在一定局限，但结合符号方法后在策略分析方面表现出色。

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>


### [8] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: E-Test利用LLM自动识别和生成覆盖生产场景的测试用例，显著提升测试套件覆盖率，F1分数优于现有方法，降低人工维护成本。


<details>
  <summary>Details</summary>
Motivation: 测试套件本质上是有缺陷的，难以覆盖所有可能的执行场景，尤其是在管理大型且长期运行的测试套件时，发现边界和未覆盖场景的测试用例非常耗时且困难。

Method: 提出了一种名为E-Test的新方法，通过利用大规模语言模型（LLMs）分析生产环境中出现的执行场景，从而识别测试套件未覆盖的场景，并自动生成相应的测试用例扩充原有测试套件。E-Test具有两个关键流程：（1）利用生产和监控数据识别未被测试的场景；（2）基于LLMs生成新的测试用例填补覆盖缺口。

Result: 通过在实际开源Java项目和Defects4J中收集的1975个场景上进行评估，E-Test相比现有的回归测试和现场测试方法，在未测试执行场景的检索上显著优于对比方法。具体来看，现有方法F1分数最高为0.34，普通LLM方法最高为0.39，而E-Test达到了0.55。

Conclusion: E-Test能显著提升测试套件对生产环境中真实执行场景的覆盖率，减少人工维护测试套件的工作量，在增强测试套件质量及软件可靠性方面效果优异。

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [9] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 本文提出了AI驱动的电子表格操作文档（SOD）任务，证明大型语言模型能较好地将表格操作代码转化为自然语言说明，为提升电子表格文档化与协作奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 当前，知识工作者广泛使用电子表格进行商业、会计和金融相关工作，但由于缺乏系统的文档记录方法，导致自动化、协作和知识传递受阻，甚至可能造成重要机构知识的流失。

Method: 本文提出了电子表格操作文档（SOD）这一AI任务，通过生成电子表格操作的人类可读说明，评估五种大型语言模型（GPT-4o、GPT-4o-mini、LLaMA-3.3-70B、Mixtral-8x7B、Gemma2-9B）在SOD任务上的表现，并应用BLEU、GLEU、ROUGE-L、METEOR等指标。具体地，作者构建了包含111个代码片段及其自然语言描述的基准数据集。

Result: 多种大型语言模型能够较为准确地生成电子表格操作的文档说明，验证了SOD作为提升电子表格可复现性、可维护性和协作性的可行性。

Conclusion: 大型语言模型在电子表格文档自动生成中表现良好，为后续增强电子表格领域的知识传承与协作流程提供了可能性，但仍存在诸多挑战有待解决。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [10] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: KGACG框架通过多智能体协作和反馈机制，将应用需求与架构自动转化为可执行代码，有效提升大型软件项目的开发效率和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 当前由大语言模型（LLMs）驱动的自动化代码生成提升了开发效率，但在处理复杂的应用级软件代码时仍存在挑战，多智能体框架虽有潜力，却在大规模代码生成和合理组织项目结构等方面表现不佳。

Method: 提出了一种知识驱动的应用级代码生成框架KGACG，将软件需求说明和架构设计文档通过协同闭环处理（包含代码组织与规划智能体COPA、编码智能体CA、测试智能体TA，以及反馈机制）转化为可执行代码。

Result: 在Java坦克大战游戏的案例研究中，展示了KGACG中各智能体之间的协同过程，并面对相关挑战。

Conclusion: KGACG框架致力于推动应用级软件开发自动化，尤其在复杂项目需求及架构转代码的过程中提升自动化程度和代码组织合理性。

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [11] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 1. 提出了一种让SWE智能体在引入新特性过程中“无心”制造bug的合成方法，贴合真实开发。2. 该方法生成的数据效率高、质量优，显著提升了语言模型在SWE-bench上的表现。


<details>
  <summary>Details</summary>
Motivation: 高质量的bug对于训练下一代基于大语言模型的软件工程（SWE）智能体至关重要。然而，现有的bug生成方法往往与真实开发过程脱节，难以模拟实际开发人员在引入新特性时出现的真实bug模式。该文旨在提出更真实有效的bug合成方法。

Method: 本文提出一种新颖的方法，指示SWE智能体在向代码库引入新特性时无意中破坏测试，从而产生bug。这种方法能更真实地模拟开发过程，与传统通过局部扰动生成bug的方法相比，更贴近人类开发行为。

Result: 通过定性分析，证明该方法生成的bug与人类编辑的bug更为接近。实验表明，该方法生成的bug数据训练效率更高，仅用一半的数据量（1.2k对比3k）就能使微调性能提升2%。基于本方法训练的新模型FrogBoss（32B参数）和FrogMini（14B参数）都在SWE-bench Verified测试集上取得了当前最优成绩。

Conclusion: 该工作提出了一种更贴近真实开发流程的bug合成方法，显著提升了基于大语言模型的软件工程智能体在bug修复任务中的性能。其生成的数据能更高效地用于模型训练，对相关研究和实际应用具有重要意义。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [12] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 本文提出MuoFuzz，通过学习变异器序列概率，实现更高效模糊测试。实验表明该方法获得更高代码覆盖率并挖掘出更多漏洞，优于AFL++和MOPT。


<details>
  <summary>Details</summary>
Motivation: 现有灰盒模糊测试器通常采用随机或独立概率选择变异器，忽略了变异器顺序间的联合影响。作者猜测按不同顺序组合变异器可能对生成高覆盖率测试用例有显著影响，进而探索一种能自适应学习最优变异器应用顺序的方法，为提升模糊测试效率提供新思路。

Method: 本文通过实验分析所有可能的变异器（mutator）对的有效性，并拟合线性模型，探究变异器应用顺序对灰盒模糊测试器（greybox fuzzer）有效性的影响。随后提出了一种新模糊测试器MuoFuzz，该方法能根据历史变异器选择情况自适应学习最优变异器序列，并通过概率随机游走选择下一步变异器。与AFL++和MOPT等主流方法对比，在FuzzBench和MAGMA基准上评价性能。

Result: 实验结果显示：(1) 变异器顺序确实存在交互效应，对模糊测试效果有影响；(2) 所提MuoFuzz在FuzzBench和MAGMA基准数据集上代码覆盖率最高，并在漏洞发现方面优于AFL++和MOPT，能额外找到AFL++未发现的4个漏洞，以及AFL++和MOPT都未发现的1个漏洞。

Conclusion: 本文证明了变异器顺序对灰盒模糊测试器有效性有显著影响，并提出MuoFuzz，通过自适应学习变异器序列显著提升测试效果。实验证明该方法优于现有主流模糊测试工具。

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [13] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: 该论文针对生成式人工智能在中型和大型组织中的落地难题，提出了专属的FAIGMOE框架，弥补了现有技术采纳理论的不足，涵盖全过程指导，并辅以实用工具，但需进一步实证验证。


<details>
  <summary>Details</summary>
Motivation: 目前主流技术采纳理论（如TAM、TOE、DOI）在GenAI应用场景下缺乏针对性，尤其无法兼顾中型组织的资源与能力约束，以及大型企业的组织复杂性。因此亟需一种专门面向GenAI、多规模组织的治理与落地框架。

Method: 本文采用理论综合方法，将技术采纳理论、组织变革管理和创新扩散视角整合为四大阶段性框架，并针对不同规模组织的具体需求进行设计。

Result: 提出了FAIGMOE框架，分为战略评估、规划与用例开发、实施与集成、运营与优化四阶段，针对组织规模和复杂性提供可拓展的指导，并纳入具体的GenAI要素（如提示工程、模型编排、幻觉管理），填补了当前采纳文献的空白。

Conclusion: FAIGMOE框架为中型及大型企业采用生成式人工智能（GenAI）提供了首个综合性理论指导，覆盖从战略评估到落地优化的整个流程，并包含可操作的实施协议与治理模板。

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [14] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: 将Kubernetes及其他项目的构建工具从Bazel降级到Go Build后，整体维护变易但构建性能损失明显，资源消耗增加。基于制品的工具虽难维护但性能优于语言特定方案。


<details>
  <summary>Details</summary>
Motivation: 构建系统的性能直接影响开发者生产效率。虽然现代基于制品（artifact-based）的构建工具加快了构建速度，但维护难度使一些开发团队弃用它们，转而采用更易维护的解决方案。此前虽有相关研究，但降级后的真正影响仍缺乏深入探讨。

Method: 对Kubernetes项目从Bazel（基于制品的构建工具）降级到Go Build（语言特定的构建工具）进行了案例研究。重现并分析降级期间变更集的全量和增量构建性能。同时在四个其他项目上进行了复现对比实验。

Result: Bazel的构建速度快于Go Build，但其内存占用和CPU负载更高。降级后构建时间增加，但内存消耗下降。估算降级可能导致CI资源成本提升高达76%。在其他被降级的项目上也有类似现象，Bazel持续表现出更高的内存消耗。

Conclusion: 弃用基于制品的构建工具，尽管提高了可维护性，但对大型项目来说，往往会带来显著的性能损失。相关观察结果有助于利益相关方权衡构建工具选型的利弊。

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [15] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: 本文提出并实现了一种基于模型驱动的遗留系统代码迁移方案，有效实现了PL/SQL向Java的迁移，提升了代码结构与可维护性。


<details>
  <summary>Details</summary>
Motivation: 现代软件技术的成功促使许多企业希望将原有的RAD应用（如Oracle Forms）迁移到更现代的架构和语言（如Java）。本文旨在解决如何有效地将PL/SQL单体代码迁移到分层结构的Java代码。

Method: 提出了一种基于模型驱动的再工程方法，将遗留PL/SQL代码转换为Java。通过KDM模型表达遗留代码，并采用类似TDD（测试驱动开发）的方法，逐步开发模型转换，并对生成代码进行三类验证。

Result: 实现了PL/SQL到Java的迁移工具，并详细说明了再工程方法的实现和验证过程。同时评估了应用模型驱动工程过程中的若干问题。

Conclusion: 模型驱动再工程方法不仅可以迁移遗留系统，还能保证迁移后代码的可维护性和质量，取得了良好的迁移和验证效果。

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [16] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: NSync系统利用大模型解释云API调用，实现对基础设施漂移的检测和自动修正，将漂移变更同步回IaC配置。实验表明NSync显著优于传统方法，准确率和效率均有提升。


<details>
  <summary>Details</summary>
Motivation: 目前主流云基础设施管理工具包括控制台、CLI、SDK和越来越流行的IaC（基础设施即代码）框架如Terraform。然而，IaC框架在与其他工具同时使用时，难以感知外部变更，导致“基础设施漂移”，使IaC配置文件信息过时，后续操作易出错或撤销有效更改。解决这一问题十分重要。

Method: 提出了NSync系统，通过监控云API调用，自动检测和感知不是通过IaC发生的基础设施变更（即漂移），并基于API日志利用大语言模型（LLM）推断高层次意图，自动生成并更新IaC配置。此外，系统设计了可注入真实漂移的评测框架，用以系统性测试漂移检测和修正能力。

Result: 在五个真实Terraform项目和372个基础设施漂移场景下，与现有基线方法相比，NSync在准确率上提升显著（pass@3从0.71提高到0.97），同时提高了token使用效率（提升1.47倍）。

Conclusion: NSync能有效检测和纠正IaC与实际基础设施之间的漂移，有效改善多工具混用下的基础设施一致性问题，并且性能（准确率和资源效率）比现有方法更优。

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [17] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: 提出并实现了Classport，用于Java的运行时依赖检测，在实际项目中验证了其有效性，有助于提升软件供应链安全。


<details>
  <summary>Details</summary>
Motivation: Java缺乏对运行时依赖内省的支持，而运行时观察依赖对于软件供应链安全至关重要，因此亟需解决这一问题。

Method: 通过将依赖信息嵌入到Java类文件中，Classport能在程序运行时检索当前所用的依赖。

Result: 在六个真实项目上评估了Classport，验证了系统的可行性和有效性，可以在运行时识别依赖关系。

Conclusion: Classport系统可以有效地在Java运行时检测和检索依赖信息，为软件供应链安全中的运行时完整性检查提供了新的可能性。

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [18] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: 论文提出结构规则和系统对称性是软件平台保持架构鲁棒性的关键，通过一致性接口和行为在变换下为系统提供稳定性。


<details>
  <summary>Details</summary>
Motivation: 探讨软件平台通过结构保留与对称性变换，提升架构稳定性和健壮性的原理。

Method: 通过分析软件平台如何作为结构保持系统，研究其在特定变换（称为对称性）下的一致性接口与行为。

Result: 发现结构规则的维护有助于提升平台的架构鲁棒性。

Conclusion: 架构的鲁棒性源于对结构规则的持续维护。

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [19] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: 提出了一种增强网络安全和IP保护的分布式协同仿真方案，并在多种网络环境下验证了其实用性与性能。


<details>
  <summary>Details</summary>
Motivation: 分布式协同仿真有助于不同利益相关方的协同建模与仿真，同时保护其知识产权（IP），但目前缺乏关于在完全无泄露风险下，进行连续时间或混合系统分布式协同仿真的明确指导规范。

Method: 提出了一种基于UniFMU的分布式协同仿真方法，该方法增强了网络安全性和知识产权保护机制，确保连接由客户端发起、所有模型和二进制文件均运行在可信平台上。通过在四种不同网络环境下，用两个协同仿真案例进行展示和功能验证。

Result: 实验证明了该方法在不同网络设置下的功能有效性，并分析了在保护知识产权与分布式协同性能之间的权衡。

Conclusion: 基于UniFMU的分布式协同仿真方法能有效增强网络安全性和知识产权保护，适用于不同网络环境。但在保护IP和提高性能之间存在平衡需求。

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [20] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: 本文通过访谈分析演绎验证应用的障碍与促进因素，并据此为相关实践者提出改进建议，旨在推动该技术的普及。


<details>
  <summary>Details</summary>
Motivation: 尽管演绎验证在特定项目中已显示出有效性，但尚未成为主流技术。作者希望通过系统性研究，推动演绎验证在更广泛领域的实际应用。

Method: 通过对来自工业界和学术界的30位验证实践者进行半结构化访谈，并采用主题分析法系统分析所得数据。

Result: 除了证实高专业门槛等常见挑战外，还发现了如证明维护、自动化控制不足及可用性问题等此前未被充分关注的障碍，并据此给出了促进演绎验证应用的建议。

Conclusion: 该研究总结了促进和阻碍演绎验证广泛应用的因素，并为行业实践者、工具开发者和研究人员提出了具体的改进建议。

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [21] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: 本文系统评估了多种大语言模型在代码故障定位任务上的表现，特别关注不同提示方式对准确性、效率和成本的影响。结果显示，选用合理的上下文和提示策略能提高模型表现，但不同方法和模型间存在明显性能差异与权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在自动程序修复等代码相关任务表现出色，但其修复效果高度依赖于前置的故障定位步骤，而目前针对这一步的全面评估还比较缺乏。

Method: 系统性实证研究，评估代表性开源（Qwen2.5-coder-32b-instruct，DeepSeek-V3）和闭源（GPT-4.1 mini，Gemini-2.5-flash）LLM在语句级代码故障定位任务上的能力，使用HumanEval-Java和Defects4J数据集，并分析不同提示策略（标准提示、少样本示例、推理链）的影响，考察模型在准确率、时间效率和经济成本等维度的表现。

Result: 加入bug报告上下文能够显著提升模型性能。少样本学习有提升潜力，但边际效应递减明显。推理链策略的有效性很依赖模型的自身推理能力。

Conclusion: 不同大语言模型在故障定位任务中有各自的性能特点和取舍，本研究提供了对现有LLM优缺点及优化故障定位有效性的策略的洞见。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [22] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: 当前Java去臃肿工具不够完善，容易误删必要代码或保留多余代码，影响软件稳定性。作者开发了Deblometer基准，系统评估三款工具表现，突显了业界改进去臃肿工具的迫切需求。


<details>
  <summary>Details</summary>
Motivation: 软件开发中广泛采用库作为依赖，但实际运行时仅有少数依赖真正被使用，多余代码导致项目臃肿。因此，开发者需要有效工具来移除未使用的依赖和代码结构，以提升软件性能和安全性。

Method: 作者开发了Deblometer，一个包含59个由人工整理的测试用例的微基准，用于评估Java debloating工具对Java语言各种特性的支持。基准为每个用例给出依赖和代码的必要性，有助于精准地衡量工具的soundness和precision。作者利用Deblometer对Deptrim、JShrink和ProGuard三个流行Java去臃肿工具进行对比评测。

Result: 评测显示，所有工具都误删了必要的代码结构，导致语义变化甚至程序崩溃。尤其是动态类加载特性使所有工具在soundness方面存在问题。具体而言，Deptrim保留了更多多余结构，ProGuard则更多删除了必要结构，JShrink因对注解支持有限导致去臃肿结果损坏。

Conclusion: 现有Java去臃肿工具在保证功能完整与代码清晰之间存在明显不足，容易造成程序不稳定和不可用。需进一步改进这些工具以提升去臃肿软件的稳定性和可靠性。

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Resource-Aware Hybrid Quantum Programming with General Recursion and Quantum Control](https://arxiv.org/abs/2510.20452)
*Kostia Chardonnet,Emmanuel Hainry,Romain Péchoux,Thomas Vinet*

Main category: cs.LO

TL;DR: 本文提出了 Hyrql 混合量子语言及语义保持编译方法，实现了量子程序的通用资源复杂度分析，无需依赖特定量子门集合，推广了复杂度分析技术并证明了其实用性。


<details>
  <summary>Details</summary>
Motivation: 目前针对量子语言的资源分析依赖于具体的量子门集合，导致通用性不足。为解决此问题，需要一种不依赖初始量子门集的语言，以便泛化资源复杂度分析方法。

Method: 提出了一种新的混合量子语言 Hyrql，该语言无需预设量子门集合，并设计了一种语义保持的编译算法，将 Hyrql 程序转化为类型简单的项重写系统，使之可以复用现有对项重写系统复杂度分析的技术。

Result: Hyrql 支持基于泛用算法的资源复杂度分析，通过多个实例验证了该方法的通用性与有效性。

Conclusion: Hyrql 语言及其编译方法能够对不同量子门集合下的量子程序进行通用资源分析，突破了现有方法在门集依赖上的局限。

Abstract: This paper introduces the hybrid quantum language with general recursion
$\mathtt{Hyrql}$, driven towards resource-analysis. By design, $\mathtt{Hyrql}$
does not require the specification of an initial set of quantum gates and,
hence, is well amenable towards a generic cost analysis. Indeed, languages
using different sets of quantum gates lead to representations of quantum
circuits whose complexity varies. Towards resource-analysis, a
semantics-preserving compilation algorithm to simply-typed term rewrite systems
is described; allowing a generic reuse of all known techniques for analyzing
the complexity of term rewrite systems. We prove the versatility of this
approach through many examples.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse](https://arxiv.org/abs/2510.19858)
*Jindi Wang,Yidi Zhang,Zhaoxing Li*

Main category: cs.CL

TL;DR: 提出并验证了一个基于DeBERTa变换器的知识建构水平自动分类模型，能有效分析YouTube等平台评论中的知识建构类型，显著优于现有方法，有望促进尺度化的学习话语自动分析应用。


<details>
  <summary>Details</summary>
Motivation: 在线学习平台（如YouTube）中的科学讨论包含大量用户评论，但如何自动化判别这些评论的知识建构水平尚缺有效手段。

Method: 提出DeBERTa-KC模型，基于DeBERTa-v3，结合Focal Loss、标签平滑和R-Drop正则化，处理类别不均衡问题。构建2万条人工标注的评论数据集（四类KC类别），执行端到端的数据处理与实验流程，并基于10折交叉验证评估性能。

Result: DeBERTa-KC模型在宏F1得分上达到0.836±0.008，显著优于传统和现有变换器模型（p<0.01）；在高阶知识建构类别（Explore、Negotiate）上表现突出。

Conclusion: 大语言模型能够有效识别和分类在线非正式学习环境中用户评论的知识建构水平，展示出可扩展、理论驱动的自动化话语分析与评估工具的潜力。

Abstract: This study presents DeBERTa-KC, a transformer-based model for automatic
classification of knowledge construction (KC) levels in online science learning
discourse. Using comments collected from four popular YouTube science channels
(2022--2024), a balanced corpus of 20,000 manually annotated samples was
created across four KC categories: \textit{nonKC}, \textit{Share},
\textit{Explore}, and \textit{Negotiate}. The proposed model extends DeBERTa-v3
with Focal Loss, Label Smoothing, and R-Drop regularization to address class
imbalance and enhance generalization. A reproducible end-to-end pipeline was
implemented, encompassing data extraction, annotation, preprocessing, training,
and evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved
a macro-F1 of $0.836 \pm 0.008$, significantly out-performing both classical
and transformer baselines ($p<0.01$). Per-category results indicate strong
sensitivity to higher-order epistemic engagement, particularly in
\textit{Explore} and \textit{Negotiate} discourse. These findings demonstrate
that large language models can effectively capture nuanced indicators of
knowledge construction in informal digital learning environments, offering
scalable, theory-informed approaches to discourse analysis and the development
of automated tools for assessing epistemic engagement.

</details>


### [25] [An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics](https://arxiv.org/abs/2510.19866)
*Xincheng Liu*

Main category: cs.CL

TL;DR: 不同AI模型和提示框架对教学计划的生成效果影响显著。DeepSeek模型生成的内容最易读，RACE提示框架可最大程度减少事实错误并提升课程标准符合度。整体建议结合强可读性模型与RACE框架，明确列出教学目标及课程标准，以得到最优AI辅助教学方案。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在评估AI生成的教学计划的教学合理性和可用性，针对不同的大型语言模型和提示框架，分析其性能差异，为教育领域提供高效可用的AI辅助教学方案。

Method: 对五个主流大语言模型（ChatGPT、Claude、Gemini、DeepSeek、Grok）结合三种结构化提示框架（TAG、RACE、COSTAR），分别生成关于高中物理主题“电磁波谱”的15份教学计划，通过四项自动化计算指标（可读性与语言复杂度、事实准确性与幻觉检测、标准与课程对齐、学习目标的认知要求）进行分析比较。

Result: 结果显示，语言模型选型影响教学计划的语言可读性，DeepSeek表现最佳，Claude最复杂；而提示框架在事实准确性和教学完整性方面作用更大，RACE框架表现最佳。此外，所有模型生成的学习目标主要集中在布鲁姆认知等级的基础层级，较少涉及高阶认知目标。

Conclusion: 教学计划的可读性主要受模型设计影响，而教学可靠性与课程标准对齐则依赖于所用提示框架。最佳配置是采用优化可读性的模型结合RACE框架，并明确包含物理概念、课程标准及高阶学习目标的清单。

Abstract: This study evaluates the pedagogical soundness and usability of AI-generated
lesson plans across five leading large language models: ChatGPT (GPT-5), Claude
Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,
three structured prompt frameworks were tested: TAG (Task, Audience, Goal),
RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,
Style, Tone, Audience, Response Format).
  Fifteen lesson plans were generated for a single high-school physics topic,
The Electromagnetic Spectrum. The lesson plans were analyzed through four
automated computational metrics: (1) readability and linguistic complexity, (2)
factual accuracy and hallucination detection, (3) standards and curriculum
alignment, and (4) cognitive demand of learning objectives.
  Results indicate that model selection exerted the strongest influence on
linguistic accessibility, with DeepSeek producing the most readable teaching
plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).
  The prompt framework structure most strongly affected the factual accuracy
and pedagogical completeness, with the RACE framework yielding the lowest
hallucination index and the highest incidental alignment with NGSS curriculum
standards. Across all models, the learning objectives in the fifteen lesson
plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There
were limited higher-order verbs in the learning objectives extracted.
  Overall, the findings suggest that readability is significantly governed by
model design, while instructional reliability and curricular alignment depend
more on the prompt framework. The most effective configuration for lesson plans
identified in the results was to combine a readability-optimized model with the
RACE framework and an explicit checklist of physics concepts, curriculum
standards, and higher-order objectives.

</details>


### [26] [From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model](https://arxiv.org/abs/2510.19871)
*Yatai Ji,Teng Wang,Yuying Ge,Zhiheng Liu,Sidi Yang,Ying Shan,Ping Luo*

Main category: cs.CL

TL;DR: ReDiff通过主动修正自身输出，在离散扩散模型生成中解决了因训练-推理不一致导致的错误级联，大幅提升生成内容的连贯性和准确性，实现了更优的并行化生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的离散扩散模型虽在视觉-语言任务中显示出潜力，但在实际应用中，由于训练与推理阶段的不一致（train-inference discrepancy），在并行解码时初始token错误导致错误级联和语义幻想，严重阻碍了其性能。

Method: 提出了一种名为ReDiff的增强型扩散框架，将生成过程由被动去噪转变为主动修正。具体包括两阶段训练：第一阶段通过修正合成错误增强模型的基础修正能力；第二阶段引入在线自我纠正环路，让模型学习专家纠正自身草稿的方法，实现错误驱动的学习。

Result: 实验表明，ReDiff显著提升了生成内容的连贯性和事实准确性，支持比传统去噪法更稳定高效的并行生成。

Conclusion: ReDiff框架打破了离散扩散模型生成中的错误级联问题，大幅提高了生成效率和质量，推动了视觉-语言任务的实际落地。

Abstract: Discrete diffusion models have emerged as a promising direction for
vision-language tasks, offering bidirectional context modeling and theoretical
parallelization. However, their practical application is severely hindered by a
train-inference discrepancy, which leads to catastrophic error cascades:
initial token errors during parallel decoding pollute the generation context,
triggering a chain reaction of compounding errors and leading to syntactic
errors and semantic hallucinations. To address this fundamental challenge, we
reframe the generation process from passive denoising to active refining. We
introduce ReDiff, a refining-enhanced diffusion framework that teaches the
model to identify and correct its own errors. Our approach features a two-stage
training process: first, we instill a foundational revision capability by
training the model to revise synthetic errors; second, we implement a novel
online self-correction loop where the model is explicitly trained to revise its
own flawed drafts by learning from an expert's corrections. This mistake-driven
learning endows the model with the crucial ability to revisit and refine its
already generated output, effectively breaking the error cascade. Extensive
experiments demonstrate that ReDiff significantly improves the coherence and
factual accuracy of generated content, enabling stable and efficient parallel
generation far superior to traditional denoising methods. Our codes and models
are available at https://rediff-hku.github.io/.

</details>


### [27] [Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention](https://arxiv.org/abs/2510.19875)
*J Rosser,José Luis Redondo García,Gustavo Penha,Konstantina Palla,Hugues Bouchard*

Main category: cs.CL

TL;DR: 本文提出了Sparse Tracing和Stream算法，极大提升了大模型长上下文下的注意力可解释性分析效率，可在单机消费级GPU上运行，并有效保留关键信息流，是链式推理监控与信息流追踪的实用工具。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力机制可解释性分析方法在处理百万token级别的大规模上下文时，计算量和内存需求呈二次方增长，难以扩展，无法满足多token链式推理或大模型长上下文可解释性分析的现实需求。

Method: 提出了Sparse Tracing技术，核心是可编译的层级化剪枝算法Stream，利用动态稀疏注意力，在近线性时间与线性空间复杂度下估算每个注意力头的稀疏掩码，从而在单次遍历中分析大规模注意力模式。Stream采用二分搜索方式，仅为每个查询保留少量关键块，显著减少需要分析的注意力连边，同时保持模型预测行为。

Result: Sparse Tracing可精准识别长链式思考中的关键信息锚点，裁剪掉97-99%的token间交互；在RULER基准测试中，也可保留主要信息检索路径，同时舍弃90-96%的非关键交互，并揭示针-输出的逐层信息流动路线。

Conclusion: Sparse Tracing和Stream算法使得对大模型超长上下文的注意力模式可解释性分析变得高效、可扩展，并可在消费级GPU上运行，无需超大缓存，有效推动链式思维监控的普及化。

Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional
Mechanistic Interpretability techniques for analyzing attention scale
quadratically with context length, demanding terabytes of memory beyond 100,000
tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic
sparse attention to efficiently analyze long context attention patterns. We
present Stream, a compilable hierarchical pruning algorithm that estimates
per-head sparse attention masks in near-linear time $O(T \log T)$ and linear
space $O(T)$, enabling one-pass interpretability at scale. Stream performs a
binary-search-style refinement to retain only the top-$k$ key blocks per query
while preserving the model's next-token behavior. We apply Stream to long
chain-of-thought reasoning traces and identify thought anchors while pruning
97-99\% of token interactions. On the RULER benchmark, Stream preserves
critical retrieval paths while discarding 90-96\% of interactions and exposes
layer-wise routes from the needle to output. Our method offers a practical
drop-in tool for analyzing attention patterns and tracing information flow
without terabytes of caches. By making long context interpretability feasible
on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.
Code is available at https://anonymous.4open.science/r/stream-03B8/.

</details>


### [28] [Automated HIV Screening on Dutch EHR with Large Language Models](https://arxiv.org/abs/2510.19879)
*Lang Zhou,Amrish Jhingoer,Yinghao Luo,Klaske Vliegenthart--Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 文章提出用大型语言模型分析电子健康记录中的临床文本数据，显著提升了HIV筛查的准确性并减少漏诊。


<details>
  <summary>Details</summary>
Motivation: 高效的HIV筛查和早期诊断对减少疾病传播至关重要。传统大规模实验室检测不可行，而电子健康记录（EHR）的广泛应用为诊断带来新机遇。已有工作多集中于结构化数据，忽视了包含潜在关键信息的临床文本数据。

Method: 论文提出了一种新的流程，利用大型语言模型（LLM）分析EHR中的非结构化文本，判断患者是否适合进一步HIV检测。

Result: 在鹿特丹伊拉斯谟大学医学中心临床数据上的实验表明，该流程在保持低假阴性率的同时，取得了较高的诊断准确性。

Conclusion: 利用LLM分析EHR中的非结构化文本数据能有效提升疾病筛查准确性，为HIV早期诊断提供了新的技术路径。

Abstract: Efficient screening and early diagnosis of HIV are critical for reducing
onward transmission. Although large scale laboratory testing is not feasible,
the widespread adoption of Electronic Health Records (EHRs) offers new
opportunities to address this challenge. Existing research primarily focuses on
applying machine learning methods to structured data, such as patient
demographics, for improving HIV diagnosis. However, these approaches often
overlook unstructured text data such as clinical notes, which potentially
contain valuable information relevant to HIV risk. In this study, we propose a
novel pipeline that leverages a Large Language Model (LLM) to analyze
unstructured EHR text and determine a patient's eligibility for further HIV
testing. Experimental results on clinical data from Erasmus University Medical
Center Rotterdam demonstrate that our pipeline achieved high accuracy while
maintaining a low false negative rate.

</details>


### [29] [An Expert-grounded benchmark of General Purpose LLMs in LCA](https://arxiv.org/abs/2510.19886)
*Artur Donaldson,Bharathan Balaji,Cajetan Oriekezie,Manish Kumar,Laure Patouillard*

Main category: cs.CL

TL;DR: 首次系统专家评估多种LLMs在生命周期评估（LCA）中的表现，发现约四成输出信息不准确或误导，但解释质量及格式表现良好。开源模型部分优于商业模型。LLMs虽有助于减轻重复性劳动，但如无额外数据支持，存在较大结果风险。


<details>
  <summary>Details</summary>
Motivation: 人工智能（特别是大型语言模型LLMs）越来越被探索用于支持生命周期评估（LCA），但目前缺乏系统化、标准化的评估框架，且对其可靠性、稳健性和可用性尚无明确证据。作者试图填补这一空白，首次以专家测评方式对LLMs在LCA领域进行基准评估。

Method: 评估了11种通用型LLMs（含商业和开源模型）在22项LCA相关任务上的表现，由17位有经验的实践者依据科学准确性、解释质量、稳健性、可验证性及规范遵循度等标准进行测评，收集了168份专家评审。

Result: 专家判定37%的模型响应包含不准确或误导性信息。准确性和解释质量在多数模型（即便是较小模型）中平均或较好，格式遵守度也较高。模型幻觉（虚构内容）率差异大，部分模型虚构引用率达40%。开源模型在准确性和解释质量等指标上部分优于或与商业模型持平，无明显优劣分界。

Conclusion: LLMs在LCA应用存在风险，不能盲目作为万能工具，但在提升解释质量及减轻低难度重复劳动方面有明显优势。缺乏基础数据支持的通用型LLMs在LCA任务中的风险需关注。

Abstract: Purpose: Artificial intelligence (AI), and in particular large language
models (LLMs), are increasingly being explored as tools to support life cycle
assessment (LCA). While demonstrations exist across environmental and social
domains, systematic evidence on their reliability, robustness, and usability
remains limited. This study provides the first expert-grounded benchmark of
LLMs in LCA, addressing the absence of standardized evaluation frameworks in a
field where no clear ground truth or consensus protocols exist.
  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial
and open-source families, across 22 LCA-related tasks. Seventeen experienced
practitioners reviewed model outputs against criteria directly relevant to LCA
practice, including scientific accuracy, explanation quality, robustness,
verifiability, and adherence to instructions. We collected 168 expert reviews.
  Results: Experts judged 37% of responses to contain inaccurate or misleading
information. Ratings of accuracy and quality of explanation were generally
rated average or good on many models even smaller models, and format adherence
was generally rated favourably. Hallucination rates varied significantly, with
some models producing hallucinated citations at rates of up to 40%. There was
no clear-cut distinction between ratings on open-weight versus closed-weight
LLMs, with open-weight models outperforming or competing on par with
closed-weight models on criteria such as accuracy and quality of explanation.
  Conclusion: These findings highlight the risks of applying LLMs na\"ively in
LCA, such as when LLMs are treated as free-form oracles, while also showing
benefits especially around quality of explanation and alleviating labour
intensiveness of simple tasks. The use of general-purpose LLMs without
grounding mechanisms presents ...

</details>


### [30] [Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities](https://arxiv.org/abs/2510.19892)
*Nishant Balepur,Dang Nguyen,Dayeon Ki*

Main category: cs.CL

TL;DR: 本研究提出用桌游（Dixit）做为评估工具，相较于传统评测方法，能更全面、客观地衡量多模态大语言模型能力，并找出其改进空间。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型（MLM）的评估多依赖静态的单项基准测试或主观、昂贵且容易被模型投机取巧的对比评测，无法全面反映模型综合能力。

Method: 提出以游戏化的方式评估MLM，具体选择Dixit（妙语说书人）桌游，通过让模型和人类参与者一起玩游戏，从而在规则明确、任务复杂的游戏环境下全面衡量模型能力。通过实验让五个MLM参与Dixit游戏，并与传统基准测试结果进行相关性分析，同时观察人类与MLM的博弈过程揭示模型推理策略。

Result: Dixit游戏中的模型胜率排名与主流MLM基准测试结果完全相关。人机对抗揭示了MLM与人类在策略和推理上的差异并指出了模型改进空间。

Conclusion: 游戏化评测为多模态大语言模型提供了更全面和客观的评估框架，不仅能反映现有基准结果，还能揭示模型在实际复杂任务中的优劣和改进方向。

Abstract: Multi-modal large language models (MLMs) are often assessed on static,
individual benchmarks -- which cannot jointly assess MLM capabilities in a
single task -- or rely on human or model pairwise comparisons -- which is
highly subjective, expensive, and allows models to exploit superficial
shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these
issues, we propose game-based evaluations to holistically assess MLM
capabilities. Games require multiple abilities for players to win, are
inherently competitive, and are governed by fix, objective rules, and makes
evaluation more engaging, providing a robust framework to address the
aforementioned challenges. We manifest this evaluation specifically through
Dixit, a fantasy card game where players must generate captions for a card that
trick some, but not all players, into selecting the played card. Our
quantitative experiments with five MLMs show Dixit win-rate rankings are
perfectly correlated with those on popular MLM benchmarks, while games between
human and MLM players in Dixit reveal several differences between agent
strategies and areas of improvement for MLM reasoning.

</details>


### [31] [Large Language Model enabled Mathematical Modeling](https://arxiv.org/abs/2510.19895)
*Guoyun Zhang*

Main category: cs.CL

TL;DR: 本文探索了DeepSeek-R1大模型在实际优化建模中的应用，通过多种策略有效提升准确性并降低幻觉，展示了其在运筹学领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法需要大量领域知识来将实际问题转化为数学模型，而现有大模型在实际应用中成本高且存在幻觉等现实限制。作者希望探索更高效、更实用的大模型用于运筹学建模问题。

Method: 采用DeepSeek-R1大语言模型，系统性评估其在四个关键运筹学基准（NL4OPT、IndustryOR、EasyLP、ComplexOR）上的表现。研究内容包括建立基线评估、发展幻觉分类体系，并应用多种缓解策略（如LLM-as-a-Judge、少样本学习、工具调用和多智能体框架）来降低幻觉、提升模型效果和一致性。

Result: 初步结果表明，DeepSeek-R1在运筹学场景中表现突出，通过引入缓解策略进一步降低了幻觉，提升了模型的建模准确率和输出与用户意图的一致性。

Conclusion: DeepSeek-R1模型在优化建模领域展现出良好的实用性和成本效益，配合适当策略能够有效弥补传统大模型的不足，为运筹学决策支持提供了新的实现路径。

Abstract: The integration of Large Language Models (LLMs) with optimization modeling
offers a promising avenue for advancing decision-making in operations research
(OR). Traditional optimization methods,such as linear programming, mixed
integer programming, and simulation depend heavily on domain expertise to
translate real-world problems into solvable mathematical models. While solvers
like Gurobi and COPT are powerful, expert input remains essential for defining
objectives, constraints, and variables. This research investigates the
potential of LLMs, specifically the DeepSeek-R1 model, to bridge this
formulation gap using natural language understanding and code generation.
Although prior models like GPT-4, Claude, and Bard have shown strong
performance in NLP and reasoning tasks, their high token costs and tendency
toward hallucinations limit real-world applicability in supply chain contexts.
In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained
with reinforcement learning, presents a viable alternative. Despite its success
in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied
OR scenarios remains under explored. This study systematically evaluates
DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and
ComplexOR. Our methodology includes baseline assessments, the development of a
hallucination taxonomy, and the application of mitigation strategies like
LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent
Framework. These techniques aim to reduce hallucinations, enhance formulation
accuracy, and better align model outputs with user intent.

</details>


### [32] [Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation](https://arxiv.org/abs/2510.19897)
*Jackson Hassell,Dan Zhang,Hannah Kim,Tom Mitchell,Estevam Hruschka*

Main category: cs.CL

TL;DR: 该论文提出利用记忆增强（结合标注数据和大模型自生成批评）提升大模型无需参数更新的任务适应能力，显著优于常规检索方法并引入新指标揭示不同模型和记忆策略对学习表现的影响。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在处理新分类任务时，常需微调模型参数，代价高昂且灵活性差，且过程不透明。如何利用标注数据和大模型自身生成的信息，在无需参数更新的情况下提升学习能力，是个值得探究的问题。

Method: 提出了一种基于记忆增强的学习框架。该框架融合了带标签的数据和由大模型自生成的批评（critiques），通过情景记忆（存储具体实例批评）和语义记忆（凝练成可迁移任务级指导）相结合，引导模型在无需参数更新下优化分类表现。此外，设计了“suggestibility”新指标来定量分析模型受不同监督表示影响的反应。

Result: 在多个任务测试中，融入批评的信息可使模型准确率相比只用标签的检索增强（RAG）基线最多提升24.8%。比较OpenAI与开源模型，发现它们在处理事实型与偏好型数据的行为上有明显差异。新引入的suggestibility指标解释了这些行为异同及相应的记忆策略影响。

Conclusion: 基于记忆和反思机制的学习方式能够显著提升大模型无参数更新场景下的适应性与可解释性，是构建更智能LLM代理的重要方向。

Abstract: We investigate how agents built on pretrained large language models can learn
target classification functions from labeled examples without parameter
updates. While conventional approaches like fine-tuning are often costly,
inflexible, and opaque, we propose a memory-augmented framework that leverages
both labeled data and LLM-generated critiques. Our framework uses episodic
memory to store instance-level critiques-capturing specific past
experiences-and semantic memory to distill these into reusable, task-level
guidance. Across a diverse set of tasks, incorporating critiques yields up to a
24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines
that rely only on labels. Through extensive empirical evaluation, we uncover
distinct behavioral differences between OpenAI and opensource models,
particularly in how they handle fact-oriented versus preference-based data. To
interpret how models respond to different representations of supervision
encoded in memory, we introduce a novel metric, suggestibility. This helps
explain observed behaviors and illuminates how model characteristics and memory
strategies jointly shape learning dynamics. Our findings highlight the promise
of memory-driven, reflective learning for building more adaptive and
interpretable LLM agents.

</details>


### [33] [LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation](https://arxiv.org/abs/2510.19967)
*Le Ren,Xiangjian Zeng,Qingqiang Wu,Ruoxuan Liang*

Main category: cs.CL

TL;DR: LyriCAR是一种无监督歌词翻译新框架，融合适应性课程学习设计，不但显著优化了训练效率，还在多项指标上领先现有方法，尤其在英译中歌词任务表现突出。


<details>
  <summary>Details</summary>
Motivation: 歌词翻译需要兼顾多种音乐性约束，如韵律和跨行连贯性，但现有方法多依赖手工规则和句子级建模，难以学习更复杂的音乐-语言模式，且在段落级别很难保持全局统一性。

Method: 提出了一种全新的歌词翻译框架LyriCAR。该方法完全无监督，采用难度感知的课程设计器和自适应课程策略，通过逐步增加挑战引导模型学习，有效分配训练资源、加速训练收敛，并提升翻译的整体质量。

Result: 在英-中歌词翻译任务上，LyriCAR在标准翻译指标和多维度奖励分数上均超越了强基线方法，并且自适应课程策略在保持性能的同时大幅减少（约40%）训练步骤。

Conclusion: LyriCAR是一个高效且可控的无监督歌词翻译工具，能在保证音乐性和语言性约束下提升翻译质量，并显著减少训练成本，为歌词翻译提供了新思路。

Abstract: Lyric translation is a challenging task that requires balancing multiple
musical constraints. Existing methods often rely on hand-crafted rules and
sentence-level modeling, which restrict their ability to internalize
musical-linguistic patterns and to generalize effectively at the paragraph
level, where cross-line coherence and global rhyme are crucial. In this work,
we propose LyriCAR, a novel framework for controllable lyric translation that
operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware
curriculum designer and an adaptive curriculum strategy, ensuring efficient
allocation of training resources, accelerating convergence, and improving
overall translation quality by guiding the model with increasingly complex
challenges. Extensive experiments on the EN-ZH lyric translation task show that
LyriCAR achieves state-of-the-art results across both standard translation
metrics and multi-dimensional reward scores, surpassing strong baselines.
Notably, the adaptive curriculum strategy reduces training steps by nearly 40%
while maintaining superior performance. Code, data and model can be accessed at
https://github.com/rle27/LyriCAR.

</details>


### [34] [LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation](https://arxiv.org/abs/2510.19988)
*Xin Lian,Kenneth D. Forbus*

Main category: cs.CL

TL;DR: 本文提出结合LLM与符号NLP优点的混合系统，既扩大了知识覆盖，又保证推理结构的可解释和准确。实验证明该混合方法在科学文本信息抽取任务上表现优异，优于符号系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）能广泛应用于自然语言处理任务，但基于概率推断的机制容易出现如事实幻觉和结构不一致等错误。相比之下，符号NLP系统通过语义规则和资源实现可解释的深层理解，但覆盖面有限，且需要专业知识维护。

Method: 探讨了一种混合方法，将LLMs的广泛覆盖能力与符号NLP生成关系性结构表达的能力结合。具体做法是利用LLMs进行文本重述和简化，自动填补知识空缺，同时利用符号NLP生成能支持推理和递增学习的结构化表示。

Result: 在常识性科学文本的量与因果律抽取任务上，与纯符号方法及纯LLM方法进行对比实验，混合方法显著优于纯符号系统。

Conclusion: 混合LLM与符号NLP的方法在保持覆盖面的同时提升了推理和学习的可解释性与准确性，效果优于单独使用符号方法。

Abstract: Despite the broad applicability of large language models (LLMs), their
reliance on probabilistic inference makes them vulnerable to errors such as
hallucination in generated facts and inconsistent output structure in natural
language understanding (NLU) tasks. By contrast, symbolic NLU systems provide
interpretable understanding grounded in curated lexicons, semantic resources,
and syntactic & semantic interpretation rules. They produce relational
representations that can be used for accurate reasoning and planning, as well
as incremental debuggable learning. However, symbolic NLU systems tend to be
more limited in coverage than LLMs and require scarce knowledge representation
and linguistics skills to extend and maintain. This paper explores a hybrid
approach that integrates the broad-coverage language processing of LLMs with
the symbolic NLU capabilities of producing structured relational
representations to hopefully get the best of both approaches. We use LLMs for
rephrasing and text simplification, to provide broad coverage, and as a source
of information to fill in knowledge gaps more automatically. We use symbolic
NLU to produce representations that can be used for reasoning and for
incremental learning. We evaluate this approach on the task of extracting and
interpreting quantities and causal laws from commonsense science texts, along
with symbolic- and LLM-only pipelines. Our results suggest that our hybrid
method works significantly better than the symbolic-only pipeline.

</details>


### [35] [A Fundamental Algorithm for Dependency Parsing (With Corrections)](https://arxiv.org/abs/2510.19996)
*Michael A. Covington*

Main category: cs.CL

TL;DR: 提出了模拟人脑逐词解析语言的依存树解析基础算法，理论复杂度与传统方法相当，但实际更高效。


<details>
  <summary>Details</summary>
Motivation: 希望提升自然语言解析的效率，让解析过程更加接近人脑实际处理语言的方式，规避传统短语结构法的不足。

Method: 逐词解析与依存树结构结合，采用实时附加策略。分析与传统的短语结构解析对比，关注处理速度与复杂度。

Result: 理论复杂度为O(n^3)，但对人类语言来说，极端复杂的情况只会出现在较短的句子中，实际性能优于传统方法。

Conclusion: 提出了一种用于将自然语言句子解析为依存树的基础算法。该算法与传统的短语结构解析器不同，它逐词操作，能尽快地将每个词进行附加，模拟了人脑解析语言的特性。

Abstract: This paper presents a fundamental algorithm for parsing natural language
sentences into dependency trees. Unlike phrase-structure (constituency)
parsers, this algorithm operates one word at a time, attaching each word as
soon as it can be attached, corresponding to properties claimed for the parser
in the human brain. Like phrase-structure parsing, its worst-case complexity is
$O(n^3)$, but in human language, the worst case occurs only for small $n$.

</details>


### [36] [Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs](https://arxiv.org/abs/2510.20001)
*Yunpeng Xiao,Carl Yang,Mark Mai,Xiao Hu,Kai Shu*

Main category: cs.CL

TL;DR: 现有医学问答数据集无法充分反映真实临床决策。该文提出将临床任务划分为临床背景和临床问题两个维度，系统总结现有资源和方法，扩展评估指标，指出开发更有临床意义大模型的标准路径和挑战。


<details>
  <summary>Details</summary>
Motivation: 现有医学数据集（如MedQA）过于简化真实临床决策过程，无法真实反映临床实际需求。因此需要更科学系统的评估方法和任务设定。

Method: 将临床决策任务划分为两个维度：临床背景和临床问题，并据此总结现有数据集和基准总结，以及各种用于改进临床决策的方法，包括训练和测试时的技术。

Result: 建立了评估范式，对现有数据集/方法进行归类和总结，扩展了评估指标至效率和可解释性，并提出了未来挑战，为临床相关大模型的标准化与发展指明方向。

Conclusion: 提出了一个统一范式，用于标准化医疗临床决策相关任务，帮助指导大模型在真实临床环境中的开发和评估。

Abstract: Large language models (LLMs) show promise for clinical use. They are often
evaluated using datasets such as MedQA. However, Many medical datasets, such as
MedQA, rely on simplified Question-Answering (Q\A) that underrepresents
real-world clinical decision-making. Based on this, we propose a unifying
paradigm that characterizes clinical decision-making tasks along two
dimensions: Clinical Backgrounds and Clinical Questions. As the background and
questions approach the real clinical environment, the difficulty increases. We
summarize the settings of existing datasets and benchmarks along two
dimensions. Then we review methods to address clinical decision-making,
including training-time and test-time techniques, and summarize when they help.
Next, we extend evaluation beyond accuracy to include efficiency,
explainability. Finally, we highlight open challenges. Our paradigm clarifies
assumptions, standardizes comparisons, and guides the development of clinically
meaningful LLMs.

</details>


### [37] [Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training](https://arxiv.org/abs/2510.20002)
*Alexandra Apostolopoulou,Konstantinos Kanaris,Athanasios Koursaris,Dimitris Tsakalidis,George Domalis,Ioannis E. Livieris*

Main category: cs.CL

TL;DR: 本论文开发了高质量希腊语及法律领域大规模语料，系统性评测并推出多种现代transformer架构嵌入模型，首次引入希腊-英语法律领域双语模型，相关GEM模型显著优于现有水平。


<details>
  <summary>Details</summary>
Motivation: 现有对现代希腊语的自然语言处理研究存在分散、结构单一、依赖有限上下文窗口的问题，尤其在法律等高价值领域，模型通常受早期transformer架构及512-token限制，无法有效处理长文本。为改善此局面，有必要开发更适合希腊语及特殊领域的先进模型。

Method: 1. 构建多个大规模希腊语语料库，侧重高质量数据筛选和预处理，并覆盖通用与法律等专业领域；2. 基于这些数据，预训练并系统评测多种现代transformer架构如ELECTRA、ConvBERT和ModernBERT（之前未用于希腊语）；3. 首次提出适用于法律领域的双语（希腊语-英语）嵌入模型。

Result: 实验结果显示，该系列新模型（尤其是GEM-RoBERTa和GEM-ConvBERT）在下游任务中显著优于现有基线模型，证实了构建高质量数据集和引入多样架构的最优性。

Conclusion: 通过高质量数据构建与多样化架构尝试，显著提升了现代希腊语（特别是法律领域）自然语言处理模型的表现和应用潜力。

Abstract: The advancement of natural language processing for morphologically rich,
moderately-resourced languages like Modern Greek is often hindered by a
fragmented research landscape, a lack of architectural diversity and reliance
on limited context-length models. This is particularly true in specialized,
high-value domains such as law, where existing models are frequently confined
to early transformer architectures with a restrictive 512-token window,
insufficient for analyzing long legal documents. To address these challenges,
this paper presents Greek Embedding Models, a new family of transformer models
for Greek language built upon a foundation of extensive, quality-driven data
curation. We detail the construction of several large-scale Greek corpora,
emphasizing a rigorous, quality-based filtering and preprocessing methodology
to create high-value training datasets from both general-domain and specialized
legal sources. On this carefully curated foundation, we pre-train and
systematically evaluate a diverse suite of modern architectures, which has not
previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.
Furthermore, we propose the first bilingual Greek-English Embedding Models
tailored for the legal domain. The extensive experiments on downstream tasks
demonstrate that the new class of models establish the effectiveness of the
proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models
significantly outperform existing baselines.

</details>


### [38] [Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models](https://arxiv.org/abs/2510.20033)
*David Dukić*

Main category: cs.CL

TL;DR: 本论文针对预训练神经语言模型的迁移学习，提出了多任务融合、架构优化和生成式微调三种创新方法，有效提升了序列标注任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的迁移学习在序列标注任务上表现有待提升，特别是在神经语言模型的适应上。针对如何更好地利用预训练语言模型进行序列标注，需要创新的迁移学习方法。

Method: 提出三种提升方法：一是多任务学习模型，整合来自域无关文本处理系统的额外信号，用于提升事件触发词检测的领域迁移效果；二是对自回归语言模型架构进行修改，使层间信息能够双向流动，提高模型表现；三是提出在自回归语言模型上，通过结合监督型in-context微调和面向响应的适应策略，实现生成式序列标注。

Result: 经过实验，上述改进方案和方法能显著提升预训练神经语言模型在序列标注任务上的迁移学习表现。

Conclusion: 针对序列标注任务，通过提出多任务学习、架构调整以及新型微调框架，能有效提升大语言模型迁移学习的精度和效率。

Abstract: This doctoral thesis improves the transfer learning for sequence labeling
tasks by adapting pre-trained neural language models. The proposed improvements
in transfer learning involve introducing a multi-task model that incorporates
an additional signal, a method based on architectural modifications in
autoregressive large language models, and a sequence labeling framework for
autoregressive large language models utilizing supervised in-context
fine-tuning combined with response-oriented adaptation strategies. The first
improvement is given in the context of domain transfer for the event trigger
detection task. The domain transfer of the event trigger detection task can be
improved by incorporating an additional signal obtained from a
domain-independent text processing system into a multi-task model. The second
improvement involves modifying the model's architecture. For that purpose, a
method is proposed to enable bidirectional information flow across layers of
autoregressive large language models. The third improvement utilizes
autoregressive large language models as text generators through a generative
supervised in-context fine-tuning framework. The proposed model, method, and
framework demonstrate that pre-trained neural language models achieve their
best performance on sequence labeling tasks when adapted through targeted
transfer learning paradigms.

</details>


### [39] [ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering](https://arxiv.org/abs/2510.20036)
*Marianne Menglin Liu,Daniel Garcia,Fjona Parllaku,Vikas Upadhyay,Syed Fahad Allam Shah,Dan Roth*

Main category: cs.CL

TL;DR: ToolScope通过合并冗余工具和智能筛选工具，缓解了工具重叠和上下文受限问题，大幅提升了大语言模型的工具选择效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理（LLM agents）在解决复杂任务时需要外部工具，但现实中的工具集常存在冗余、描述重叠，造成工具选择歧义。同时，LLM输入上下文受限，难以高效利用大规模工具集。

Method: 提出ToolScope方法，包括两部分：1）ToolScopeMerger实现工具合并与自动纠错，减少冗余；2）ToolScopeRetriever对工具相关性排序并选取最相关工具，使工具集压缩至上下文限制内且保持精度。

Result: 在三种最先进LLM和三个开源工具使用基准测试上，工具选择准确率提升8.38%到38.6%。

Conclusion: ToolScope显著提升了LLM在多工具环境下的工具选择能力和准确率。

Abstract: Large language model (LLM) agents rely on external tools to solve complex
tasks, but real-world toolsets often contain redundant tools with overlapping
names and descriptions, introducing ambiguity and reducing selection accuracy.
LLMs also face strict input context limits, preventing efficient consideration
of large toolsets. To address these challenges, we propose ToolScope, which
includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and
fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and
select only the most relevant tools for each query, compressing toolsets to fit
within context limits without sacrificing accuracy. Evaluations on three
state-of-the-art LLMs and three open-source tool-use benchmarks show gains of
8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's
effectiveness in enhancing LLM tool use.

</details>


### [40] [From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge](https://arxiv.org/abs/2510.20043)
*Nafis Chowdhury,Moinul Haque,Anika Ahmed,Nazia Tasnim,Md. Istiak Hossain Shihab,Sajjadur Rahman,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文构建了孟加拉语文化知识数据集，发现当前多语种LLMs在文化知识理解上表现不足，加入上下文信息能显著提升其表现，凸显了文化化训练和上下文感知的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前多语言基准推动了LLMs文化评估，但低资源文化的细微差别捕捉不足。

Method: 提出了包含孟加拉文化内容（民间传统、美食、方言）的BLanCK数据集，并在多个多语种语言模型上进行评估。

Result: 多语种模型在非文化类别表现良好，但文化知识类表现较差，为模型提供上下文后，所有模型表现均显著提升。

Conclusion: 上下文感知结构与文化化训练数据对于提升模型在低资源文化知识方面的性能至关重要。

Abstract: Recent progress in NLP research has demonstrated remarkable capabilities of
large language models (LLMs) across a wide range of tasks. While recent
multilingual benchmarks have advanced cultural evaluation for LLMs, critical
gaps remain in capturing the nuances of low-resource cultures. Our work
addresses these limitations through a Bengali Language Cultural Knowledge
(BLanCK) dataset including folk traditions, culinary arts, and regional
dialects. Our investigation of several multilingual language models shows that
while these models perform well in non-cultural categories, they struggle
significantly with cultural knowledge and performance improves substantially
across all models when context is provided, emphasizing context-aware
architectures and culturally curated training data.

</details>


### [41] [Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training](https://arxiv.org/abs/2510.20059)
*Mehrdad Ghassabi,Sadra Hakim,Hamidreza Baradaran Kashani,Pedram Rostami*

Main category: cs.CL

TL;DR: 采用RLAIF和DPO在波斯语医学问答任务中，利用较小推理数据集，即显著提升了小模型的性能，优于以往大规模训练方法，验证了聚焦推理的训练方式在低资源语言领域的高效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在医学问答等专业领域，特别是像波斯语这样资源不足的语言环境下，提高小型语言模型的推理能力具有重要意义。

Method: 使用强化学习与AI反馈（RLAIF）和直接偏好优化（DPO）方法，通过把医学多项选择问答数据集翻译成波斯语，生成拒绝-偏好答案对，促使教师和学生模型产生推理链（CoT）响应，并据此构建正确和错误推理轨迹数据集。模型训练基于首选和被拒绝答案的共450万token数据。

Result: 训练后的模型在波斯语医学推理任务上表现优异，超越了在5700万token数据上训练的先前模型gaokerena-V。

Conclusion: 合理设计、聚焦推理能力的训练方案能够以有限数据实现领域特定语言模型的高效、有效提升。

Abstract: Enhancing reasoning capabilities in small language models is critical for
specialized applications such as medical question answering, particularly in
underrepresented languages like Persian. In this study, we employ Reinforcement
Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to
improve the reasoning skills of a general-purpose Persian language model. To
achieve this, we translated a multiple-choice medical question-answering
dataset into Persian and used RLAIF to generate rejected-preferred answer
pairs, which are essential for DPO training. By prompting both teacher and
student models to produce Chain-of-Thought (CoT) reasoning responses, we
compiled a dataset containing correct and incorrect reasoning trajectories.
This dataset, comprising 2 million tokens in preferred answers and 2.5 million
tokens in rejected ones, was used to train a baseline model, significantly
enhancing its medical reasoning capabilities in Persian. Remarkably, the
resulting model outperformed its predecessor, gaokerena-V, which was trained on
approximately 57 million tokens, despite leveraging a much smaller dataset.
These results highlight the efficiency and effectiveness of reasoning-focused
training approaches in developing domain-specific language models with limited
data availability.

</details>


### [42] [CreativityPrism: A Holistic Benchmark for Large Language Model Creativity](https://arxiv.org/abs/2510.20091)
*Zhaoyi Joey Hou,Bowei Alvin Zhang,Yining Lu,Bhiman Kumar Baghel,Anneliese Brei,Ximing Lu,Meng Jiang,Faeze Brahman,Snigdha Chaturvedi,Haw-Shiuan Chang,Daniel Khashabi,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: 该论文提出了一个分解创造力的新评价框架CreativityPrism，从质量、创新性、多样性三方面对LLM进行多领域多指标测评，发现模型创造力表现和评估维度间存在复杂差异，呼吁从整体视角评估AI创造力。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型创造力的评价基本处于碎片化状态，定义和测量标准因领域和任务不同而有较大差异，缺乏统一和系统性的评估框架。鉴于创造力并非一个固定概念，作者期望提出更全面的创造力评估方法。

Method: 提出了一套名为CreativityPrism的评估分析框架，将创造力拆分为质量、创新性和多样性三个维度，结合九个任务、三个领域（发散性思维、创意写作、逻辑推理）、二十项任务专属评价指标，对创作性进行细致考量。并采用该框架测试并分析了17个主流闭源及开源大型语言模型的表现。

Result: 研究发现，闭源LLM与开源LLM在创造力评估上存在显著差距。模型在同一领域内各任务表现高度相关，不同领域间相关性较低。在评价维度上，质量与多样性高度相关，但创新性表现与其他两个维度相关性较弱。

Conclusion: 在某一创造力任务（或维度）上表现优异的模型，未必能在其他任务（或维度）上同样表现良好，强调了对LLM创造力全面、跨维度评估的必要性。

Abstract: Creativity is often seen as a hallmark of human intelligence. While large
language models (LLMs) are increasingly perceived as producing creative text,
there is still no holistic framework to evaluate their creativity across
diverse scenarios. Existing evaluation methods remain fragmented, with dramatic
variation across domains and tasks, largely due to differing definitions and
measurements of creativity. Inspired by the hypothesis that creativity is not
one fixed idea, we propose CreativityPrism, an evaluation analysis framework
that decomposes creativity into three dimensions: quality, novelty, and
diversity. CreativityPrism incorporates nine tasks, three domains, i.e.,
divergent thinking, creative writing, and logical reasoning, and twenty
evaluation metrics, which measure each dimension in task-specific, unique ways.
We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on
CreativityPrism and analyze the performance correlations among different
metrics and task domains. Our results reveal a notable gap between proprietary
and open-source models. Overall, model performance tends to be highly
correlated across tasks within the same domain and less so across different
domains. Among evaluation dimensions, diversity and quality metrics show strong
correlations - models that perform well on one often excel on the other -
whereas novelty exhibits much weaker correlation with either. These findings
support our hypothesis that strong performance in one creativity task or
dimension does not necessarily generalize to others, underscoring the need for
a holistic evaluation of LLM creativity.

</details>


### [43] [Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning](https://arxiv.org/abs/2510.20098)
*Yajie Li,Albert Galimov,Mitra Datta Ganapaneni,Pujitha Thejaswi,De Meng,Priyanshu Kumar,Saloni Potdar*

Main category: cs.CL

TL;DR: ARTER提出了一种高效实体链接系统，通过智能划分实体提及，结合低成本实体链接与有针对性的LLM推理，显著提升了精度和资源利用率，兼顾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 实体链接任务传统上需要大量标注数据及模型微调，近期方法虽利用大语言模型（LLM）通过提示降低训练需求，但LLM推理开销大，效率低。该论文旨在提出高性能、低资源消耗的新方法，解决当前LLM实体链接方法存在的效率瓶颈。

Method: 提出了ARTER（Adaptive Routing and Targeted Entity Reasoning）结构化流水线。该方法结合候选生成、基于上下文评分、自适应路由与选择性推理，对实体提及进行划分，并分别用低计算量实体链接器（如ReFinED）及有针对性的LLM推理处理易难案例。ARTER通过嵌入和LLM两种信号，对检索到的候选进行分类，实现资源分配优化。

Result: 在五个标准数据集上，ARTER优于ReFinED实体链接器，最高提升4.47%，平均提升2.53%。与将所有实体都用LLM推理的流水线相比，ARTER性能相近，但LLM token消耗降低一半，效率显著提升。

Conclusion: ARTER方法兼顾了实体链接的精度与效率，既减少了对LLM资源的依赖，也实现了高性能，对实际应用具备较大推动意义。

Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and
extensive model fine-tuning. While recent few-shot methods leverage large
language models (LLMs) through prompting to reduce training requirements, they
often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER
(Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline
that achieves high performance without deep fine-tuning by strategically
combining candidate generation, context-based scoring, adaptive routing, and
selective reasoning. ARTER computes a small set of complementary signals(both
embedding and LLM-based) over the retrieved candidates to categorize contextual
mentions into easy and hard cases. The cases are then handled by a
low-computational entity linker (e.g. ReFinED) and more expensive targeted
LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms
ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets,
and performs comparably to pipelines using LLM-based reasoning for all
mentions, while being as twice as efficient in terms of the number of LLM
tokens.

</details>


### [44] [BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation](https://arxiv.org/abs/2510.20151)
*Haoyuan Li,Zhengyuan Shen,Sullam Jeoung,Yueyan Chen,Jiayu Li,Qi Zhu,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.CL

TL;DR: 本文提出的BoundRL方法，通过token级分割和标签预测，大幅提升了小模型处理复杂结构化文本的能力，并显著减少推理成本。强化学习与中间候选机制进一步优化了性能。


<details>
  <summary>Details</summary>
Motivation: 随着结构化文本复杂性的增加，传统的句子或段落级分割方法难以适应包含表格、代码、占位符等内容的多样文本。有效进行语义分割成为亟需解决的问题。

Method: 提出了BoundRL方法，实现对长结构化文本的token级分割与标签预测。创新点包括仅生成起始token序列，然后在原文本中定位并重建完整内容，以大幅降低推理成本和减少幻觉现象。采用了能验证奖励的强化学习（RLVR），且奖励函数同时优化重建的准确性与语义一致性，并通过扰动部分生成段落引入中间候选，缓解熵塌陷问题。

Result: 实验显示，BoundRL可让小规模（1.7B参数）语言模型在复杂生成式AI提示的结构化文本分割任务上超越了大模型的few-shot提示性能。采用RLVR和自设计奖励后，效果显著优于监督微调，中间候选机制也进一步提升了性能和泛化能力。

Conclusion: BoundRL以高效且有效的方式，实现了复杂结构化文本的细粒度分割和标签预测，极大降低了计算成本，并能提升小模型在实际场景中的能力。

Abstract: As structured texts become increasingly complex across diverse domains --
from technical reports to generative AI prompts -- the need for text
segmentation into semantically meaningful components becomes critical. Such
texts often contain elements beyond plain language, including tables, code
snippets, and placeholders, which conventional sentence- or paragraph-level
segmentation methods cannot handle effectively. To address this challenge, we
propose BoundRL, a novel and efficient approach that jointly performs
token-level text segmentation and label prediction for long structured texts.
Instead of generating complete contents for each segment, it generates only a
sequence of starting tokens and reconstructs the complete contents by locating
these tokens within the original texts, thereby reducing inference costs by
orders of magnitude and minimizing hallucination. To adapt the model for the
output format, BoundRL~performs reinforcement learning with verifiable rewards
(RLVR) with a specifically designed reward that jointly optimizes document
reconstruction fidelity and semantic alignment. To mitigate entropy collapse,
it further constructs intermediate candidates by systematically perturbing a
fraction of generated sequences of segments to create stepping stones toward
higher-quality solutions. To demonstrate BoundRL's effectiveness on
particularly challenging structured texts, we focus evaluation on complex
prompts used for LLM applications. Experiments show that BoundRL enables small
language models (1.7B parameters) to outperform few-shot prompting of much
larger models. Moreover, RLVR with our designed reward yields significant
improvements over supervised fine-tuning, and incorporating intermediate
candidates further improves both performance and generalization.

</details>


### [45] [Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?](https://arxiv.org/abs/2510.20154)
*Anthony Dubreuil,Antoine Gourru,Christine Largeron,Amine Trabelsi*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型在立场检测任务中的群体与文本复杂度偏见，如错误关联支持大麻观点与低文本难度，将非裔方言与反特朗普立场对应，警示NLP社区关注此类敏感任务上的模型偏见问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在许多自然语言任务中表现出对社会群体的刻板印象和偏见，如仇恨言论检测或情感分析，但在立场检测任务上的偏见评估却很少被关注。立场检测常涉及政治和群体话题，是极为敏感的NLP任务。

Method: 作者利用零样本（zero-shot）设置，让大语言模型进行立场检测。对现有数据集中的帖子自动标注了两个属性：特定群体方言/俗语，以及文本复杂度/可读性，研究这些属性是否影响模型的立场判断。

Result: 研究发现，大语言模型在立场检测中表现出明显的刻板印象。例如，模型错误地将支持大麻立场与低文本复杂度关联，将非裔美国人方言错误地与反对特朗普关联。

Conclusion: 大语言模型在立场检测任务中存在显著偏见，受文本风格和复杂度等属性影响，亟需引起社区重视并进一步研究。

Abstract: Large Language Models inherit stereotypes from their pretraining data,
leading to biased behavior toward certain social groups in many Natural
Language Processing tasks, such as hateful speech detection or sentiment
analysis. Surprisingly, the evaluation of this kind of bias in stance detection
methods has been largely overlooked by the community. Stance Detection involves
labeling a statement as being against, in favor, or neutral towards a specific
target and is among the most sensitive NLP tasks, as it often relates to
political leanings. In this paper, we focus on the bias of Large Language
Models when performing stance detection in a zero-shot setting. We
automatically annotate posts in pre-existing stance detection datasets with two
attributes: dialect or vernacular of a specific group and text
complexity/readability, to investigate whether these attributes influence the
model's stance detection decisions. Our results show that LLMs exhibit
significant stereotypes in stance detection tasks, such as incorrectly
associating pro-marijuana views with low text complexity and African American
dialect with opposition to Donald Trump.

</details>


### [46] [DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking](https://arxiv.org/abs/2510.20168)
*Tian Lan,Bin Zhu,Qianghuai Jia,Junyang Ren,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文提出DeepWideSearch基准，首次系统评测智能体同时处理信息深度与广度检索的能力。实验发现当前方法表现极差，暴露出多项关键局限，并公开资源以推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用（如市场分析、商业发展）中，智能体无法兼顾深度推理和广泛收集的瓶颈，缺乏同时关注信息深度和信息广度的评测方法和基准。

Method: 提出并构建了DeepWideSearch基准，通过重新设计和组合已有数据集，生成覆盖15个领域的220道多跳深度和广度信息整合问题。通过大量实验分析智能体在该任务上的表现，并进行错误模式分析。

Result: 尽管采用了最先进技术，智能体在DeepWideSearch上的平均成功率仅为2.39%。系统性分析揭示出四大失败模式：缺乏反思、过度依赖内部知识、检索不足、上下文超载。

Conclusion: 当前的信息检索智能体在整合深度推理（multi-hop检索）和广度信息收集方面表现不足，现有最先进系统在新任务中的表现也很差。提出的DeepWideSearch能够显著揭示当前方法的局限性，有助于推动更强大的信息检索智能体的研究。

Abstract: Current search agents fundamentally lack the ability to simultaneously
perform \textit{deep} reasoning over multi-hop retrieval and
\textit{wide}-scale information collection-a critical deficiency for real-world
applications like comprehensive market analysis and business development. To
bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly
designed to evaluate agents to integrate depth and width in information
seeking. In DeepWideSearch, agents must process a large volume of data, each
requiring deep reasoning over multi-hop retrieval paths. Specifically, we
propose two methods to converse established datasets, resulting in a curated
collection of 220 questions spanning 15 diverse domains. Extensive experiments
demonstrate that even state-of-the-art agents achieve only 2.39% average
success rate on DeepWideSearch, highlighting the substantial challenge of
integrating depth and width search in information-seeking tasks. Furthermore,
our error analysis reveals four failure modes: lack of reflection, overreliance
on internal knowledge, insufficient retrieval, and context overflow-exposing
key limitations in current agent architectures. We publicly release
DeepWideSearch to catalyze future research on more capable and robust
information-seeking agents.

</details>


### [47] [Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2510.20176)
*Yuhang Zhou,Mingrui Zhang,Ke Li,Mingyi Wang,Qiao Liu,Qifei wang,Jiayi Liu,Fei Liu,Serena Li,Weiwi Li,Mingze Gao,Abhishek Kumar,Xiangjun Fan,Zhuokai Zhao,Lizhu Zhang*

Main category: cs.CL

TL;DR: 通过多智能体协作和强化学习训练，Mixture-of-Minds在表格理解任务上超越现有方法，实现了更高的推理准确率。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，对表格的理解和推理非常重要，但现有的大语言模型方法存在局限性：微调方法容易出算术错误和幻觉，工具方法虽然表格操作精准却缺乏语义理解。因此，需要整合强推理能力和可靠表格处理的新方法。

Method: 提出了Mixture-of-Minds，一个多智能体框架，将表格推理任务分解为规划、编码和回答三个专门角色，通过代码执行实现精准表格操作。进一步采用自我改进训练框架，利用蒙特卡洛树搜索（MCTS）生成伪金标准轨迹，并通过强化学习优化智能体。

Result: Mixture-of-Minds在TableBench数据集上取得62.13%的表现，超过了OpenAI-o4-mini-high模型，显示出该方法在表格理解上的显著提升。

Conclusion: 结构化的多智能体工作流结合强化学习，有望显著提升表格推理和理解能力。

Abstract: Understanding and reasoning over tables is a critical capability for many
real-world applications. Large language models (LLMs) have shown promise on
this task, but current approaches remain limited. Fine-tuning based methods
strengthen language reasoning; yet they are prone to arithmetic errors and
hallucination. In contrast, tool-based methods enable precise table
manipulation but rely on rigid schemas and lack semantic understanding. These
complementary drawbacks highlight the need for approaches that integrate robust
reasoning with reliable table processing. In this work, we propose
Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into
three specialized roles: planning, coding, and answering. This design enables
each agent to focus on a specific aspect of the task while leveraging code
execution for precise table manipulation. Building on this workflow, we
introduce a self-improvement training framework that employs Monte Carlo Tree
Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents
with reinforcement learning (RL). Extensive experiments show that
Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and
surpassing OpenAI-o4-mini-high. These results demonstrate the promise of
combining structured multi-agent workflows with RL to advance table
understanding.

</details>


### [48] [Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models](https://arxiv.org/abs/2510.20198)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.CL

TL;DR: 该研究系统性地考察了LLMs基于文本输入的空间推理能力，发现其在高复杂度空间任务上的表现明显下滑，揭示了当前语言模型在空间能力上的结构性局限，为后续跨语言与空间范畴的研究指明了重要方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在语言理解方面已取得显著进展，但它们在空间推理和抽象认知能力上的局限尚未充分探讨。该研究旨在揭示LLMs在空间任务上的有效性和潜在不足，为未来语言与空间融合的基准测试提供参考。

Method: 构建了五种任务（象限定位、几何变换、距离评估、单词搜索、滑块谜题）来评估LLMs在以文本输入为基础的网格结构环境中的空间推理与计算能力，并通过逐步增大任务的网格规模提升复杂度。

Result: LLMs在小规模、低复杂度任务上能获得中等水平的表现，但随着任务规模和复杂度的提升，准确率平均下降42.7%，某些任务下跌幅高达84%。只要初始准确率超过50%的测试，准确率下跌均大于48%。

Conclusion: 现有的大型语言模型（LLMs）在涉入更高复杂性的空间推理任务时表现迅速下降，表明其底层架构缺乏对空间表征的有效建构。

Abstract: This paper explores the spatial reasoning capability of large language models
(LLMs) over textual input through a suite of five tasks aimed at probing their
spatial understanding and computational abilities. The models were tested on
both fundamental spatial reasoning and multi-step problem-solving within
structured grid-based environments using tasks such as quadrant identification,
geometric transformations, distance evaluation, word searches, and tile
sliding. Each task was scaled in complexity through increasing grid dimensions,
requiring models to extend beyond simple pattern recognition into abstract
spatial reasoning. Our results reveal that while LLMs demonstrate moderate
success in all tasks with small complexity and size, performance drops off
rapidly as scale increases, with an average loss in accuracy of 42.7%, and
reaching as high as 84%. Every test that began with over 50% accuracy showed a
loss of at least 48%, illustrating the consistent nature of the deterioration.
Furthermore, their struggles with scaling complexity hint at a lack of robust
spatial representations in their underlying architectures. This paper
underscores the gap between linguistic and spatial reasoning in LLMs, offering
insights into their current limitations, and laying the groundwork for future
integrative benchmarks at the intersection of language and geometry.

</details>


### [49] [Decoding-Free Sampling Strategies for LLM Marginalization](https://arxiv.org/abs/2510.20208)
*David Pohl,Marco Cognetta,Junyoung Lee,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 本文针对LLM输出概率边缘化问题，提出无需生成的采样方法，大幅提高效率且精度可接受，在多项实际任务中证实其实用性。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型采用子词分词技术，以在模型体积、推理速度和词汇覆盖率之间权衡。但这种方法导致推理时仅评估一种特定分词方式的概率，而实际同一文本可有多种分词表示。近期研究建议采用边缘化方式，即综合所有可能分词的概率质量。这一方法但因分词组合数量巨大而计算困难，常用采样近似，但采样需模型生成内容，代价高昂，受限于运行预算，影响近似精度。

Method: 本文提出了一类无需模型生成步骤的采样策略，即“解码无关采样”(decoding-free sampling)，这种方法完全依赖于廉价且与模型、分词器无关的采样手段，仅需计算给定分词的序列概率，速度远快于生成采样。

Result: 实验在多种开源模型上验证了解码无关采样策略的近似质量和速度，结果表明该方法在运行成本极低的情况下能够获得足够准确的边缘概率估计，并在一系列下游推理任务中应用取得有效结果。

Conclusion: 解码无关采样策略在边缘化LLM输出概率时既高效又准确，为下游推理任务提供了实用和可扩展的新方法。

Abstract: Modern language models operate on subword-tokenized text in order to make a
trade-off between model size, inference speed, and vocabulary coverage. A side
effect of this is that, during inference, models are evaluated by measuring the
probability of only the specific tokenization produced as the output, despite
there being many possible ways to represent the same text with a subword
vocabulary. Recent studies have argued instead for evaluating LLMs by
marginalization - the probability mass of all tokenizations of a given text.
  Marginalization is difficult due to the number of possible tokenizations of a
text, so often approximate marginalization is done via sampling. However, a
downside of sampling is that an expensive generation step must be performed by
the LLM for each sample, which limits the number of samples that can be
acquired given a runtime budget, and therefore also the accuracy of the
approximation. Since computing the probability of a sequence given the
tokenization is relatively cheap compared to actually generating it, we
investigate sampling strategies that are decoding-free - they require no
generation from the LLM, instead relying entirely on extremely cheap sampling
strategies that are model and tokenizer agnostic.
  We investigate the approximation quality and speed of decoding-free sampling
strategies for a number of open models to find that they provide sufficiently
accurate marginal estimates at a small fraction of the runtime cost and
demonstrate its use on a set of downstream inference tasks.

</details>


### [50] [Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders](https://arxiv.org/abs/2510.20239)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.CL

TL;DR: 该研究提出一个结合文本、音频和面部特征的三模态融合方法，用于同时评估抑郁和PTSD的严重程度。实验表明，该方法在多病共现场景下较单一模态更准确且更稳健，特征归因能辅助临床决策，推动自动精神疾病评估的实用性和解释性。


<details>
  <summary>Details</summary>
Motivation: 抑郁和PTSD常共病且症状相关，现有自动化评估方法多为二元且特定于单一疾病，临床诊断亟需兼容严重程度、跨疾病的估计和解释性决策支持。

Method: 融合面试文本（句级Transformer嵌入）、音频特征（Mel统计和增量）、及面部信号（动作单元、凝视、头部与姿态描述），通过标准化特征和校准的后融合分类器，实现抑郁症（PHQ-8，5级）与PTSD（3级）的多级严重程度预测，并输出特征归因分析。

Result: 三模态融合模型在多病共现的抑郁与PTSD评估上，准度和加权F1与最优单模态基线持平但鲁棒性优于基线，尤其在噪声或模态缺失情况下表现更佳。PTSD回归误差显著下降，类别一致性提升，极端类别识别可靠。特征消融显示文本对抑郁贡献最大，音频和面部信息对PTSD至关重要，特征归因与临床语言及行为标志相符。

Conclusion: 提出的三模态情感严重程度融合框架在抑郁症和PTSD的联合自动评估中表现优异，提高了决策曲线效用和模型鲁棒性，支持临床可重复性和医生参与的辅助诊断。

Abstract: Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.

</details>


### [51] [Context-level Language Modeling by Learning Predictive Context Embeddings](https://arxiv.org/abs/2510.20280)
*Beiya Dai,Yuliang Liu,Daozheng Xue,Qipeng Guo,Kai Chen,Xinbing Wang*

Main category: cs.CL

TL;DR: ContextLM通过结合next-token与next-context预测，提升了语言模型对长程语义的把握，在多种主流模型上带来了性能提升，并能高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型受限于单token级别预测，难以充分捕捉高层语义结构及长距离上下文关系。

Method: 提出了ContextLM框架，在传统NTP（下一个token预测）基础上，引入next-context prediction目标，通过预测多token语境，强化模型对局部与长程上下文的理解。

Result: 在GPT2和Pythia等多种模型（规模至1.5B参数）上实验证明，ContextLM在困惑度和下游任务表现上均实现了稳定提升，且几乎不增加计算开销。

Conclusion: ContextLM在保持兼容标准自回归评估方式的情况下，显著提升了语言模型的性能和长程语义处理能力。

Abstract: Next-token prediction (NTP) is the cornerstone of modern large language
models (LLMs) pretraining, driving their unprecedented capabilities in text
generation, reasoning, and instruction following. However, the token-level
prediction limits the model's capacity to capture higher-level semantic
structures and long-range contextual relationships. To overcome this
limitation, we introduce \textbf{ContextLM}, a framework that augments standard
pretraining with an inherent \textbf{next-context prediction} objective. This
mechanism trains the model to learn predictive representations of multi-token
contexts, leveraging error signals derived from future token chunks. Crucially,
ContextLM achieves this enhancement while remaining fully compatible with the
standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).
Extensive experiments on the GPT2 and Pythia model families, scaled up to
$1.5$B parameters, show that ContextLM delivers consistent improvements in both
perplexity and downstream task performance. Our analysis indicates that
next-context prediction provides a scalable and efficient pathway to stronger
language modeling, yielding better long-range coherence and more effective
attention allocation with minimal computational overhead.

</details>


### [52] [Citation Failure: Definition, Analysis and Efficient Mitigation](https://arxiv.org/abs/2510.20303)
*Jan Buchmann,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文针对LLM在RAG系统中出现的引用失败问题（回答准确但证据未完全提供）进行系统分析，提出CITECONTROL基准与CITENTION融合框架，显著提升了引用质量。


<details>
  <summary>Details</summary>
Motivation: RAG系统的引用本用于简化验证，但在引用失败时（回答正确但证据未完全引用）难以有效验证，并且和回答失败需区分。当前研究多未深入分析引用失败，本文旨在弥补这一空白并提出解决策略。

Method: 两步法：第一步系统研究引用失败的发生条件及其与回答和证据关系的影响，提出CITECONTROL基准测试多种复杂关系下的失败模式；第二步设计CITENTION框架融合多种引用技术进行性能提升。

Result: CITECONTROL实验显示，随着证据关系复杂度提升，引用失败频率增加。结合多种引用方法有助于改善引用性能。CITENTION框架验证有效，能显著提高LLM引用完整性及效果，且有较好迁移性。

Conclusion: 提出了CITENTION框架，结合生成式、注意力和检索方法，有效提升了LLM引用质量，在CITECONTROL及迁移任务中均取得显著改进。

Abstract: Citations from LLM-based RAG systems are supposed to simplify response
verification. However, this does not hold for citation failure, when a model
generates a helpful response, but fails to cite complete evidence. In contrast
to previous work, we propose to disentangle this from response failure, where
the response itself is flawed, and citing complete evidence is impossible. To
address citation failure, this work follows a two-step approach: (1) We study
when citation failure occurs and (2) how it can be mitigated. For step 1, we
extend prior work by investigating how the relation between response and
evidence affects citation quality. We introduce CITECONTROL, a benchmark that
systematically varies this relation to analyze failure modes. Experiments show
that failures increase with relational complexity and suggest that combining
citation methods could improve performance, motivating step 2. To improve LLM
citation efficiently, we propose CITENTION, a framework integrating generative,
attention-based, and retrieval-based methods. Results demonstrate substantial
citation improvements on CITECONTROL and in transfer settings. We make our data
and code publicly available.

</details>


### [53] [Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering](https://arxiv.org/abs/2510.20304)
*Lei Tang,Wei Zhou,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 该论文首次系统地评估了过程奖励模型在表格问答任务上的表现。发现目前模型结合文本和代码验证有助于答案筛选，但泛化能力不足，且步骤级验证难以提升准确率。文章揭示了PRMs在TQA上的局限性，并为建立更强健的过程验证器提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 尽管PRMs在数学等结构化领域取得了理想成绩，其在处理表格问答类半结构化任务上的通用性和有效性尚未被系统研究。TQA任务独具挑战，如充斥无关信息、步骤联系松散和特定领域推理等。

Method: 系统性评估了先进的生成式过程奖励模型在TQA任务中的效果，包括从答案层面和步骤层面对模型进行分析，同时研究了文本和代码验证结合的方法。

Result: 结合文本和代码的验证方式在解决方案选择方面有帮助，但模型对领域外数据的泛化能力较弱。步骤级验证与最终答案准确率的相关性低，原因可能是解决方案中的步骤间因果关系不紧密。

Conclusion: 当前过程奖励模型（PRMs）在表格问答任务（TQA）中的表现存在局限性，尤其是在弱因果联系和步骤间依赖不强的情况下很难准确验证和选择答案。

Abstract: Process reward models (PRMs) improve complex reasoning in large language
models (LLMs) by grading candidate solutions step-by-step and selecting answers
via aggregated step scores. While effective in domains such as mathematics,
their applicability to tasks involving semi-structured data, like table
question answering (TQA) remains unexplored. TQA poses unique challenges for
PRMs, including abundant irrelevant information, loosely connected reasoning
steps, and domain-specific reasoning. This work presents the first systematic
study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from
both answer and step perspectives. Results show that PRMs that combine textual
and code verification can aid solution selection but struggle to generalize to
out-of-domain data. Analysis reveals a weak correlation between performance in
step-level verification and answer accuracy, possibly stemming from weak step
dependencies and loose causal links. Our findings highlight limitations of
current PRMs on TQA and offer valuable insights for building more robust,
process-aware verifiers.

</details>


### [54] [Teaching Language Models to Reason with Tools](https://arxiv.org/abs/2510.20342)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: 本文提出CoRT，一种利用代码解释器优化大型推理模型（如OpenAI-o1），通过注入多样化提示数据和优化交互流程，使模型在数学推理任务上既更准确又更高效，提升效果达8%，token使用显著减少，相关资源已开源。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（如OpenAI-o1）在自然语言推理方面表现出色，但在处理复杂数学运算时存在效率低下或不准确问题。虽然可以通过集成计算工具（如Code Interpreter）解决部分问题，但模型内部概率推理与外部确定性知识之间存在冲突，导致模型推理过程低效。

Method: 提出了一种新的后训练框架CoRT（Code-Optimized Reasoning Training），用于教大型推理模型更有效利用Code Interpreter。该框架采用Hint-Engineering进行数据合成，在推理路径中的最佳位置注入多样化提示，生成高质量、代码集成的推理数据。采用拒绝采样和强化学习优化多轮CI使用和内部思考的交替过程，并通过监督微调对不同规模模型进行后训练。

Result: 利用30个高质量样本对1.5B到32B参数规模的模型进行了后训练。实验证明CoRT在五个具有挑战性的数学推理任务上，对DeepSeek-R1-Distill-Qwen-32B和1.5B模型分别带来了4%和8%的性能提升。同时，效率也显著提高，32B模型减少了约30%的token使用，1.5B模型减少了约50%的token使用。相关模型和代码已开源。

Conclusion: CoRT通过高质量训练数据和流程优化显著提升了大型推理模型在与外部代码解释器协同处理复杂数学推理任务时的准确性与效率，为模型与计算工具融合提供了有效范式。

Abstract: Large reasoning models (LRMs) like OpenAI-o1 have shown impressive
capabilities in natural language reasoning. However, these models frequently
demonstrate inefficiencies or inaccuracies when tackling complex mathematical
operations. While integrating computational tools such as Code Interpreters
(CIs) offers a promising solution, it introduces a critical challenge: a
conflict between the model's internal, probabilistic reasoning and the
external, deterministic knowledge provided by the CI, which often leads models
to unproductive deliberation. To overcome this, we introduce CoRT
(Code-Optimized Reasoning Training), a post-training framework designed to
teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a
new data synthesis strategy that strategically injects diverse hints at optimal
points within reasoning paths. This approach generates high-quality,
code-integrated reasoning data specifically tailored to optimize LRM-CI
interaction. Using this method, we have synthesized 30 high-quality samples to
post-train models ranging from 1.5B to 32B parameters through supervised
fine-tuning. CoRT further refines the multi-round interleaving of external CI
usage and internal thinking by employing rejection sampling and reinforcement
learning. Our experimental evaluations demonstrate CoRT's effectiveness,
yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B
and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging
mathematical reasoning datasets. Moreover, CoRT significantly enhances
efficiency, reducing token usage by approximately 30\% for the 32B model and
50\% for the 1.5B model compared to pure natural language reasoning baselines.
The models and code are available at: https://github.com/ChengpengLi1003/CoRT.

</details>


### [55] [Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models](https://arxiv.org/abs/2510.20351)
*Matteo Silvestri,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.CL

TL;DR: 本文发现大语言模型在表格推理任务中的高表现部分源于记忆公开数据集的语义信息，而非真正具备泛化推理能力。建议未来评估时注意消除语义泄露，提高评测的公平性和有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在结构化表格推理任务上的评估容易忽视数据集污染（如模型对公开数据的记忆），需辨析其推理能力和实际记忆之间的关系。

Method: 通过受控实验设计，对LLMs在含有语义线索（如有意义的列名或可解释类别值）和无语义线索的数据集上的表现进行比较，探索其泛化能力与记忆能力的界限。

Result: LLMs仅在含有强语义线索的数据集上表现较好，移除或打乱这些线索后，模型表现迅速降低至接近随机水平，揭示了语义泄露对评估结果的显著影响。

Conclusion: LLMs在表格数据推理任务中的优异表现，部分源自于记忆了已公开数据集，而非真正的推理或泛化能力。评估标准需要重新考虑以避免语义泄露。

Abstract: Large Language Models (LLMs) are increasingly evaluated on their ability to
reason over structured data, yet such assessments often overlook a crucial
confound: dataset contamination. In this work, we investigate whether LLMs
exhibit prior knowledge of widely used tabular benchmarks such as Adult Income,
Titanic, and others. Through a series of controlled probing experiments, we
reveal that contamination effects emerge exclusively for datasets containing
strong semantic cues-for instance, meaningful column names or interpretable
value categories. In contrast, when such cues are removed or randomized,
performance sharply declines to near-random levels. These findings suggest that
LLMs' apparent competence on tabular reasoning tasks may, in part, reflect
memorization of publicly available datasets rather than genuine generalization.
We discuss implications for evaluation protocols and propose strategies to
disentangle semantic leakage from authentic reasoning ability in future LLM
assessments.

</details>


### [56] [FreeChunker: A Cross-Granularity Chunking Framework](https://arxiv.org/abs/2510.20356)
*Wenxuan Zhang,Yuan-Hao Jiang,Yonghe Wu*

Main category: cs.CL

TL;DR: FreeChunker突破传统固定粒度分块，按句灵活组合检索，大幅提升检索效果和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统的分块策略使用固定粒度，依赖静态边界，难以适应不同查询需求，因此需要更灵活高效的分块方法。

Method: 提出FreeChunker跨粒度编码框架，以句子为原子单位，实现支持任意句子组合的灵活检索，代替传统静态分块。

Result: 在LongBench V2上，FreeChunker在检索性能上优于传统分块方法，且计算效率大幅提升。

Conclusion: FreeChunker通过灵活的分块策略提升了RAG系统在检索性能和效率方面的表现，展现了对复杂查询的更强适应性。

Abstract: Chunking strategies significantly impact the effectiveness of
Retrieval-Augmented Generation (RAG) systems. Existing methods operate within
fixed-granularity paradigms that rely on static boundary identification,
limiting their adaptability to diverse query requirements. This paper presents
FreeChunker, a Cross-Granularity Encoding Framework that fundamentally
transforms the traditional chunking paradigm: the framework treats sentences as
atomic units and shifts from static chunk segmentation to flexible retrieval
supporting arbitrary sentence combinations. This paradigm shift not only
significantly reduces the computational overhead required for semantic boundary
detection but also enhances adaptability to complex queries. Experimental
evaluation on LongBench V2 demonstrates that FreeChunker achieves superior
retrieval performance compared to traditional chunking methods, while
significantly outperforming existing approaches in computational efficiency.

</details>


### [57] [Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)](https://arxiv.org/abs/2510.20358)
*Francesca Padovani,Bastian Bunzeck,Manar Ali,Omar Momen,Arianna Bisazza,Hendrik Buschmeier,Sina Zarrieß*

Main category: cs.CL

TL;DR: 本文研究了用对话数据专门预训练与不同微调策略对小型语言模型的影响。结果显示，这些模型在标准测试中表现一般，但在对话相关任务上效果显著，特别是经过DPO微调后性能提升明显。


<details>
  <summary>Details</summary>
Motivation: 探索仅使用对话数据预训练是否能获得在形式和功能上适合的语言模型，并提升模型在实际对话任务中的能力。

Method: 使用对话数据进行预训练，随后采用多种微调策略（如PPO、DPO）来增强模型生成更具交流性的文本能力，并在标准测试和自定义测试任务上进行评估。

Result: 模型在大多数BabyLM标准测试上表现不佳，但在自定义的对话延续预测任务上表现出色。PPO微调对模型影响复杂甚至带来负面作用，而DPO微调则能进一步提升模型在自定义对话基准上的表现。

Conclusion: 对话数据专属预训练的小型语言模型在特定对话任务上表现优异，但在标准基准测试上表现不足。不同的微调方法对模型性能有不同影响。

Abstract: We investigate whether pre-training exclusively on dialogue data results in
formally and functionally apt small language models. Based on this pre-trained
llamalogue model, we employ a variety of fine-tuning strategies to enforce
"more communicative" text generations by our models. Although our models
underperform on most standard BabyLM benchmarks, they excel at dialogue
continuation prediction in a minimal pair setting. While PPO fine-tuning has
mixed to adversarial effects on our models, DPO fine-tuning further improves
their performance on our custom dialogue benchmark.

</details>


### [58] [The Impact of Negated Text on Hallucination with Large Language Models](https://arxiv.org/abs/2510.20375)
*Jaehyung Seo,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文首次系统探讨了否定文本对LLMs幻觉检测能力的影响。实验和分析发现，现有LLMs难以正确应对否定输入，易出现逻辑或忠实性问题。研究提示未来需针对否定性语境提升模型表现，提出了相应数据集和分析手段。


<details>
  <summary>Details</summary>
Motivation: 虽然近年来大型语言模型（LLMs）中幻觉（hallucination）现象的研究取得了积极进展，但针对否定文本对幻觉问题的影响尚未深入探讨。该研究希望填补这方面的研究空白。

Method: 提出了三个重要的未解答的科学问题，并通过构建新的NegHalu数据集，将现有的幻觉检测数据集用否定表达进行重构，设计实验证明LLMs在否定文本中的表现，同时对模型在处理否定输入时的内部状态进行追踪分析。

Result: 实验结果表明，LLMs在检测否定文本中的幻觉时表现较差，经常给出逻辑不一致或不真实的判断。模型处理否定输入时，内部状态存在难以缓解的挑战。

Conclusion: 否定文本会加剧LLMs幻觉检测的困难，当前模型不够可靠，提升其处理否定性信息的能力亟需关注。

Abstract: Recent studies on hallucination in large language models (LLMs) have been
actively progressing in natural language processing. However, the impact of
negated text on hallucination with LLMs remains largely unexplored. In this
paper, we set three important yet unanswered research questions and aim to
address them. To derive the answers, we investigate whether LLMs can recognize
contextual shifts caused by negation and still reliably distinguish
hallucinations comparable to affirmative cases. We also design the NegHalu
dataset by reconstructing existing hallucination detection datasets with
negated expressions. Our experiments demonstrate that LLMs struggle to detect
hallucinations in negated text effectively, often producing logically
inconsistent or unfaithful judgments. Moreover, we trace the internal state of
LLMs as they process negated inputs at the token level and reveal the
challenges of mitigating their unintended effects.

</details>


### [59] [VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation](https://arxiv.org/abs/2510.20381)
*Son T. Luu,Trung Vo,Hiep Nguyen,Khanh Quoc Tran,Kiet Van Nguyen,Vu Tran,Ngan Luu-Thuy Nguyen,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 该论文介绍了VLSP 2025 MLQA-TSR多模态法律问答任务及公开数据集，聚焦越南交通标志法规问答，展示了检索与问答两个子任务的最新结果。


<details>
  <summary>Details</summary>
Motivation: 推动越南多模态法律文本处理研究，建立交通标志法规多模态问答基准，为智能系统构建与评测创造条件。

Method: 设定二个子任务：多模态法律检索和多模态问答，评测相关模型在越南交通标志法规问答的数据集上的表现。

Result: 在多模态法律检索任务中最优F2分数为64.55%，多模态问答的准确率为86.30%。

Conclusion: 该任务为越南交通标志法规的多模态法律问答领域提供了重要的数据集和基线，为智能法律系统的研究奠定基础。

Abstract: This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question
answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025
MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal
question answering. The goal is to advance research on Vietnamese multimodal
legal text processing and to provide a benchmark dataset for building and
evaluating intelligent systems in multimodal legal domains, with a focus on
traffic sign regulation in Vietnam. The best-reported results on VLSP 2025
MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an
accuracy of 86.30% for multimodal question answering.

</details>


### [60] [NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew](https://arxiv.org/abs/2510.20386)
*Shaltiel Shmidman,Avi Shmidman,Moshe Koppel*

Main category: cs.CL

TL;DR: 本研究推出了基于现代NeoBERT架构的希伯来语及双语BERT模型，在相关基准测试中取得了显著提升，并已开源，有助于推动希伯来语自然语言处理的发展。


<details>
  <summary>Details</summary>
Motivation: BERT模型持续表现优异，但架构相对较旧，如今有现代化的改进模型如Llama3、Qwen3。近期现代化的BERT模型（ModernBERT和NeoBERT）已显著提升性能和扩展上下文窗口，但针对希伯来语的改进有限。作者希望开发更适合希伯来语的现代BERT模型。

Method: 提出NeoDictaBERT和NeoDictaBERT-bilingual模型，基于NeoBERT架构，专注于希伯来语文本。描述了训练过程，并在多项基准测试上评估表现。

Result: NeoDictaBERT及其双语版本在几乎所有希伯来语基准测试上优于现有模型，NeoDictaBERT-bilingual在检索任务上性能突出，超越同规模其他多语言模型。

Conclusion: 新模型为希伯来语NLP任务提供了更优的基础，并有助于希伯来语自然语言处理领域的研究与发展。作者已将模型开源，以推动社区进步。

Abstract: Since their initial release, BERT models have demonstrated exceptional
performance on a variety of tasks, despite their relatively small size
(BERT-base has ~100M parameters). Nevertheless, the architectural choices used
in these models are outdated compared to newer transformer-based models such as
Llama3 and Qwen3. In recent months, several architectures have been proposed to
close this gap. ModernBERT and NeoBERT both show strong improvements on English
benchmarks and significantly extend the supported context window. Following
their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual:
BERT-style models trained using the same architecture as NeoBERT, with a
dedicated focus on Hebrew texts. These models outperform existing ones on
almost all Hebrew benchmarks and provide a strong foundation for downstream
tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on
retrieval tasks, outperforming other multilingual models of similar size. In
this paper, we describe the training process and report results across various
benchmarks. We release the models to the community as part of our goal to
advance research and development in Hebrew NLP.

</details>


### [61] [Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction](https://arxiv.org/abs/2510.20411)
*Suchir Salhan,Hongyi Gu,Donya Rooein,Diana Galvan-Sosa,Gabrielle Gaudeau,Andrew Caines,Zheng Yuan,Paula Buttery*

Main category: cs.CL

TL;DR: 提出ContingentChat框架对BabyLM模型进行后训练，提升了模型在多轮儿童对话中的语法性和连贯性，但对话应变性依然难以完全解决。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中的“应变性”指的是交流双方之间的及时、直接、有意义的互动，本文关注如何提升儿童与照护者的多轮对话质量。现有BabyLM模型在对话的应变性方面存在挑战。

Method: 作者提出了ContingentChat，这是一种教师-学生框架，针对BabyLM模型通过后训练过程和新的对齐数据集进行微调，同时测试了自适应教师解码策略。

Result: 通过新的对齐数据集后训练之后，BabyLM模型在语法和连贯性上提升明显，而自适应教师解码策略带来的提升有限。

Conclusion: ContingentChat展示了针对性后训练可以提升模型多轮对话的质量，但应变性问题依然是BabyLM面临的难题。

Abstract: Multi-turn dialogues between a child and a caregiver are characterized by a
property called contingency - that is, prompt, direct, and meaningful exchanges
between interlocutors. We introduce ContingentChat, a teacher-student framework
that benchmarks and improves multi-turn contingency in a BabyLM trained on 100M
words. Using a novel alignment dataset for post-training, BabyLM generates
responses that are more grammatical and cohesive. Experiments with adaptive
teacher decoding strategies show limited additional gains. ContingentChat
demonstrates the benefits of targeted post-training for dialogue quality and
indicates that contingency remains a challenging goal for BabyLMs.

</details>


### [62] [LM-mixup: Text Data Augmentation via Language Model based Mixup](https://arxiv.org/abs/2510.20449)
*Zhijie Deng,Zhouan Shen,Ling Li,Yao Zhou,Zhaowei Zhu,Yanji He,Wei Wang,Jiaheng Wei*

Main category: cs.CL

TL;DR: 该论文首次系统定义并解决了指令蒸馏任务，提出的LM-Mixup方法可将低质量或冗余指令数据转化为高质量训练数据，显著提升LLM微调效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 指令微调对于大语言模型（LLM）的对齐至关重要，但高质量数据稀缺，低质量数据虽多却往往被丢弃，造成信息损失。现有方法难以有效利用这些低质量数据，相关评价标准也不明确。

Method: 提出了“指令蒸馏”任务，通过综合性数据构建流程制作了包含低质量指令及其高质量蒸馏对的MIXTURE数据集。方法LM-Mixup先用MIXTURE做有监督微调，再结合三重互补奖赏（质量、语义一致性、格式合规）使用Group Relative Policy Optimization强化学习优化。

Result: LM-Mixup能够有效利用不完善的数据集，仅用约3%的蒸馏数据即超过对全量数据的训练，并与SOTA高质量数据筛选法在多个基准上竞争。

Conclusion: 只要合理蒸馏与增强，低质量数据也是极具价值的资源，通过LM-Mixup可显著提升指令微调LLM的效率与性能。

Abstract: Instruction tuning is crucial for aligning Large Language Models (LLMs), yet
the quality of instruction-following data varies significantly. While
high-quality data is paramount, it is often scarce; conversely, abundant
low-quality data is frequently discarded, leading to substantial information
loss. Existing data augmentation methods struggle to augment this low-quality
data effectively, and the evaluation of such techniques remains poorly defined.
To address this, we formally define the task of Instruction Distillation:
distilling multiple low-quality and redundant inputs into high-quality and
coherent instruction-output pairs. Specifically, we introduce a comprehensive
data construction pipeline to create MIXTURE, a 144K-sample dataset pairing
low-quality or semantically redundant imperfect instruction clusters with their
high-quality distillations. We then introduce LM-Mixup, by first performing
supervised fine-tuning on MIXTURE and then optimizing it with reinforcement
learning. This process uses three complementary reward signals: quality,
semantic alignment, and format compliance, via Group Relative Policy
Optimization (GRPO). We demonstrate that LM-Mixup effectively augments
imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for
only about 3% of the entire dataset, not only surpasses full-dataset training
but also competes with state-of-the-art high-quality data selection methods
across multiple benchmarks. Our work establishes that low-quality data is a
valuable resource when properly distilled and augmented with LM-Mixup,
significantly enhancing the efficiency and performance of instruction-tuned
LLMs.

</details>


### [63] [Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models](https://arxiv.org/abs/2510.20460)
*Christian Hobelsberger,Theresa Winner,Andreas Nawroth,Oliver Mitevski,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 本文系统评测了四种LLM输出不确定性估计方法，发现CoCoA混合方法在提高输出可靠性方面表现最佳，并给出了应用中选用依据建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出的可靠性难以保证，需要为其输出的置信度与不确定性建立量化方法，以提升实际应用安全性。

Method: 系统性评估了四种LLM输出置信度估计方法：VCE、MSP、Sample Consistency以及混合型方法CoCoA，并在四个问答任务上用最新开源LLM进行实验。

Result: 不同的不确定性度量捕捉了模型置信度的不同方面，混合方法CoCoA总体效果最好，在答案校准与区分度方面表现优异。

Conclusion: 推荐按具体应用需求选择不确定性评估方法，混合型CoCoA在综合可靠性上有优势，并分析了各方法的权衡。

Abstract: Large language models (LLMs) produce outputs with varying levels of
uncertainty, and, just as often, varying levels of correctness; making their
practical reliability far from guaranteed. To quantify this uncertainty, we
systematically evaluate four approaches for confidence estimation in LLM
outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For
the evaluation of the approaches, we conduct experiments on four
question-answering tasks using a state-of-the-art open-source LLM. Our results
show that each uncertainty metric captures a different facet of model
confidence and that the hybrid CoCoA approach yields the best reliability
overall, improving both calibration and discrimination of correct answers. We
discuss the trade-offs of each method and provide recommendations for selecting
uncertainty measures in LLM applications.

</details>


### [64] [Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs](https://arxiv.org/abs/2510.20475)
*Lukas Edman,Alexander Fraser*

Main category: cs.CL

TL;DR: 本文提出了一种改进的掩码语言建模（MLM）方法，并应用于BabyLM Challenge 2025，在(Super)GLUE任务上显著提升效果。同时，该方法引入子词嵌入以增强形态泛化能力，最终在strict-small赛道超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 标准的MLM方法在语言模型微调和泛化能力上存在局限，尤其是在数据受限的小模型环境下，迫切需要提升其表现和泛化能力。

Method: 调整MLM的掩码概率使其根据模型预测能力自适应变化，并结合子词层面的词嵌入技术提升形态学泛化性能。

Result: 新型MLM方法大幅提升了模型在(Super)GLUE任务上的表现，采用子词嵌入后模型的形态学泛化能力进一步增强，在strict-small评测赛道超越了原有基线。

Conclusion: 改进后的MLM在小规模数据集和多种评价任务上优于标准MLM，提升了模型的泛化与预测能力。

Abstract: We describe our strategy for the 2025 edition of the BabyLM Challenge. Our
main contribution is that of an improved form of Masked Language Modeling
(MLM), which adapts the probabilities of the tokens masked according to the
model's ability to predict them. The results show a substantial increase in
performance on (Super)GLUE tasks over the standard MLM. We also incorporate
sub-token embeddings, finding that this increases the model's morphological
generalization capabilities. Our submission beats the baseline in the
strict-small track.

</details>


### [65] [RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging](https://arxiv.org/abs/2510.20479)
*Bowen Wang,Haiyuan Wan,Liwen Shi,Chen Yang,Peng He,Yue Ma,Haochen Han,Wenhao Li,Tiao Tan,Yongjian Li,Fangming Liu,Yifan Gong,Sheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了RECALL，无需历史数据即可实现LLM多领域持续学习，通过表示感知的模型合并，有效对抗遗忘且性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有连续学习方法要么需要原始数据或任务标签，要么会牺牲模型性能。而在实际应用中，经常无法获得历史数据或任务边界，因此需要更高效的数据无关方案。

Method: RECALL利用LLM的内部表示作为知识代理，通过聚类典型样本计算模型间层级隐藏表示的相似性，并采用自适应、分层的参数融合对齐模型知识，无需历史数据。

Result: 在五个NLP任务和多个连续学习场景下，RECALL在知识保留和泛化性能均超过了对比基线，证实了其可扩展性和有效性。

Conclusion: RECALL可以实现多领域无缝整合，并能有效抵抗灾难性遗忘，知识保留和泛化能力优于现有方法。

Abstract: We unveil that internal representations in large language models (LLMs) serve
as reliable proxies of learned knowledge, and propose RECALL, a novel
representation-aware model merging framework for continual learning without
access to historical data. RECALL computes inter-model similarity from
layer-wise hidden representations over clustered typical samples, and performs
adaptive, hierarchical parameter fusion to align knowledge across models. This
design enables the preservation of domain-general features in shallow layers
while allowing task-specific adaptation in deeper layers. Unlike prior methods
that require task labels or incur performance trade-offs, RECALL achieves
seamless multi-domain integration and strong resistance to catastrophic
forgetting. Extensive experiments across five NLP tasks and multiple continual
learning scenarios show that RECALL outperforms baselines in both knowledge
retention and generalization, providing a scalable and data-free solution for
evolving LLMs.

</details>


### [66] [Steering Evaluation-Aware Language Models To Act Like They Are Deployed](https://arxiv.org/abs/2510.20487)
*Tim Tian Hua,Andrew Qin,Samuel Marks,Neel Nanda*

Main category: cs.CL

TL;DR: 本文提出通过激活引导向量抑制大型语言模型在安全评估时的评估意识，使其行为更贴近真实部署情境，提高安全评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在被评估时会调整自身行为以看起来更符合要求，导致安全评估结果不可靠。本文希望解决模型在评估过程中过度优化自身表现的问题。

Method: 提出了激活引导（activation steering）技术，通过增加引导向量到LLM的激活值，抑制模型对评估场景的敏感性。具体包括两步训练：首先让模型通过持续预训练学会检测评估线索并据此调整行为；其次通过专家迭代训练模型在评估环境下使用特定行为。

Result: 该方法成功训练出具有评估意识的模型，并验证了激活引导能有效抑制模型评估意识，使其在评估过程中表现更像真实部署情景，即使存在评估线索。引导向量基于原始模型创建，证明了技术在实际评估流程中的价值。

Conclusion: 激活引导技术能显著提高LLM安全评估的可靠性，使模型在评估时更像真实部署，避免因评估意识影响结果。AI评估者可采用该方式改善安全评估流程。

Abstract: Large language models (LLMs) can sometimes detect when they are being
evaluated and adjust their behavior to appear more aligned, compromising the
reliability of safety evaluations. In this paper, we show that adding a
steering vector to an LLM's activations can suppress evaluation-awareness and
make the model act like it is deployed during evaluation. To study our steering
technique, we train an LLM to exhibit evaluation-aware behavior using a
two-step training process designed to mimic how this behavior could emerge
naturally. First, we perform continued pretraining on documents with factual
descriptions of the model (1) using Python type hints during evaluation but not
during deployment and (2) recognizing that the presence of a certain evaluation
cue always means that it is being tested. Then, we train the model with expert
iteration to use Python type hints in evaluation settings. The resulting model
is evaluation-aware: it writes type hints in evaluation contexts more than
deployment contexts. However, this gap can only be observed by removing the
evaluation cue. We find that activation steering can suppress evaluation
awareness and make the model act like it is deployed even when the cue is
present. Importantly, we constructed our steering vector using the original
model before our additional training. Our results suggest that AI evaluators
could improve the reliability of safety evaluations by steering models to act
like they are deployed.

</details>


### [67] [Robust Preference Alignment via Directional Neighborhood Consensus](https://arxiv.org/abs/2510.20498)
*Ruochen Mao,Yuling Shi,Xiaodong Gu,Jiaheng Wei*

Main category: cs.CL

TL;DR: 大语言模型通常只适应主流人类偏好，难以满足个性化需求。本文提出RPS方法，通过多偏好邻域采样与筛选，在无需额外训练的情况下，有效提升模型在罕见偏好上的出色表现，实现更可靠的个性化对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）通常只根据训练数据反映的主流、平均人类偏好进行调整，导致模型在应对特定、个别需求时表现不佳，即“偏好覆盖差距”。现有解决方法依赖于昂贵且耗时的再训练，但难以涵盖多样化偏好，模型灵活性与稳健性不足。

Method: 提出一种名为鲁棒偏好选择（RPS）的后处理、无需再训练的方法。核心思路是利用方向性邻域共识：针对用户的偏好请求，不仅生成基于单一具体偏好的回复，而是从相关偏好的邻域采样多个回复，形成候选池，再选出最符合用户原始意图的结果。同时提供理论框架证明这种生成策略优于强基线（传统多样本采样方法）。

Result: 实验覆盖三种主流对齐范式（DPA、DPO、SFT），RPS方法在应对训练数据代表性不足区域的复杂偏好时，无需重新训练模型即可显著提升对齐效果，在高难度偏好场景的胜率最高可达69%。

Conclusion: RPS是一种实用且有理论保障的方案，能在不改变模型参数的前提下，显著增强大语言模型与人类不同偏好的对齐鲁棒性和可靠性。

Abstract: Aligning large language models with human preferences is critical for
creating reliable and controllable AI systems. A human preference can be
visualized as a high-dimensional vector where different directions represent
trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet,
because the training data often reflects dominant, average preferences, LLMs
tend to perform well on common requests but fall short in specific, individual
needs. This mismatch creates a preference coverage gap. Existing methods often
address this through costly retraining, which may not be generalized to the
full spectrum of diverse preferences. This brittleness means that when a user's
request reflects a nuanced preference deviating from the training data's
central tendency, model performance can degrade unpredictably. To address this
challenge, we introduce Robust Preference Selection (RPS), a post-hoc,
training-free method by leveraging directional neighborhood consensus. Instead
of forcing a model to generate a response from a single, highly specific
preference, RPS samples multiple responses from a local neighborhood of related
preferences to create a superior candidate pool. It then selects the response
that best aligns with the user's original intent. We provide a theoretical
framework showing our neighborhood generation strategy is provably superior to
a strong baseline that also samples multiple candidates. Comprehensive
experiments across three distinct alignment paradigms (DPA, DPO, and SFT)
demonstrate that RPS consistently improves robustness against this baseline,
achieving win rates of up to 69% on challenging preferences from
under-represented regions of the space without any model retraining. Our work
presents a practical, theoretically-grounded solution for enhancing the
reliability of preference-aligned models.

</details>


### [68] [Hierarchical Sequence Iteration for Heterogeneous Question Answering](https://arxiv.org/abs/2510.20505)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.CL

TL;DR: 该论文提出HSEQ迭代方法，通过统一结构表示与智能体协同检索，显著提升多模态多步问答的准确性和效率，实现跨数据类型的高性能问答系统。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG（检索增强生成）在处理多步问题和异构证据源时容易出现准确率与速度等方面的权衡，效率和资源消耗受限，亟需更加统一且高效的方法。

Method: 提出了Hierarchical Sequence (HSEQ) Iteration方法，将文本、表格和知识图谱统一线性化为可逆的层次序列，并用结构标签标注；通过结构感知的迭代机制，辅助两个智能体完成证据检索与答案生成；还可循环精炼以纠正矛盾。

Result: 在HotpotQA、HybridQA/TAT-QA、MetaQA等多模态问答任务中，HSEQ方法在EM/F1指标上均优于多种基线（单步、多跳、Agent式RAG）；且具备高效率。

Conclusion: HSEQ方法统一支持文本、表格和知识图谱三类数据，利用结构引导和预算感知的迭代，提升回答准确性、效率及证据可溯性，具有较强的普适性和实用性。

Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions
and heterogeneous evidence sources, trading accuracy against latency and
token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration
for Heterogeneous Question Answering, a unified framework that (i) linearize
documents, tables, and knowledge graphs into a reversible hierarchical sequence
with lightweight structural tags, and (ii) perform structure-aware iteration to
collect just-enough evidence before answer synthesis. A Head Agent provides
guidance that leads retrieval, while an Iteration Agent selects and expands
HSeq via structure-respecting actions (e.g., parent/child hops, table
row/column neighbors, KG relations); Finally the head agent composes
canonicalized evidence to genearte the final answer, with an optional
refinement loop to resolve detected contradictions. Experiments on HotpotQA
(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1
gains over strong single-pass, multi-hop, and agentic RAG baselines with high
efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic
unification that enables a single policy to operate across text, tables, and
KGs without per-dataset specialization; (2) guided, budget-aware iteration that
reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and
(3) evidence canonicalization for reliable QA, improving answers consistency
and auditability.

</details>


### [69] [Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset](https://arxiv.org/abs/2510.20508)
*Paul Lerner,François Yvon*

Main category: cs.CL

TL;DR: 以往通过英文问卷模拟研究LLMs政治偏见有限，本文用欧洲议会多语种平行语料库，发现主流党派演讲在翻译中质量更高，揭示了多语种翻译中的政治偏见问题。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型（LLMs）的政治偏见研究多基于英文问卷模拟。本文提出一个新视角，从多语种翻译的公平性原则出发，研究政治偏见，以弥补以往方法的局限性。

Method: 本文系统性比较了欧洲议会（EP）演讲在多语种翻译中的质量，分析不同政治派别（左、中、右的主流党派及边缘党派）演讲在翻译中的表现。研究依赖于新构建的、包含政治归属信息的21语种多语平行语料库EuroParl，涵盖三年数据、1000多名发言人等丰富维度。

Result: 结果显示，主流党派（左、中、右）的演讲在自动翻译中质量显著高于边缘党派，揭示了语言模型存在政治偏向性。

Conclusion: 多语种机器翻译中的系统性偏差反映出大型语言模型在政治内容处理上的不公平性，这种偏见会影响信息传播的公平性与完整性。

Abstract: The political biases of Large Language Models (LLMs) are usually assessed by
simulating their answers to English surveys. In this work, we propose an
alternative framing of political biases, relying on principles of fairness in
multilingual translation. We systematically compare the translation quality of
speeches in the European Parliament (EP), observing systematic differences with
majority parties from left, center, and right being better translated than
outsider parties. This study is made possible by a new, 21-way multiparallel
version of EuroParl, the parliamentary proceedings of the EP, which includes
the political affiliations of each speaker. The dataset consists of 1.5M
sentences for a total of 40M words and 249M characters. It covers three years,
1000+ speakers, 7 countries, 12 EU parties, 25 EU committees, and hundreds of
national parties.

</details>


### [70] [ARC-Encoder: learning compressed text representations for large language models](https://arxiv.org/abs/2510.20535)
*Hippolyte Pilchen,Edouard Grave,Patrick Pérez*

Main category: cs.CL

TL;DR: 本文提出了ARC-Encoder，将长文本上下文高效压缩为连续向量，替代token输入，有效提升解码大模型时的推理效率，并能适配多种不同解码器，无需对LLM本身微调。ARC-Encoder在多个基准上达到最优性能，适用于多模型调用场景。


<details>
  <summary>Details</summary>
Motivation: 由于检索增强生成（Retrieval-Augmented Generation, RAG）与思维链（Chain-of-Thought）等技术的应用，大模型推理时的上下文越来越长，导致推理成本增加。现有的上下文压缩技术虽然能够缓解这一问题，但通常需要对目标模型进行微调或结构修改，会影响模型的通用能力。作者希望提出一种不依赖于模型微调的新型上下文压缩方法。

Method: 本文提出了一种新的上下文压缩方法：设计并系统研究了一种编码器（ARC-Encoder），将文本上下文压缩为比原始token数更少的连续表示，该表示可直接替代解码器LLM中的token embedding。作者系统性地分析了编码器的训练策略与结构选择，并使得ARC-Encoder可灵活适配多种解码器。

Result: 实验表明，ARC-Encoder能够在多种LLM使用场景下实现更优的性能（state-of-the-art），同时提升推理效率。它还能被灵活地适配到不同的LLM解码器，实现单一编码器对多模型的泛化能力。

Conclusion: ARC-Encoder是一种高效且灵活的上下文压缩方法，在提高计算效率的同时保持甚至提升了解码器LLM的效果，并能支持多种不同的LLM结构，对设计可移植的、兼容性强的编码器提供了有效解决方案。

Abstract: Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .

</details>


### [71] [The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts](https://arxiv.org/abs/2510.20543)
*Sangmitra Madhusudan,Kaige Chen,Ali Emami*

Main category: cs.CL

TL;DR: 本研究提出了可精确衡量语言模型是否真正理解句法结构还是仅仅依赖于语义关联的新基准数据集CenterBench，发现主流大模型在复杂语句下会更多依赖语义而非结构理解，从而暴露出在深层句法和因果理解任务上的短板。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在解析复杂句法结构时，究竟是在真正分析句法结构，还是仅仅依赖于已学到的语义关联？这一问题一直缺乏有效区分方法。

Method: 作者提出了CenterBench数据集，该数据集包含9720个关于中心嵌套句子的理解问题，每个句子都配有一个在语法上相同但语义不合常理的变体，并针对表层理解、句法依赖以及因果推理进行六项提问，通过测试六种语言模型来观察模型在不同复杂度下的表现。

Result: 随着句子复杂度增加，模型对语义合理句与不合理句的表现差距系统性扩大（中位差距最高可达26.8个百分点），说明模型易于放弃句法分析，转而依赖语义模式。而在考察因果推理的问题时，语义合理性的影响反而会降低模型的表现。推理类模型虽能提升准确率，但还是表现出语义捷径、过度推理与拒答等现象。相比之下，人类对语义的依赖并不随复杂度系统性增强。

Conclusion: CenterBench是首个能够识别语言模型在什么时候从结构分析转向语义模式匹配的框架，有助于深入理解模型的推理机制和局限性。

Abstract: When language models correctly parse "The cat that the dog chased meowed,"
are they analyzing syntax or simply familiar with dogs chasing cats? Despite
extensive benchmarking, we lack methods to distinguish structural understanding
from semantic pattern matching. We introduce CenterBench, a dataset of 9,720
comprehension questions on center-embedded sentences (like "The cat [that the
dog chased] meowed") where relative clauses nest recursively, creating
processing demands from simple to deeply nested structures. Each sentence has a
syntactically identical but semantically implausible counterpart (e.g., mailmen
prescribe medicine, doctors deliver mail) and six comprehension questions
testing surface understanding, syntactic dependencies, and causal reasoning.
Testing six models reveals that performance gaps between plausible and
implausible sentences widen systematically with complexity, with models showing
median gaps up to 26.8 percentage points, quantifying when they abandon
structural analysis for semantic associations. Notably, semantic plausibility
harms performance on questions about resulting actions, where following causal
relationships matters more than semantic coherence. Reasoning models improve
accuracy but their traces show semantic shortcuts, overthinking, and answer
refusal. Unlike models whose plausibility advantage systematically widens with
complexity, humans shows variable semantic effects. CenterBench provides the
first framework to identify when models shift from structural analysis to
pattern matching.

</details>


### [72] [GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning](https://arxiv.org/abs/2510.20548)
*Jinchang Luo,Mingquan Cheng,Fan Wan,Ni Li,Xiaoling Xia,Shuangshuang Tian,Tingcheng Bian,Haiwei Wang,Haohuan Fu,Yan Tao*

Main category: cs.CL

TL;DR: 作者针对多跳问答中的RAG系统在推理结构和证据利用上的不足，提出了可规划与奖励机制结合的新框架GlobalRAG，在多项评估中大幅提升了准确性，并以更少数据取得更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习增强的检索生成（RAG）在多跳问答中面临缺乏全局规划和执行不可信等瓶颈，导致多步推理及证据整合不良。

Method: 提出GlobalRAG框架，将问题分解为子目标，并通过强化学习的奖励设计（规划质量和子目标完成度）推动结构化的推理和高效证据利用；同时引入权重退火策略以平衡过程与结果导向的目标。

Result: 在数据量仅为强基线42%的情况下，GlobalRAG在标注集与未标注集平均提升EM与F1指标14.2%。

Conclusion: GlobalRAG能够有效提升多跳问答任务中的全局推理能力，显著超越现有强基线方法，并且训练数据需求更低。

Abstract: Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.

</details>


### [73] [Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search](https://arxiv.org/abs/2510.20567)
*Zhouwei Zhai,Mengxiang Chen,Haoyun Xia,Jin Li,Renquan Zhou,Min Yang*

Main category: cs.CL

TL;DR: 针对电商搜索存在的语义匹配和用户认知过程不匹配的问题，作者提出多智能体认知决策框架（MACDF），提升了复杂查询下的推荐效果和用户满意度，并经京东平台线上实测验证，表明该方法有望变革电商搜索。


<details>
  <summary>Details</summary>
Motivation: 当前主流的电商搜索以检索排序为主，过于依赖用户查询和商品之间的简单匹配，无法满足用户在复杂决策过程中的多阶段认知需求，导致语义理解有缺陷、跨平台信息搜寻成本高、缺乏专业购物指导。

Method: 提出了一种多智能体认知决策框架（MACDF），将电商搜索由被动检索转变为主动决策支持，利用多智能体协作更好地模拟用户的认知和决策过程。

Result: MACDF在复杂查询场景（包括否定、多约束、推理需求等）下显著提升了推荐准确率和用户满意度。并在京东搜索平台的线上A/B测试中验证了实际效果。

Conclusion: 多智能体认知系统能有效提升电商搜索体验，具备革新现有范式的潜力。

Abstract: The retrieval-ranking paradigm has long dominated e-commerce search, but its
reliance on query-item matching fundamentally misaligns with multi-stage
cognitive decision processes of platform users. This misalignment introduces
critical limitations: semantic gaps in complex queries, high decision costs due
to cross-platform information foraging, and the absence of professional
shopping guidance. To address these issues, we propose a Multi-Agent Cognitive
Decision Framework (MACDF), which shifts the paradigm from passive retrieval to
proactive decision support. Extensive offline evaluations demonstrate MACDF's
significant improvements in recommendation accuracy and user satisfaction,
particularly for complex queries involving negation, multi-constraint, or
reasoning demands. Online A/B testing on JD search platform confirms its
practical efficacy. This work highlights the transformative potential of
multi-agent cognitive systems in redefining e-commerce search.

</details>


### [74] [Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks](https://arxiv.org/abs/2510.20584)
*Jiangang Hao,Wenju Cui,Patrick Kyllonen,Emily Kerzabi*

Main category: cs.CL

TL;DR: 本研究证实，ChatGPT自动编码沟通数据在性别与种族群体间不存在显著偏差，支持其在大规模协作评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前大规模评估沟通与协作通常依赖人工劳动密集型的数据编码工作。虽然已有研究证明ChatGPT能通过编码规则实现人类级别的准确率，但其对不同群体（如性别和种族）是否存在偏差尚不明确。

Method: 使用典型的协作问题解决编码框架，结合谈判、问题解决和决策三种协作任务的数据，分析ChatGPT自动编码在性别和种族群体中的潜在偏差。

Result: 实验结果显示，ChatGPT编码在性别和种族群体间没有显著偏差。

Conclusion: ChatGPT可用于大规模协作和沟通评估，并不存在对主要人口群体的系统性偏差。

Abstract: Assessing communication and collaboration at scale depends on a labor
intensive task of coding communication data into categories according to
different frameworks. Prior research has established that ChatGPT can be
directly instructed with coding rubrics to code the communication data and
achieves accuracy comparable to human raters. However, whether the coding from
ChatGPT or similar AI technology exhibits bias against different demographic
groups, such as gender and race, remains unclear. To fill this gap, this paper
investigates ChatGPT-based automated coding of communication data using a
typical coding framework for collaborative problem solving, examining
differences across gender and racial groups. The analysis draws on data from
three types of collaborative tasks: negotiation, problem solving, and decision
making. Our results show that ChatGPT-based coding exhibits no significant bias
across gender and racial groups, paving the road for its adoption in
large-scale assessment of collaboration and communication.

</details>


### [75] [BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection](https://arxiv.org/abs/2510.20610)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: 对三个模型微调后做AI生成阿拉伯语文本检测，XLM-RoBERTa表现最好。多语言模型具备突出泛化能力，获比赛第五名。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语AI生成文本检测是一个具有挑战性的任务，现有模型的有效性尚需探索。作者希望比较多语言模型与专门的阿拉伯语模型在该任务上的表现。

Method: 采用三种预训练Transformer模型（AraELECTRA、CAMeLBERT和XLM-RoBERTa），并在提供的数据集上进行微调，完成二分类任务。

Result: 多语言模型XLM-RoBERTa的F1分数最高（0.7701），超越了专门的阿拉伯语模型。团队取得比赛第五名。

Conclusion: 多语言预训练模型在阿拉伯语AI生成文本检测任务中展现了强大的泛化能力，比专用模型表现更好，体现了该领域的复杂性与挑战。

Abstract: This paper details our submission to the Ara- GenEval Shared Task on Arabic
AI-generated text detection, where our team, BUSTED, se- cured 5th place. We
investigated the effec- tiveness of three pre-trained transformer mod- els:
AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each
model on the provided dataset for a binary classification task. Our findings
revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the
highest performance with an F1 score of 0.7701, outperforming the spe- cialized
Arabic models. This work underscores the complexities of AI-generated text
detection and highlights the strong generalization capa- bilities of
multilingual models.

</details>


### [76] [Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model](https://arxiv.org/abs/2510.20635)
*Haoyu Wang,Sihang Jiang,Yuyan Chen,Yitong Wang,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本研究基于五维好奇心量表，系统评估了大语言模型的好奇心表现。结果显示大模型较人类求知欲更强，但在不确定情境下更保守；且好奇行为有助于提升其推理与主动学习。结果支持LLMs具备类人好奇心及持续增强学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 好奇心是人类学习新知识的重要驱动力。随着大语言模型（LLMs）在自然语言处理中的进步，引发了有关这些模型是否具备类似人类“好奇驱动学习”能力的讨论。本文旨在探讨并量化LLMs的好奇心表现及其对模型推理和学习能力的影响。

Method: 以人类好奇心问卷五维好奇心量表（5DCR）为基础，设计了包含信息探索、刺激寻求、社交好奇等多维度的全面评估框架，用于评测LLMs展现出的好奇心水平。同时深入分析了好奇行为与模型思考方式之间的关系。

Result: 实验结果显示，LLMs展现出比人类更强的求知欲，但在不确定环境下依然偏向保守选择。进一步发现，增强的好奇行为能够提升模型的推理和主动学习能力。

Conclusion: LLMs具备与人类类似的好奇特质，并展现出通过好奇心促进学习和推理的潜力，为后续LLMs学习能力与创新研究提供了实验依据。

Abstract: Curiosity serves as a pivotal conduit for human beings to discover and learn
new knowledge. Recent advancements of large language models (LLMs) in natural
language processing have sparked discussions regarding whether these models
possess capability of curiosity-driven learning akin to humans. In this paper,
starting from the human curiosity assessment questionnaire Five-Dimensional
Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework
that covers dimensions such as Information Seeking, Thrill Seeking, and Social
Curiosity to assess the extent of curiosity exhibited by LLMs. The results
demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but
still tend to make conservative choices when faced with uncertain environments.
We further investigated the relationship between curiosity and thinking of
LLMs, confirming that curious behaviors can enhance the model's reasoning and
active learning abilities. These findings suggest that LLMs have the potential
to exhibit curiosity similar to that of humans, providing experimental support
for the future development of learning capabilities and innovative research in
LLMs.

</details>


### [77] [The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI](https://arxiv.org/abs/2510.20647)
*Alan Saji,Raj Dabre,Anoop Kunchukuttan,Ratish Puduppully*

Main category: cs.CL

TL;DR: 大型推理模型更擅长用英语解题，但翻译过程易导致错解，多语言本地推理机制亟待改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）在多种问答任务上表现优秀，但其多语言推理能力尚未被深入探讨。现实中，非英语问题往往被模型转化为英语进行推理，这带来了可解释性及语言文化细节处理的隐忧。

Method: 论文系统性地比较了LRM在英语与原问题语言中的推理表现。评估任务涵盖MGSM和GPQA Diamond，除答案准确率外，还分析了推理过程中的认知特性。

Result: 研究发现，英文推理轨迹展现了更高的认知行为，并且通常能获得更高的最终答案准确率，随着任务难度提升，表现差距进一步拉大。然而，模型以英语为中心的推理策略易出现“迷失翻译”现象，部分情况下翻译步骤会带来错误，反倒不如直接用原语言推理。

Conclusion: 虽然英语推理通常更准确、认知性更强，但英语中心化存在明显弊端，导致某些复杂场景下出错率上升。因此，提高模型在多语言环境下的本地化推理能力具有重要意义。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on mathematical,
scientific, and other question-answering tasks, but their multilingual
reasoning abilities remain underexplored. When presented with non-English
questions, LRMs often default to reasoning in English, raising concerns about
interpretability and the handling of linguistic and cultural nuances. We
systematically compare an LRM's reasoning in English versus the language of the
question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond
measuring answer accuracy, we also analyze cognitive attributes in the
reasoning traces. We find that English reasoning traces exhibit a substantially
higher presence of these cognitive behaviors, and that reasoning in English
generally yields higher final-answer accuracy, with the performance gap
increasing as tasks become more complex. However, this English-centric strategy
is susceptible to a key failure mode - getting "Lost in Translation," where
translation steps lead to errors that would have been avoided by question's
language reasoning.

</details>


### [78] [\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding](https://arxiv.org/abs/2510.20670)
*Junghyun Min,York Hay Ng,Sophia Chan,Helena Shunhua Zhao,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 该论文提出了粤语自然语言理解评测基准CantoNLU，涵盖多项任务并对不同模型进行了系统评测。结果表明，粤语适配模型性能优越，普通话模型在部分条件下也可用。该基准及相关资源已开放，助力粤语NLP研究。


<details>
  <summary>Details</summary>
Motivation: 粤语虽然使用者众多，但由于政策和分裂语言现象，相关资源较为匮乏，缺少有效的自然语言理解评测框架。

Method: 提出CantoNLU粤语自然语言理解评测基准，涵盖七个语法与语义任务（词义消歧、语法可接受性判断、语言识别、自然语言推理、情感分析、词性标注、依存句法分析），并对多种模型基线（普通话无粤语训练模型、持续在粤语文本基础上训练的普通话模型、从零训练的粤语单语模型）进行性能测试。

Result: 粤语适配模型整体表现最佳，单语模型在语法任务上表现更好；普通话模型在部分任务中有竞争力，表明在粤语数据稀缺时直接迁移也可行。

Conclusion: CantoNLU为粤语自然语言处理研究提供了基础评测工具与数据，推动领域发展，相关数据、代码与模型均公开释放。

Abstract: Cantonese, although spoken by millions, remains under-resourced due to policy
and diglossia. To address this scarcity of evaluation frameworks for Cantonese,
we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese natural
language understanding (NLU). This novel benchmark spans seven tasks covering
syntax and semantics, including word sense disambiguation, linguistic
acceptability judgment, language detection, natural language inference,
sentiment analysis, part-of-speech tagging, and dependency parsing. In addition
to the benchmark, we provide model baseline performance across a set of models:
a Mandarin model without Cantonese training, two Cantonese-adapted models
obtained by continual pre-training a Mandarin model on Cantonese text, and a
monolingual Cantonese model trained from scratch. Results show that
Cantonese-adapted models perform best overall, while monolingual models perform
better on syntactic tasks. Mandarin models remain competitive in certain
settings, indicating that direct transfer may be sufficient when Cantonese
domain data is scarce. We release all datasets, code, and model weights to
facilitate future research in Cantonese NLP.

</details>


### [79] [Neural Diversity Regularizes Hallucinations in Small Models](https://arxiv.org/abs/2510.20690)
*Kushal Chakrabarti,Nirmal Balachundhar*

Main category: cs.CL

TL;DR: 本文提出利用神经多样性机制（ND-LoRA）来降低语言模型产生幻觉的概率，经实验证明在不增加参数和数据的情况下，能有效提升模型输出的可靠性。因此，神经多样性或可作为参数规模和数据外提升模型能力的新途径。


<details>
  <summary>Details</summary>
Motivation: 现有提升语言模型可靠性主要依赖增加参数、算力、数据，但幻觉问题依然存在。想要在固定模型和数据规模下再提高语言模型的可靠性，需要探索新的机制。受投资组合理论启发，尝试通过提升模型“神经多样性”来降低幻觉发生率。

Method: 提出ND-LoRA（Neural Diversity Low-Rank Adaptation）架构，结合并行LoRA适配器和Barlow Twins正则化，通过消融实验和因果干预来验证神经多样性对减少幻觉的影响，并分析相关性与幻觉率的关系。

Result: ND-LoRA方法最高能减少25.6%、平均减少14.6%的模型幻觉率，对模型通用准确率无负面影响。实验表明LoRA适配器和正则化具有协同效应，神经多样性是主要干预因素。不同任务对神经多样性的最佳需求不同，说明该机制具备任务自适应性。相关性分析验证理论预测。

Conclusion: 神经多样性（即去相关的并行神经表示）能够有效减少语言模型幻觉发生率，并且作用独立于模型参数和数据规模，是提升模型可靠性的第三条途径。

Abstract: Language models continue to hallucinate despite increases in parameters,
compute, and data. We propose neural diversity -- decorrelated parallel
representations -- as a principled mechanism that reduces hallucination rates
at fixed parameter and data budgets. Inspired by portfolio theory, where
uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination
probability is bounded by representational correlation: $P(H) \leq
f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language
models need an optimal amount of neurodiversity. To validate this, we introduce
ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA
adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces
hallucinations by up to 25.6% (and 14.6% on average) without degrading general
accuracy. Ablations show LoRA adapters and regularization act synergistically,
causal interventions prove neurodiversity as the mediating factor and
correlational analyses indicate scale: a 0.1% neural correlation increase is
associated with a 3.8% hallucination increase. Finally, task-dependent
optimality emerges: different tasks require different amounts of optimal
neurodiversity. Together, our results highlight neural diversity as a third
axis of scaling -- orthogonal to parameters and data -- to improve the
reliability of language models at fixed budgets.

</details>


### [80] [Structure-Conditional Minimum Bayes Risk Decoding](https://arxiv.org/abs/2510.20700)
*Bryan Eikema,Anna Rutkiewicz,Mario Giulianelli*

Main category: cs.CL

TL;DR: 本文针对MBR解码在开放任务中的局限性，提出提升结构敏感性的效用函数改进方法，并通过新建数据集和评估指标验证，显示方法在生成质量和结构最优性上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: MBR解码在机器翻译领域效果显著，但在开放性任务（如对话和指令跟随）中可能因结果空间结构多样而受限。作者认为，现有MBR使用的基于相似度的效用函数可能会导致选出的回复仅具有代表性，但在某一具体隐含结构上次优。

Method: 提出三种轻量级效用函数的改进，增强MBR对生成结果隐含结构的敏感性。作者还自建数据集，涵盖对话行为、情感和回复结构三种隐含结构，并设计两项评估MBR结构最优性的指标，用于验证方法有效性。

Result: 分析显示，传统的相似度效用函数在结构最优性指标上表现较差，而改进方法明显提升结构最优性。在实际指令跟随数据集（如AlpacaEval和MT-Bench）上，结构敏感性增强可提高生成质量，优胜率最高提升13.7个百分点。

Conclusion: 对MBR进行结构敏感性改进能显著改善开放性任务中的生成质量，突破了基于相似性的传统方法表现瓶颈，具有现实应用价值。

Abstract: Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternative
to traditional generation strategies. While MBR has proven effective in machine
translation, where the variability of a language model's outcome space is
naturally constrained, it may face challenges in more open-ended tasks such as
dialogue or instruction-following. We hypothesise that in such settings,
applying MBR with standard similarity-based utility functions may result in
selecting responses that are broadly representative of the model's
distribution, yet sub-optimal with respect to any particular grouping of
generations that share an underlying latent structure. In this work, we
introduce three lightweight adaptations to the utility function, designed to
make MBR more sensitive to structural variability in the outcome space. To test
our hypothesis, we curate a dataset capturing three representative types of
latent structure: dialogue act, emotion, and response structure (e.g., a
sentence, a paragraph, or a list). We further propose two metrics to evaluate
the structural optimality of MBR. Our analysis demonstrates that common
similarity-based utility functions fall short by these metrics. In contrast,
our proposed adaptations considerably improve structural optimality. Finally,
we evaluate our approaches on real-world instruction-following benchmarks,
AlpacaEval and MT-Bench, and show that increased structural sensitivity
improves generation quality by up to 13.7 percentage points in win rate.

</details>


### [81] [User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios](https://arxiv.org/abs/2510.20721)
*Xiaoyuan Wu,Roshni Kaushik,Wenkai Li,Lujo Bauer,Koichi Onoue*

Main category: cs.CL

TL;DR: 用户在评估LLM对隐私敏感场景的回复时，意见分歧较大，而代理LLM之间评价一致但无法反映用户实际感受。未来应加强用户参与和优化LLM与用户感知一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种任务中被广泛应用，但在需要涉及个人隐私信息的情境下，LLMs如何识别和保护这些隐私尚存在问题。以往评估主要依赖LLM自身或代理LLM来判断隐私合规性，忽视了真实用户的视角。同时，帮助性和隐私保护之间的权衡也未被充分探讨。

Method: 研究设计了用户研究，邀请94位参与者在90个PrivacyLens隐私敏感场景中对LLM的相同回复进行 privacy-preservation 和 helpfulness 两方面的评价。这些结果与五个代理LLM的评价进行了对比。

Result: 用户在隐私保护和帮助性评价方面一致性较低，显示每个人对LLM回复的主观感受有很大差异。而五个代理LLM之间评价高度一致，但各自与用户评价相关性很低。

Conclusion: LLM回复在隐私和帮助性方面的用户认知高度个体化，代理LLM很难准确预测真实用户的感受。今后应更多进行以用户为中心的研究，并提升代理LLM与用户感知的对齐。

Abstract: Large language models (LLMs) have seen rapid adoption for tasks such as
drafting emails, summarizing meetings, and answering health questions. In such
uses, users may need to share private information (e.g., health records,
contact details). To evaluate LLMs' ability to identify and redact such private
information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with
real-life scenarios. Using these benchmarks, researchers have found that LLMs
sometimes fail to keep secrets private when responding to complex tasks (e.g.,
leaking employee salaries in meeting summaries). However, these evaluations
rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking
real users' perceptions. Moreover, prior work primarily focused on the
privacy-preservation quality of responses, without investigating nuanced
differences in helpfulness. To understand how users perceive the
privacy-preservation quality and helpfulness of LLM responses to
privacy-sensitive scenarios, we conducted a user study with 94 participants
using 90 scenarios from PrivacyLens. We found that, when evaluating identical
responses to the same scenario, users showed low agreement with each other on
the privacy-preservation quality and helpfulness of the LLM response. Further,
we found high agreement among five proxy LLMs, while each individual LLM had
low correlation with users' evaluations. These results indicate that the
privacy and helpfulness of LLM responses are often specific to individuals, and
proxy LLMs are poor estimates of how real users would perceive these responses
in privacy-sensitive scenarios. Our results suggest the need to conduct
user-centered studies on measuring LLMs' ability to help users while preserving
privacy. Additionally, future research could investigate ways to improve the
alignment between proxy LLMs and users for better estimation of users'
perceived privacy and utility.

</details>


### [82] [Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing](https://arxiv.org/abs/2510.20727)
*Xizhi Wu,Madeline S. Kreider,Philip E. Empey,Chenyu Li,Yanshan Wang*

Main category: cs.CL

TL;DR: 大语言模型（LLM）在从肿瘤临床记录中自动提取氟嘧啶治疗及毒性信息方面优于传统和深度学习方法，具备极高准确度，对药物安全和临床研究支持潜力巨大。


<details>
  <summary>Details</summary>
Motivation: 氟嘧啶类药物广泛用于结直肠癌和乳腺癌治疗，但常见副作用如手足综合征和心脏毒性往往只在临床记录中体现。为了提高此类信息的自动提取效率，作者致力于开发和评估NLP方法，辅助医疗数据整理和药物安全研究。

Method: 作者构建了一个包含236份临床记录的金标准数据集，由领域专家注释治疗方案及毒性相关类别。采用了包括规则方法、机器学习（随机森林、SVM、LR）、深度学习（BERT、ClinicalBERT）及大语言模型（LLM，含零样本及错误分析提示）在内的多种NLP方法，模型使用80:20训练-测试集划分进行评估。

Result: 在5个注释类目标中，LLM的错误分析提示在治疗和毒性信息提取方面均达到F1=1.000，表现最佳；零样本也在治疗F1=1.000，毒性F1=0.876；LR和SVM在毒性提取排名第二（F1=0.937）；BERT和ClinicalBERT在深度学习方法中表现有限（F1<0.9）；规则方法作为基线，F1约为0.857-0.858。

Conclusion: LLM为基础的NLP方法在提取氟嘧啶治疗与毒性信息方面效果最好，较传统方法与深度学习显著提升，未来有望支持肿瘤学研究和药物安全监测。

Abstract: Objective: Fluoropyrimidines are widely prescribed for colorectal and breast
cancers, but are associated with toxicities such as hand-foot syndrome and
cardiotoxicity. Since toxicity documentation is often embedded in clinical
notes, we aimed to develop and evaluate natural language processing (NLP)
methods to extract treatment and toxicity information.
  Materials and Methods: We constructed a gold-standard dataset of 236 clinical
notes from 204,165 adult oncology patients. Domain experts annotated categories
related to treatment regimens and toxicities. We developed rule-based, machine
learning-based (Random Forest, Support Vector Machine [SVM], Logistic
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language
models (LLM)-based NLP approaches (zero-shot and error-analysis prompting).
Models used an 80:20 train-test split.
  Results: Sufficient data existed to train and evaluate 5 annotated
categories. Error-analysis prompting achieved optimal precision, recall, and F1
scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot
prompting reached F1=1.000 for treatment and F1=0.876 for toxicities
extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning
underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and
ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods
served as our baseline with F1 scores of 0.857 in treatment and 0.858 in
toxicities.
  Discussion: LMM-based approaches outperformed all others, followed by machine
learning methods. Machine and deep learning approaches were limited by small
training data and showed limited generalizability, particularly for rare
categories.
  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine
treatment and toxicity information from clinical notes, and has strong
potential to support oncology research and pharmacovigilance.

</details>


### [83] [Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost](https://arxiv.org/abs/2510.20780)
*Runzhe Zhan,Zhihong Huang,Xinyi Yang,Lidia S. Chao,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文首次系统性分析LRMs在机器翻译评估中作为评审者的性能。针对其存在的过度思考与评分高估等问题，提出通过模拟真实思考轨迹进行校准。实验证明该方法可显著提升评测效率和性能，具备推动自动MT评测发展的潜力。


<details>
  <summary>Details</summary>
Motivation: 近年来大型推理模型（LRMs）在推理任务上展现了强大能力，但其作为机器翻译（MT）质量评估工具仍缺乏系统性研究。本文旨在填补该领域空白，探索LRMs作为MT自动评估者的潜力以及面对的主要挑战。

Method: 作者系统性地分析了LRMs用于MT评估的适用性，发现其面临定制化评测材料、简单实例易被“过度思考”、评分机制导致过高估计等问题。为此，作者提出通过训练LRMs学习合成人类思考轨迹的方式进行校准，以优化其思考过程。

Result: 实验基于WMT24 Metrics基准，校准后能显著减少35倍的思考预算，同时提升不同规模LRMs（如Qwen-7B模型）的评测性能，最大可带来+8.7点的相关性提升。

Conclusion: 经过合理校准的LRMs在自动机器翻译质量评估中效果优异，有望推动更细粒度和高效的自动评测系统发展。

Abstract: Recent advancements in large reasoning models (LRMs) have introduced an
intermediate "thinking" process prior to generating final answers, improving
their reasoning capabilities on complex downstream tasks. However, the
potential of LRMs as evaluators for machine translation (MT) quality remains
underexplored. We provides the first systematic analysis of LRM-as-a-judge in
MT evaluation. We identify key challenges, revealing LRMs require tailored
evaluation materials, tend to "overthink" simpler instances and have issues
with scoring mechanisms leading to overestimation. To address these, we propose
to calibrate LRM thinking by training them on synthetic, human-like thinking
trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this
approach largely reduces thinking budgets by ~35x while concurrently improving
evaluation performance across different LRM scales from 7B to 32B (e.g.,
R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These
findings highlight the potential of efficiently calibrated LRMs to advance
fine-grained automatic MT evaluation.

</details>


### [84] [A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text](https://arxiv.org/abs/2510.20782)
*Alicia Sagae,Chia-Jung Lee,Sandeep Avula,Brandon Dang,Vanessa Murdock*

Main category: cs.CL

TL;DR: 文章针对责任AI公平性问题，结合实际产品描述生成任务，构建了融入公平属性的数据集与评估方法，可用于发现LLM在多维度责任AI表现上的不足，推动公平性研究。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型的方法通常侧重于文本生成等高层任务，无法针对特定AI应用进行公平性等责任维度的有效评估，不同应用中保护属性相关性差异较大。

Method: 构建了一个针对真实世界应用的带有公平属性的数据集，以产品描述生成任务为例，将公平属性（如性别化形容词和产品类别）参数化，产生丰富的有标签提示，利用这些数据评估LLM在质量、真实性、安全性和公平性方面的差距。

Result: 通过该数据集和评估方法，能够识别LLM在多个责任AI维度上的表现缺口，为公平性等责任性评测提供具体资源和方法。

Conclusion: 提出了一种结合真实应用场景的LLM评测新方案及可用资源，有助于推动责任AI研究社区对公平性等问题的深入探索。

Abstract: Current methods for evaluating large language models (LLMs) typically focus
on high-level tasks such as text generation, without targeting a particular AI
application. This approach is not sufficient for evaluating LLMs for
Responsible AI dimensions like fairness, since protected attributes that are
highly relevant in one application may be less relevant in another. In this
work, we construct a dataset that is driven by a real-world application
(generate a plain-text product description, given a list of product features),
parameterized by fairness attributes intersected with gendered adjectives and
product categories, yielding a rich set of labeled prompts. We show how to use
the data to identify quality, veracity, safety, and fairness gaps in LLMs,
contributing a proposal for LLM evaluation paired with a concrete resource for
the research community.

</details>


### [85] [Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction](https://arxiv.org/abs/2510.20787)
*Mutian He,Philip N. Garner*

Main category: cs.CL

TL;DR: 本文针对线性注意力模型在检索任务上的遗忘问题，引入混合稀疏注意力及可学习令牌逐出机制，显著提升模型效果并保持高效。


<details>
  <summary>Details</summary>
Motivation: 线性注意力模型虽然在效率上优于Transformer，但由于其有限的记忆能力，容易遗忘，导致检索密集型任务效果不佳。

Method: 探索了一系列混合模型，将令牌混合器与介于线性与全注意力之间的稀疏注意力机制结合，并提出一种新颖的可学习令牌逐出方法。结合滑动窗口注意力，通过轻量级CNN自适应保留每个头部的关键值对，保持线性注意力的常数时间和空间复杂度。同时提供稀疏注意力机制的高效Triton内核实现。

Result: 混合模型能够自适应地保留关键信息，提升线性注意力在检索密集型任务中的表现，且维护了高效的计算复杂度。

Conclusion: 提出的混合与逐出机制在检索密集任务上提升了线性注意力模型的效果，并兼顾了效率与性能。

Abstract: Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention's
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.

</details>


### [86] [Simple Context Compression: Mean-Pooling and Multi-Ratio Training](https://arxiv.org/abs/2510.20797)
*Yair Feldman,Yoav Artzi*

Main category: cs.CL

TL;DR: 作者提出了一种基于均值池化的软上下文压缩方法，广泛实验显示其表现优于主流压缩令牌方法，且支持多压缩比。论文揭示了压缩方法的复杂权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在大语言模型处理长上下文时计算开销高，需要更高效的上下文压缩方法。作者旨在探索更简单高效的上下文压缩策略。

Method: 提出并研究了一种轻量级的均值池化软上下文压缩方法，并通过训练同一压缩器输出不同压缩比来考察其效果。

Result: 均值池化方法在多种问答任务、模型家族、规模和压缩比下表现最优，且多压缩比训练带来的性能损失较小。也显示了压缩方法上存在更细致的效果差异。

Conclusion: 文中提出的简单均值池化方法在大多数情况下优于流行的压缩令牌架构，在多种压缩比训练下性能下降也很有限。总体上，不同压缩方法的效果存在复杂的权衡。

Abstract: A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.

</details>


### [87] [On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?](https://arxiv.org/abs/2510.20810)
*Mingmeng Geng,Thierry Poibeau*

Main category: cs.CL

TL;DR: 由于大语言模型生成文本的界限模糊，且既有检测方法和基准不足以应对复杂多变的实际应用场景，因此检测结果的参考价值有限，需谨慎解读。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛应用，检测由其生成的文本变得重要。然而，目前没有一致或精确地界定“LLM生成文本”的标准，且不同场景和模型的多样性使检测难度加大。

Method: 分析现有检测目标的定义，探讨实际应用场景中人类与LLM互动对检测效果的影响，并评估现有基准和评估方法的局限性。

Result: 现有检测与评估方法未能覆盖真实应用中的多样化情况，导致检测结果常被误解，并且实际意义逐渐减弱。

Conclusion: 检测器在特定条件下仍有用处，但其结果只能作为参考，不能作为决定性指标。

Abstract: With the widespread use of large language models (LLMs), many researchers
have turned their attention to detecting text generated by them. However, there
is no consistent or precise definition of their target, namely "LLM-generated
text". Differences in usage scenarios and the diversity of LLMs further
increase the difficulty of detection. What is commonly regarded as the
detecting target usually represents only a subset of the text that LLMs can
potentially produce. Human edits to LLM outputs, together with the subtle
influences that LLMs exert on their users, are blurring the line between
LLM-generated and human-written text. Existing benchmarks and evaluation
approaches do not adequately address the various conditions in real-world
detector applications. Hence, the numerical results of detectors are often
misunderstood, and their significance is diminishing. Therefore, detectors
remain useful under specific conditions, but their results should be
interpreted only as references rather than decisive indicators.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [88] [Labeling and folding multi-labeled trees](https://arxiv.org/abs/2510.20292)
*Vincent Moulton,Andreas Spillner*

Main category: cs.DM

TL;DR: 本文推广了Erdős和Székely的树-划分双射到多重标签树，提出标记算法，对应到多重集划分，并应用于系统发生网络的结构刻画。


<details>
  <summary>Details</summary>
Motivation: Erdős和Székely提出了一种将加有标签的树与集合划分对应的方法，但该方法只适用于叶子标签唯一的情况。现实中，许多应用（如系统发生网络）涉及叶子标签可重复的树（multi-labeled trees）。因此，需要推广这一方法以处理更一般的多重标签情形。

Method: 本文发展了一种多重标签树的标记算法，通过对正整数有限非空多重集的特定排序，推广了原先的反词典序法，将树的标记推广到所有节点，并通过多重集划分来描述这种树的结构。

Result: 证明了这一推广可以用于刻画从多重标签树的标记中产生的多重集划分。作为应用，发现可标记的系统发生网络正好对应于在多重标签树上的折叠过程下稳定的网络。同时，给出了叶子集为[n]的可标记系统发生网络与特定多重集划分间的双射。

Conclusion: 推广了树与集合划分间经典的对应关系到多重标签情况，丰富了组合结构与系统发生网络间的联系，为后续网络相关算法和理论研究提供了新工具。

Abstract: In 1989 Erd\H{o}s and Sz\'ekely showed that there is a bijection between (i)
the set of rooted trees with $n+1$ vertices whose leaves are bijectively
labeled with the elements of $[\ell]=\{1,2,\dots,\ell\}$ for some $\ell \leq
n$, and (ii) the set of partitions of $[n]=\{1,2,\dots,n\}$. They established
this via a labeling algorithm based on the anti-lexicographic ordering of
non-empty subsets of $[n]$ which extends the labeling of the leaves of a given
tree to a labeling of all of the vertices of that tree. In this paper, we
generalize their approach by developing a labeling algorithm for multi-labeled
trees, that is, rooted trees whose leaves are labeled by positive integers but
in which distinct leaves may have the same label. In particular, we show that
certain orderings of the set of all finite, non-empty multisets of positive
integers can be used to characterize partitions of a multiset that arise from
labelings of multi-labeled trees. As an application, we show that the recently
introduced class of labelable phylogenetic networks is precisely the class of
phylogenetic networks that are stable relative to the so-called folding process
on multi-labeled trees. We also give a bijection between the labelable
phylogenetic networks with leaf-set $[n]$ and certain partitions of multisets.

</details>


### [89] [Excluding a Line Minor via Design Matrices and Column Number Bounds for the Circuit Imbalance Measure](https://arxiv.org/abs/2510.20301)
*Daniel Dadush,Friedrich Eisenbrand,Rom Pinchasi,Thomas Rothvoss,Neta Singer*

Main category: cs.DM

TL;DR: 首次给出针对实矩阵电路失衡度量（κ）的极小生成集多项式上界，推广了整数矩阵的相关结论，丰富了一般系数矩阵的拟阵理论结构和实际应用。


<details>
  <summary>Details</summary>
Motivation: 本文旨在推广对整数矩阵极小生成集规模的上界研究，从整数矩阵的Δ-模度推广到实矩阵的电路失衡度量（circuit imbalance measure，记为κ），以期获得对所有参数范围及实矩阵有效的多项式上界。该度量对线性规划等领域有广泛应用。

Method: 作者借鉴了Δ-模度矩阵相关工作的方法，分析了由κ有界矩阵诱导的可表示拟阵，并证明这类拟阵具有minor closed性质，同时排除了具有长度O(κ)的rank 2齐次拟阵（即长度O(κ)的直线）。技术核心为：证明当排除长度为l的直线时，任意单纯rank d的复数可表示拟阵最多包含O(d^4 l)个元素。

Result: 证明了对于一组非共线的实矩阵列A∈R^(d×n)，其列数n至多为O(d^4 κ_A)，κ_A为A的电路失衡度量。该结果推广了对整数矩阵的O(d^4 Δ_A)界，首次给出了对实矩阵全参数范围的多项式上界。此外，得到了更一般的，排除长度为l的直线的单纯rank d复可表示拟阵大小的上界。

Conclusion: 本工作首次将极小生成集规模的多项式界从整数矩阵成功推广到实矩阵的电路失衡度量，理论上丰富了矩阵及拟阵理论在更广泛实系数场景下的结构结果，为线性规划等领域提供了更广普适的理论工具。

Abstract: For a real matrix $A \in \mathbb{R}^{d \times n}$ with non-collinear columns,
we show that $n \leq O(d^4 \kappa_A)$ where $\kappa_A$ is the \emph{circuit
imbalance measure} of $A$. The circuit imbalance measure $\kappa$ is a real
analogue of $\Delta$-modularity for integer matrices, satisfying $\kappa_A \leq
\Delta_A$ for integer $A$. The circuit imbalance measure has numerous
applications in the context of linear programming (see Ekbatani, Natura and
V{\'e}gh (2022) for a survey). Our result generalizes the $O(d^4 \Delta_A)$
bound of Averkov and Schymura (2023) for integer matrices and provides the
first polynomial bound holding for all parameter ranges on real matrices.
  To derive our result, similar to the strategy of Geelen, Nelson and Walsh
(2021) for $\Delta$-modular matrices, we show that real representable matroids
induced by $\kappa$-bounded matrices are minor closed and exclude a rank $2$
uniform matroid on $O(\kappa)$ elements as a minor (also known as a line of
length $O(\kappa)$).
  As our main technical contribution, we show that any simple rank $d$ complex
representable matroid which excludes a line of length $l$ has at most $O(d^4
l)$ elements. This complements the tight bound of $(l-3)\binom{d}{2} + d$ for
$l \geq 4$, of Geelen, Nelson and Walsh which holds when the rank $d$ is
sufficiently large compared to $l$ (at least doubly exponential in $l$).

</details>


### [90] [Partial Optimality in Cubic Correlation Clustering for General Graphs](https://arxiv.org/abs/2510.20431)
*David Stein,Bjoern Andres,Silvia Di Gregorio*

Main category: cs.DM

TL;DR: 本文针对NP难的高阶相关聚类（3-团），提出和实现了部分最优性条件的判定算法，通过实验证明其在实际数据中具有实际价值。


<details>
  <summary>Details</summary>
Motivation: 高阶相关聚类问题涉及最小化图中属于同一聚类的团（clique）的总成本，这类问题在实际应用中常常碰到，但由于其NP-hard的复杂性，难以获得最优解。实际应用多用局部搜索启发式方法。为改善这一状况，需要探索新的理论或算法策略。

Method: 本文针对最多3节点团（cubic correlation clustering），提出并建立了部分最优性条件，并设计与实现了用于判断这些条件的算法，通过两组数据验证了算法有效性。

Result: 数值实验表明，所定义的部分最优性条件及其判断算法在数据集上具有一定有效性，能够辅助实际求解高阶相关聚类问题。

Conclusion: 通过提出部分最优性条件与相关算法，为解决cubic correlation clustering问题提供了更好的理论基础和实用工具，对相关实际聚类任务有积极推动作用。

Abstract: The higher-order correlation clustering problem for a graph $G$ and costs
associated with cliques of $G$ consists in finding a clustering of $G$ so as to
minimize the sum of the costs of those cliques whose nodes all belong to the
same cluster. To tackle this NP-hard problem in practice, local search
heuristics have been proposed and studied in the context of applications. Here,
we establish partial optimality conditions for cubic correlation clustering,
i.e., for the special case of at most 3-cliques. We define and implement
algorithms for deciding these conditions and examine their effectiveness
numerically, on two data sets.

</details>


### [91] [A Classification of Long-Refinement Graphs for Colour Refinement](https://arxiv.org/abs/2510.20802)
*Sandra Kiefer,T. Devini de Mel*

Main category: cs.DM

TL;DR: 该文全面刻画了在颜色细化算法下迭代次数达到理论上限的低（或高）度极端图，并证实除特例外，已知家族涵盖全部极端情况，解决了基础结构和复杂度瓶颈的疑问。


<details>
  <summary>Details</summary>
Motivation: 颜色细化（Colour Refinement）算法一直是图同构检测中的核心工具，但其最大理论迭代次数可达上限的图是否存在长期未决。基于前人工作，研究者已经找到最大度为2和3的无限家族极端案例。而完整的低/高度极端图结构分类尚未解决。

Method: 作者使用逆向工程的方法，分析和刻画所有最大度不超过3、4的极端颜色细化图结构，并通过字符串紧凑描述给出结构归纳，同时利用边补操作推广分类到高最大度情形。

Result: （1）在最大度不超过3的情况下，除一例外，前人已知家族涵盖所有极端案；（2）对最大度为4的极端图给出了全面分类；（3）发现所有低度极端图均可用紧凑字符串描述，获得多项结构性质；（4）证明不可能存在仅在最后一次迭代才区分的极端图。

Conclusion: 本文对低（及高）度的颜色细化极端图进行完全分类，显著加深了对算法复杂度边界和结构本质的理解；特别地，解答了关于最后一步区分图的悬而未决问题。

Abstract: The Colour Refinement algorithm is a classical procedure to detect symmetries
in graphs, whose most prominent application is in graph-isomorphism tests. The
algorithm and its generalisation, the Weisfeiler-Leman algorithm, evaluate
local information to compute a colouring for the vertices in an iterative
fashion. Different final colours of two vertices certify that no isomorphism
can map one onto the other. The number of iterations that the algorithm takes
to terminate is its central complexity parameter. For a long time, it was open
whether graphs that take the maximum theoretically possible number of Colour
Refinement iterations actually exist. Starting from an exhaustive search on
graphs of low degrees, Kiefer and McKay proved the existence of infinite
families of such long-refinement graphs with degrees 2 and 3, thereby showing
that the trivial upper bound on the iteration number of Colour Refinement is
tight. In this work, we provide a complete characterisation of the
long-refinement graphs with low (or, equivalently, high) degrees. We show that,
with one exception, the aforementioned families are the only long-refinement
graphs with maximum degree at most 3, and we fully classify the long-refinement
graphs with maximum degree 4. To this end, via a reverse-engineering approach,
we show that all low-degree long-refinement graphs can be represented as
compact strings, and we derive multiple structural insights from this
surprising fact. Since long-refinement graphs are closed under taking edge
complements, this also yields a classification of long-refinement graphs with
high degrees. Kiefer and McKay initiated a search for long-refinement graphs
that are only distinguished in the last iteration of Colour Refinement before
termination. We conclude it in this submission by showing that such graphs
cannot exist.

</details>
