<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.CL](#cs.CL) [Total: 34]
- [cs.DM](#cs.DM) [Total: 3]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Mechanizing a Proof-Relevant Logical Relation for Timed Message-Passing Protocols](https://arxiv.org/abs/2511.19521)
*Tesla Zhang,Asher Kornfeld,Rui Li,Sonya Simkin,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 本文实现了面向时序消息协议验证的语义类型系统的机械化工具，解决了理论与实际脱节的问题，为相关领域验证工具的推广和应用打下了基础。


<details>
  <summary>Details</summary>
Motivation: Yao等人的语义类型方法在验证面向时间的消息传递协议（如物联网和实时系统）中展现出优势，尤其适用于支持异构的程序组件（包括物理对象）。但现有的形式化工具缺乏机械化，影响了其实际推广和扩展。本文旨在填补这一空白。

Method: 本文采用在Rocq定理证明器中机械化Yao等人的与时序相关的逻辑关系，并构建了可计算轨迹的代数，支持轨迹的交错、分割和拼接，证明了相关支持引理和逻辑关系的基本定理。该工作克服了依赖于内涵型类型理论证明助手在处理轨迹等式方面的局限。

Result: 成功地在 Rocq 定理证明器中机械化了以时序为中心的逻辑关系和相关理论，完善了对轨迹等式与相关操作的形式支持，使过去的理论工具具备了机器可验证、可扩展的基础。

Conclusion: 本文在理论和实践上促进了基于语义类型的、面向时序协议的程序验证工具的机械化建设，既证明了可行性，也为日后系统扩展及实际工业应用提供了坚实基础。

Abstract: Semantic typing has become a powerful tool for program verification, applying the technique of logical relations as not only a proof method, but also a device for prescribing program behavior. In recent work, Yao et al. scaled semantic typing to the verification of timed message-passing protocols, which are prevalent in, e.g., IoT and real-time systems applications. The appeal of semantic typing in this context is precisely because of its ability to support typed and untyped program components alike -- including physical objects -- which caters to the heterogeneity of these applications. Another demand inherent to these applications is timing: constraining the time or time window within which a message exchange must happen. Yao et al. equipped their logical relation not only with temporal predicates, but also with computable trajectories, to supply the evidence that an inhabitant can step from one time point to another one. While Yao et al. provide the formalization for such a verification tool, it lacks a mechanization. Mechanizing the system would not only provide a machine proof for it, but also facilitate scalability for future extensions and applications.
  This paper tackles the challenge of mechanizing the resulting proof-relevant logical relation in a proof assistant. allowing trajectories to be interleaved, partitioned, and concatenated, while the intended equality on trajectories is the equality of their graphs when seen as processes indexed by time. Unfortunately, proof assistants based on intensional type theory only have modest support for such equations, forcing a prolific use of transports. This paper reports on the process of mechanizing Yao et al.'s results, comprising the logical relation, the algebra of computable trajectories with supporting lemmas, and the fundamental theorem of the logical relation, in the Rocq theorem prover.

</details>


### [2] [Understanding Accelerator Compilers via Performance Profiling](https://arxiv.org/abs/2511.19764)
*Ayaka Yorihiro,Griffin Berlstein,Pedro Pontes García,Kevin Laeufer,Adrian Sampson*

Main category: cs.PL

TL;DR: 本论文针对加速器设计语言编译器性能不可预测的问题，提出了名为Petal的周期级分析工具，能帮助开发者定位并优化硬件设计中的性能瓶颈。经过实证，工具能显著提升硬件性能，部分应用总运行周期减少近一半。


<details>
  <summary>Details</summary>
Motivation: 现有的加速器设计语言（ADL）编译器在将高层语义转化为硬件周期级执行计划时存在不可预测性，不易控制性能且难以定位问题。作者认为这类编译器无法做到完全理想，性能不可预测性是内在难题。

Method: 提出了Petal工具，对Calyx中间语言进行周期级性能分析，通过在代码插入探针并分析寄存器传输级模拟的轨迹，将低层事件映射回上层控制结构，从而追踪每个构造在时钟周期的活跃情况。并以案例研究展示其效果。

Result: 利用Petal分析工具，可以识别实际加速器设计中的性能瓶颈，帮助开发者发现编译器未能自动优化的问题，并指导优化。实际案例中对某应用总周期数减少了46.9%。

Conclusion: ADL编译器的性能不可预测性难以根本解决，但通过理解编译器决策影响性能的分析工具，如Petal，可以帮助开发者更好地优化硬件设计。

Abstract: Accelerator design languages (ADLs), high-level languages that compile to hardware units, help domain experts quickly design efficient application-specific hardware. ADL compilers optimize datapaths and convert software-like control flow constructs into control paths. Such compilers are necessarily complex and often unpredictable: they must bridge the wide semantic gap between high-level semantics and cycle-level schedules, and they typically rely on advanced heuristics to optimize circuits. The resulting performance can be difficult to control, requiring guesswork to find and resolve performance problems in the generated hardware. We conjecture that ADL compilers will never be perfect: some performance unpredictability is endemic to the problem they solve.
  In lieu of compiler perfection, we argue for compiler understanding tools that give ADL programmers insight into how the compiler's decisions affect performance. We introduce Petal, a cycle-level Petal for the Calyx intermediate language (IL). Petal instruments the Calyx code with probes and then analyzes the trace from a register-transfer-level simulation. It maps the events in the trace back to high-level control constructs in the Calyx code to track the clock cycles when each construct was active. Using case studies, we demonstrate that Petal's cycle-level profiles can identify performance problems in existing accelerator designs. We show that these insights can also guide developers toward optimizations that the compiler was unable to perform automatically, including a reduction by 46.9\% of total cycles for one application.

</details>


### [3] [The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)](https://arxiv.org/abs/2511.20369)
*Frank Schüssele,Matthias Zumkeller,Miriam Lagunes-Rochin,Dominik Klumpp*

Main category: cs.PL

TL;DR: 该论文提出了一种自动将并发程序的交错式正确性证明转化为可独立验证且紧凑的Owicki-Gries线程模块化证明的方法，自动生成所需的ghost变量，实验结果表明该方法高效且生成证明文件更小。


<details>
  <summary>Details</summary>
Motivation: 当前算法化软件验证器存在实现上的漏洞，影响其在验证过程中的可靠性。为了独立有效地验证这些工具的结果，研究者希望为正确程序自动生成紧凑、易检查的正确性证明。特别是在并发程序领域，由于执行顺序的复杂，正确性证明难以压缩和自动化。

Method: 文中提出了一种方法，将基于交错(interleaving)的正确性证明（目前许多软件验证器生成这类证明），自动转化为Owicki-Gries风格的线程模块化(thread-modular)正确性证明。该方法会自动合成ghost变量用于记录相关的交错信息，并抽象化无关细节。

Result: 方法得到了实验验证：与基线方法相比，本方法生成的正确性证明更加紧凑，同时实际效率也较高。

Conclusion: 文中提出的方法能高效自动地为并发程序生成紧凑的正确性证明，有助于独立验证软件验证器的正确性，从而保障算法化软件验证工具的可靠性。

Abstract: Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Building Browser Agents: Architecture, Security, and Practical Solutions](https://arxiv.org/abs/2511.19477)
*Aram Vardanyan*

Main category: cs.SE

TL;DR: 本文指出当前浏览器代理最大短板是架构和安全机制，而非模型本身。通过架构创新和安全约束，代理在基准测试中表现出色，作者建议放弃追求通用智能，转向专业受控代理以提升安全和可靠性。


<details>
  <summary>Details</summary>
Motivation: 浏览器代理虽然能够实现自主网页交互，但在实际应用中面临严峻的可靠性与安全性挑战。本研究旨在分析这些挑战并提出改进方案，强调安全运行机制的重要性。

Method: 研究团队搭建并运营了一个生产级浏览器代理，通过实际案例分析了现有方法失败的原因及安全隐患，特别是针对提示注入（prompt injection）攻击，进行了安全事件分析。同时，采用了混合式上下文管理、全面的浏览器工具链以及智能提示工程等技术，优化代理架构。

Result: 所提出的代理通过混合上下文管理和专用工具，成功率达到WebGames基准测试约85%，相比此前50%的普通代理有大幅提升，虽略低于95.7%的人类表现。

Conclusion: 代理的性能瓶颈不在于模型能力，而在于架构设计。泛化的自主浏览智能存在固有安全风险，建议开发具有限制性的专用工具，通过代码层面而非大模型推理来确保安全边界。

Abstract: Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings from building and operating a production browser agent. The analysis examines where current approaches fail and what prevents safe autonomous operation. The fundamental insight: model capability does not limit agent performance; architectural decisions determine success or failure. Security analysis of real-world incidents reveals prompt injection attacks make general-purpose autonomous operation fundamentally unsafe. The paper argues against developing general browsing intelligence in favor of specialized tools with programmatic constraints, where safety boundaries are enforced through code instead of large language model (LLM) reasoning. Through hybrid context management combining accessibility tree snapshots with selective vision, comprehensive browser tooling matching human interaction capabilities, and intelligent prompt engineering, the agent achieved approximately 85% success rate on the WebGames benchmark across 53 diverse challenges (compared to approximately 50% reported for prior browser agents and 95.7% human baseline).

</details>


### [5] [Translating Large-Scale C Repositories to Idiomatic Rust](https://arxiv.org/abs/2511.20617)
*Saman Dehghan,Tianran Sun,Tianxiang Wu,Zihan Li,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出了一种名为Rustine的新工具，高效实现了C到安全、规范的Rust代码自动迁移，综合在代码安全性、可读性和可扩展性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的C转Rust的翻译技术在代码质量和可扩展性之间难以兼顾。基于转译的办法适用于大型项目，但代码安全性、符合Rust习惯和可读性差；而基于大模型（LLM）的方法则计算资源消耗大，难以广泛应用。

Method: 提出Rustine，一个全自动化流水线，实现高效、有效的C到符合Rust习惯且安全的Rust代码仓库级翻译，并在23个不同C程序上进行评测。

Result: Rustine对所有测试项目都能生成可编译的Rust代码，并在1,221,192个断言测试中通过了1,063,099个（87%功能等价），平均函数和行覆盖率分别为74.7%与72.2%。与六种已有技术对比，Rustine生成的代码更安全、更符合Rust规范、更易读。对于未通过所有测试的情况，开发者平均仅需4.5小时通过Rustine辅助就能完成调试。

Conclusion: Rustine弥补了现有C到Rust翻译技术在质量和可扩展性之间的不足，实现了高质量、可扩展的自动化代码迁移。

Abstract: Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.

</details>


### [6] [Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation](https://arxiv.org/abs/2511.19483)
*Qingsong He,Jing Nan,Jiayu Jiao,Liangjie Tang,Xiaodong Xu,Mengmeng Sun,Qingyao Wang,Minghui Yan*

Main category: cs.SE

TL;DR: Z-Space框架通过多智能体协作和精细语义匹配，实现了企业级复杂任务的高效自动化工具调用，显著减少资源消耗并提升准确率，已在多个真实业务场景得到验证。


<details>
  <summary>Details</summary>
Motivation: 随着企业级MCP服务规模快速扩展，如何在众多异构工具中高效、准确地匹配目标功能成为制约系统实际应用的关键难题。现有方法存在语义脱节、上下文输入膨胀及推理延迟高等问题。

Method: 提出数据生成导向的多智能体协作工具调用框架Z-Space，包括意图解析模型、融合子空间加权算法(FSWW)的工具过滤模块，以及动态规划和容错执行的推理代理。系统已在饿了么平台投入使用。

Result: 系统在生产环境中将工具推理的平均token消耗降低了96.26%，工具调用准确率达到92%，大幅提升了智能测试数据生成的效率和可靠性。

Conclusion: Z-Space框架能够高效、可靠地实现大规模工具调用，显著提升智能测试数据生成系统的效率和准确性。

Abstract: Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\% while achieving a 92\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.

</details>


### [7] [stable-pretraining-v1: Foundation Model Research Made Simple](https://arxiv.org/abs/2511.19484)
*Randall Balestriero,Hugues Van Assel,Sami BuGhanem,Lucas Maes*

Main category: cs.SE

TL;DR: 本文提出了stable-pretraining库，在保有高扩展性和性能优化的同时，整合了自监督学习相关核心功能，显著简化了基础模型与SSL研究的技术门槛和工程负担。


<details>
  <summary>Details</summary>
Motivation: 基金会模型和自监督学习（SSL）是现代人工智能的核心，但相关研究受限于繁琐的代码库、重复实现和实验扩展的工程负担。

Method: 提出stable-pretraining，是一个兼容PyTorch、Lightning、Hugging Face和TorchMetrics的模块化、可扩展、高性能的库。它整合了SSL常用工具（如探针、坍塌检测、数据增强和可扩展评测）于统一框架，并注重详细日志记录以提升可调试性和可重复性。

Result: stable-pretraining库可以以极小的工程开销，支持新颖的研究探索，如深度表征探测和CLIP在合成数据微调时的退化分析，且可扩展至大规模实验。

Conclusion: stable-pretraining降低了基础模型和SSL研究的门槛，提升实验迭代速度和可扩展性，有望加快领域发展和创新。

Abstract: Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.

</details>


### [8] [Evolution without an Oracle: Driving Effective Evolution with LLM Judges](https://arxiv.org/abs/2511.19489)
*Zhe Zhao,Yuheng Yang,Haibin Wen,Xiaojie Qiu,Zaixi Zhang,Qingfu Zhang*

Main category: cs.SE

TL;DR: 论文突破进化计算对客观适应度的依赖，引入MADE框架，利用LLM主观评分+问题分解，实现高效优化，并在多项复杂任务上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前进化计算依赖客观的可计算适应度函数（Oracle），难以在主观、无明确目标的开放域问题中应用，阻碍了科学发现的可能性。

Method: 提出了MADE（多智能体分解进化）框架，通过“问题规格化”把模糊指令分解为可验证的子需求，将高方差的LLM反馈转化为稳定、精确的选择压力。

Result: 在DevAI和InfoBench等复杂基准测试上，MADE框架在软件需求满足率上较强基线提升超过50%（39.9%至61.9%），在复杂指令跟随任务上达到了95%的完美通过率。

Conclusion: 本研究提出了一种全新进化优化范式，可以在没有客观可计算适应度函数的场景下，仅依赖大型语言模型（LLM）的主观判断，实现高效进化优化。

Abstract: The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through "Problem Specification." By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing "computable metrics" to "describable qualities," thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.

</details>


### [9] [CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem](https://arxiv.org/abs/2511.19510)
*Asif Zaman,Kallol Naha,Khalid Belhajjame,Hasan M. Jamil*

Main category: cs.SE

TL;DR: 利用AI自动修复和迁移老旧科学工作流到现代平台，并通过众包协作平台实现人机结合的高效复用。自动化显著减轻了人工负担，但关键步骤还需专家参与。


<details>
  <summary>Details</summary>
Motivation: 现有大量科学工作流因依赖过时服务和系统退休而失效，尤其是在Taverna等遗留系统中，导致已发布的工作流无法重复利用。该工作旨在解决遗留工作流失效和复用难的问题。

Method: 提出了一种名为CodeR^3（Code Repair, Revival and Reuse）的遗留科学工作流迁移系统，采用生成式AI对失效的旧工作流进行分析，自动将其迁移到现代的工作流平台（如Snakemake和VisFlow），并结合了工作流可视化分析、自动服务替换及人工参与验证。

Result: 在多个Taverna用例中，系统能够自动分析和转换工作流，大幅减少了人工解析和服务识别的工作量。但服务替换和数据验证等环节仍需领域专家参与，表明部分关键步骤尚无法完全自动化。最终将形成一个众包平台，为社区提供协同修复和验证工作流的功能。

Conclusion: 本文提出了一个平衡自动化与人工判断的遗留工作流复兴框架，展示了技术可行性和现实挑战。自动化在处理流程解析和识别上表现突出，部分流程仍需专家决策。未来，该系统将通过社区众包进一步提升工作流的再利用和验证能力。

Abstract: Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.

</details>


### [10] [Agint: Agentic Graph Compilation for Software Engineering Agents](https://arxiv.org/abs/2511.19635)
*Abhi Chivukula,Jay Somasundaram,Vijay Somasundaram*

Main category: cs.SE

TL;DR: 本文提出了Agint，一个可扩展、高可靠性、支持自然语言到代码DAG转换的代理系统，以高效团队协作和低延迟方法推动LLM编码代理实现原型到生产的全流程优化。


<details>
  <summary>Details</summary>
Motivation: 当前以大语言模型（LLM）为基础的编码代理在上下文管理、延迟、可靠性、可复现性和可扩展性等方面面临挑战。

Method: 提出Agint系统，包括编译器、解释器及运行时，通过分层、有类型的语义图（DAG）将自然语言指令逐步转换为具备效果感知能力的代码。系统结合LLM和函数型JIT运行时，实现动态图优化和高可靠性。配套提供包括DAG编译器、混合JIT运行时、模式生成器和数据转换工具，还配有CLI和GUI用于人类开发者及非技术用户交互。

Result: Agint实现了高效的上下文利用、低延迟与高吞吐，支持并发代码复合与可扩展的图编辑，实现代码生成和数据流的实时优化与复现，提高开发效率和代码可靠性。CLI和GUI支持交互式可视化编辑与联合作业，促进原型到生产代码无缝过渡。

Conclusion: Agint架构融合自然语言、编译技术和开发者工具，为团队级编码代理创造了高效、可组合、支持持续共创的新范式，推动LLM编码代理在可靠性和可扩展性上的突破。

Abstract: LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.

</details>


### [11] [CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection](https://arxiv.org/abs/2511.19875)
*Qingyu Zhang,Puzhuo Liu,Peng Di,Chenxiong Qian*

Main category: cs.SE

TL;DR: 本文提出首个专用于代码提交信息与代码差异一致性检测（MCI）的基准CODEFUSE-COMMITEVAL，系统分析了6种大语言模型在7类不一致情景下的表现，总结了不同增强策略的利弊，并针对高层语义一致性检测难题提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 代码版本控制系统高度依赖提交信息来说明变更原因，但这些信息常常与代码差异（diff）不一致，导致误导和维护困难等问题。之前缺乏针对检测提交信息-代码不一致（MCI）问题的专用评测基准。

Method: 作者提出了CODEFUSE-COMMITEVAL，一个专为MCI检测设计的基准，基于高质量、多样化的ApacheCM数据集。通过规则引导的变异生成7类不一致信息，并进行双重验证构造正负样本。利用该数据集评测6个主流开源大语言模型，并设置原始、少样本提示、思维链扩展等增强方式。

Result: 实验发现，模型检测出不一致提交的可靠性高于一致提交（平均召回率85.95%，精准率80.28%，特异性63.8%）。gpt-oss-20B性能最佳但资源消耗大。邻近上下文增强有利于大模型，小模型则引入噪声；少样本提示提升准确度但错误预测增加；思维链增强精度、特异性但降低召回，且耗费资源较多。对不同不一致类型的分析显示，‘目的’类高层语义难以检测且消耗高。

Conclusion: CODEFUSE-COMMITEVAL为检测和评估MCI问题提供了标准，揭示了补充更丰富上下文和数据平衡对捕捉高语义差异的重要性。

Abstract: Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.

</details>


### [12] [LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework](https://arxiv.org/abs/2511.20403)
*Andrea Lops,Fedelucio Narducci,Azzurra Ragone,Michelantonio Trizio,Claudio Barto*

Main category: cs.SE

TL;DR: 该论文提出了AgoneTest框架，用于客观评估大模型生成的Java单元测试，发现只要编译通过，自动生成测试的效果不逊色于人工测试，并且更优提示方式能提升质量。


<details>
  <summary>Details</summary>
Motivation: 单元测试是保证代码正确性的重要环节，但其过程十分耗费资源。随着大语言模型（LLM）的发展，自动化单元测试生成成为研究热点。该论文关注于评估不同LLM生成的单元测试的效果，而非提出新的生成算法。

Method: 作者提出AgoneTest，一个面向Java的LLM自动化单元测试评估框架。该框架建立了Classes2Test数据集，将Java类与其测试类进行映射，并集成了如变异分数和测试异味等多种高级评估指标，从而系统性评估生成测试的质量。

Result: 实验结果表明，只要生成的测试能成功编译，LLM生成的测试在覆盖率和缺陷检测上能够媲美甚至超越人工编写的测试。此外，改进的提示工程策略能进一步提升测试质量。

Conclusion: AgoneTest框架明确展现了大语言模型在软件测试中的应用潜力，并为模型优化、提示工程及测试实践的未来改进提供了参考。

Abstract: Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [13] [Deductive Systems for Logic Programs with Counting](https://arxiv.org/abs/2511.19565)
*Jorge Fandinno,Vladimir Lifschitz*

Main category: cs.LO

TL;DR: 本论文扩展了针对答案集编程中强等价性证明的方法，使其能够处理包含计数聚合的规则组，从而提升了理论的适用范围和实用价值。


<details>
  <summary>Details</summary>
Motivation: 在答案集编程中，需要判断不同规则组在各种上下文中的等价性，即强等价性。随着编程引入计数聚合操作，现有的强等价证明方法面临新的挑战，因此需要扩展相关理论和工具。

Method: 本论文通过扩展现有的推理系统，使其能够处理包含计数聚合的程序，实现两组规则强等价性证明的方法延伸。

Result: 论文提出了适用于包含计数聚合的程序的强等价性证明方法，填补了现有推理系统在该领域的不足。

Conclusion: 扩展的推理系统和证明方法成功适用于包含计数聚合的答案集程序，为更复杂的规则等价性判定提供了理论支持。

Abstract: In answer set programming, two groups of rules are considered strongly equivalent if they have the same meaning in any context. Strong equivalence of two programs can be sometimes established by deriving rules of each program from rules of the other in an appropriate deductive system. This paper shows how to extend this method of proving strong equivalence to programs containing the counting aggregate.

</details>


### [14] [Chopping More Finely: Finite Countermodels in Modal Logic via the Subdivision Construction](https://arxiv.org/abs/2511.19747)
*Tenyo Takahashi*

Main category: cs.LO

TL;DR: 本文提出了Subdivision Construction方法，有效证明了某类模态逻辑的有限模型性质，并在K4扩展系统中发现了具有特殊Kripke不完全度的结构。


<details>
  <summary>Details</summary>
Motivation: 有限模型性质（fmp）对于模态逻辑研究至关重要，但对广泛模态逻辑及其规则系统证明fmp的方法有限，因此需要开发新的方法来扩展适用范围。

Method: 提出了一种新的Subdivision Construction细分构造法，该方法依托稳定规范规则，构建有限模态代数，从而生成有限的反模型以证明所涉及逻辑和规则系统的有限模型性质。

Result: 该方法成功证明了由有限高度有限模态代数的稳定规范公式和规则公理化的逻辑和规则系统的有限模型性质，并发现这些逻辑和系统在对应格中是union-splittings。此外，发现了在扩展K4模态逻辑系统（NExt(K4)）中Kripke不完全度为1的一类union-splittings。

Conclusion: Subdivision Construction方法有效扩展了能够证明有限模型性质的模态逻辑及规则系统的类别，并在模态逻辑结构上给出了新的分类与不完全性观察。

Abstract: We present a new method, the Subdivision Construction, for proving the finite model property (the fmp) for broad classes of modal logics and modal rule systems. The construction builds on the framework of stable canonical rules, and produces a finite modal algebra (finite modal space) that will be a finite countermodel of such rules, yielding the fmp. We apply the Subdivision Construction for proving the fmp for logics and rule systems axiomatized by stable canonical formulas and rules of finite modal algebras of finite height. We also observe that these logics and rule systems are union-splittings in corresponding lattices. As a consequence, we identify a class of union-splittings in $\mathsf{NExt}(\mathsf{K4})$ with the degree of Kripke incompleteness 1.

</details>


### [15] [Parameterized Verification of Quantum Circuits (Technical Report)](https://arxiv.org/abs/2511.19897)
*Parosh Aziz Abdulla,Yu-Fang Chen,Michal Hečko,Lukáš Holík,Ondřej Lengál,Jyun-Ao Lin,Ramanathan Srinivasan Thinniyam*

Main category: cs.LO

TL;DR: 提出了首个用于参数化量子程序关系性质全自动验证的框架，基于新颖的同步加权树自动机和变换器，理论和实践均表明该方法高效且表达力强，显著推动了量子程序形式化验证的发展。


<details>
  <summary>Details</summary>
Motivation: 目前还缺乏对参数化量子程序的关系性质（如输入输出正确性、等价性）进行全自动验证的方法，而这些验证对于量子算法的可靠性和正确性至关重要。

Method: 提出了一种新的自动机模型：同步加权树自动机（SWTA），能精确并紧凑地捕捉参数化量子程序生成的无限量子态族。同时，引入变换器刻画量子门语义，开发了组合算法来为参数化电路构造变换器。将验证问题归约为SWTA之间的函数包含性或等价性校验，并给出了判定过程。

Result: 该方法能够在毫秒到秒级时间内，高效且准确地验证多种代表性参数化量子程序，展现了方法的表达能力和实用效率。

Conclusion: 首次实现了对参数化量子程序的关系性质进行全自动验证的新框架，理论上和实践上都得到了有效验证，对量子程序验证领域具有重要意义。

Abstract: We present the first fully automatic framework for verifying relational properties of parameterized quantum programs, i.e., a program that, given an input size, generates a corresponding quantum circuit. We focus on verifying input-output correctness as well as equivalence. At the core of our approach is a new automata model, synchronized weighted tree automata (SWTAs), which compactly and precisely captures the infinite families of quantum states produced by parameterized programs. We introduce a class of transducers to model quantum gate semantics and develop composition algorithms for constructing transducers of parameterized circuits. Verification is reduced to functional inclusion or equivalence checking between SWTAs, for which we provide decision procedures. Our implementation demonstrates both the expressiveness and practical efficiency of the framework by verifying a diverse set of representative parameterized quantum programs with verification times ranging from milliseconds to seconds.

</details>


### [16] [Separating the Wheat from the Chaff: Understanding (In-)Completeness of Proof Mechanisms for Separation Logic with Inductive Definitions](https://arxiv.org/abs/2511.20193)
*Neta Elad,Adithya Murali,Sharon Shoham*

Main category: cs.LO

TL;DR: 本文分析了分离逻辑中自动证明失败的根源，通过将SLID扩展到WSL，理论上证明WSL在特定片段是完备的，并通过原型工具成功识别和分类了导致失败的“流氓模型”。


<details>
  <summary>Details</summary>
Motivation: 分离逻辑（Separation Logic）在推理堆操作程序和共享资源方面非常流行，但其扩展SLID在自动化证明过程中存在不完整性和失败现象，作者希望深入理解导致这些失败的根本原因。

Method: 作者将SLID置于更大的逻辑WSL（弱分离逻辑）框架中，通过与一阶逻辑（FOL）的归约，理论分析WSL与SLID的区别，并利用原型工具实现了WSL的FOL编码，对现有基准测试集进行了实验验证。

Result: 作者证明WSL在量化蕴涵的非平凡片段上是完备的，并且WSL的fold/unfold证明机制对于无理论、无量词的情况是完备和可靠的。实验表明，工具能够找到许多反例模型，并对“流氓模型”进行了分类分析。

Conclusion: 通过理论与实验分析，作者揭示了SLID不完整性的根本原因，提出了WSL和象征结构用于辨识和自动发现导致证明失败的“流氓模型”，从而为现实世界中的自动证明失败现象提供了理论解释和实际工具。

Abstract: For over two decades Separation Logic has been arguably the most popular framework for reasoning about heap-manipulating programs, as well as reasoning about shared resources and permissions. Separation Logic is often extended to include inductively-defined predicates, interpreted as least fixpoints, forming Separation Logic with Inductive Definitions (SLID). Many theoretical and practical advances have been made in developing automated proof mechanisms for SLID, but these mechanisms are imperfect, and a deeper understanding of their failures is desired. As expressive as Separation Logic is, it is not surprising that it is incomplete, and in fact, it contains several sources of incompleteness that defy automated reasoning.
  In this paper we study these sources of incompleteness and how they relate to failures of proof mechanisms. We place SLID within a larger logic, that we call Weak Separation Logic (WSL). We prove that unlike SLID, WSL is complete for a non-trivial fragment of quantified entailments with background theories and inductive definitions, via a reduction to first-order logic (FOL). Moreover, we show that the ubiquitous fold/unfold proof mechanism is sound and complete for theory-free, quantifier-free WSL entailments with inductive definitions. Through this, we understand proof failures as stemming from nonstandard models present in WSL, but not allowed in SLID. These rogue models are typically infinite, and we use the formalism of symbolic structures to represent and automatically find them.
  We present a prototype tool that implements the FOL encoding of WSL and test it on an existing benchmark, which contains over 700 quantified entailment problems with inductive definitions. Our tool is able to find counter-models to many of the examples, and we provide a partial taxonomy of the rogue models, shedding some light on real-world proof failures.

</details>


### [17] [Proceedings Twentieth Conference on Theoretical Aspects of Rationality and Knowledge](https://arxiv.org/abs/2511.20540)
*Adam Bjorndahl*

Main category: cs.LO

TL;DR: TARK是一项关注理性与知识理论跨学科研究的国际会议，已举办至第20届，汇聚全球相关领域学者，促进学术交流与研究进展。


<details>
  <summary>Details</summary>
Motivation: TARK会议旨在汇集来自计算机科学、人工智能、博弈论、决策理论、哲学、逻辑学、语言学与认知科学等多个领域的研究者，共同探讨理性与知识相关的跨学科议题。

Method: 会议通过征集并遴选相关领域论文、组织学术报告和交流讨论，实现学科间的深入合作和前沿研究展示。

Result: 本届（第20届）TARK会议已成功征集并录用了相关主题的论文，并将于2025年7月在德国杜塞尔多夫举办。论文集收录了所有被接受的研究论文。

Conclusion: TARK会议在推动关于理性与知识理论方面的多学科国际交流与合作中扮演着重要角色。

Abstract: The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.
  Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, uncertainty, awareness, bounded rationality, common sense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems.
  Information about TARK is available at http://www.tark.org/.
  These proceedings contain the papers that have been accepted for presentation at the Twentieth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2025), held July 14--16, 2025, at Heinrich-Heine-Universität, Düsseldorf, Germany. The conference website can be found at https://ccc.cs.uni-duesseldorf.de/tark-2025/.

</details>


### [18] [Verifying Numerical Methods with Isabelle/HOL](https://arxiv.org/abs/2511.20550)
*Dustin Bryant,Jonathan Julian Huerta y Munive,Simon Foster*

Main category: cs.LO

TL;DR: 本文提出了一个在Isabelle/HOL中实现的数值方法验证框架，能够从形式规范自动生成可验证的且可执行的数值算法。通过实际案例验证了框架的有效性，并扩展了相关数学库。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习和网络物理系统依赖于数值算法，因此，可靠的数值方法对于构建可信赖的系统非常重要。

Method: 提出了一套基于ITrees的、在Isabelle/HOL中实现的数值方法验证框架。该框架采用友好的规范语言，支持数值程序的直接声明，并可添加变体和不变性用于正确性证明。自动化证明工具与HOL-Analysis库联动，实现规范判据的自动验证。同时与Isabelle的代码生成器集成，实现从形式化规范到可执行代码的端到端流程。

Result: 成功将验证流程应用于两种著名数值方法：二分法和不动点迭代法，并扩展了形式化数学库，包括高阶导数和皮亚诺形式的泰勒定理。

Conclusion: 该框架可有效实现数值方法的机器检验证明到可执行代码的转换，对数值方法的验证具有较强实用性。

Abstract: Modern machine learning pipelines are built on numerical algorithms. Reliable numerical methods are thus a prerequisite for trustworthy machine learning and cyber-physical systems. Therefore, we contribute a framework for verified numerical methods in Isabelle/HOL based on ITrees. Our user-friendly specification language enables the direct declaration of numerical programs that can be annotated with variants and invariants for reasoning about correctness specifications. The generated verification conditions can be discharged via automated proof methods and lemmas from the HOL-Analysis library. The ITrees foundation interacts with Isabelle's code generator to export source code. This provides an end-to-end path from formal specifications with machine-checked guarantees to executable sources. We illustrate the process of modelling numerical methods and demonstrate the effectiveness of the verification by focusing on two well-known methods, the bisection method and the fixed-point iteration method. We also contribute crucial extensions to the libraries of formalised mathematics required for this objective: higher-order derivatives and Taylor's theorem in Peano form. Finally, we qualitatively evaluate the use of the framework for verifying numerical methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search](https://arxiv.org/abs/2511.19648)
*Manil Shrestha,Edward Kim*

Main category: cs.CL

TL;DR: 通过结合符号结构方法和深度学习，提出了高效且可验证的多跳推理算法，大幅提升速度并降低成本，实验证实小模型即可实现大模型性能。


<details>
  <summary>Details</summary>
Motivation: 多跳问题解答在知识图谱上的推理，因可能路径数量激增，导致计算难度大。目前依赖大模型进行实体对齐和路径排序，但成本高、效率低，且答案缺乏结构化知识的可验证性。

Method: 提出两种混合算法：(1) LLM引导规划，单次LLM预测关系序列后通过宽度优先搜索执行，所有答案均有知识图谱支撑；(2) 基于嵌入的神经搜索，融合文本和图嵌入，通过轻量级边评分器实现，比LLM方法快百倍接近精度。并通过知识蒸馏将规划能力压缩到4B参数模型，实现大模型水平，无API成本。

Result: 在MetaQA上验证，基于知识图谱的推理全面优于无支撑的生成类方法，结构化规划比直接生成更具迁移性。提出算法无需巨型模型即可实现高效可靠的多跳推理。

Conclusion: 利用结构化归纳偏差和表征学习，能以小模型实现可验证、高效的多跳知识推理，突破现有大模型推理的高成本和低可验证性限制。

Abstract: Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.

</details>


### [20] [Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian](https://arxiv.org/abs/2511.19719)
*Mobina Mehrazar,Mohammad Amin Yousefi,Parisa Abolfath Beygi,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本文针对波斯语情感分类，评估了大语言模型生成的自我解释与人类标注结果的差异，发现模型解释信度较低，方法与评价需进一步优化以适应多语言和低资源场景。


<details>
  <summary>Details</summary>
Motivation: 面对LLM逐步应用于生成预测解释的趋势，尤其在低资源语言环境下，对这些解释的可信度和可靠性提出怀疑，亟需系统性评估其与人类一致性。

Method: 通过情感分类任务，比较LLM识别的重要单词与人类标注者的结果，采用基于token级别log概率的置信度分数衡量解释的可信度，并测试两种不同的提示顺序（先预测后解释与先解释后预测）对信度的影响。

Result: LLM在情感分类表现强劲，但解释的信度较低，其解释之间的一致性高于与人类判断的一致性，指向解释方法与评估标准的不足。

Conclusion: 当前LLM生成的解释在信度上存在显著缺陷，尤其是在低资源语言环境中难以与人类判断保持高度一致。现有的解释方法和评价指标有待改进，以提升多语言及低资源语境下的模型可靠性。

Abstract: Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.

</details>


### [21] [Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation](https://arxiv.org/abs/2511.19739)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 相较于体积更大的解码器架构，编码器型Transformer（如BioLinkBERT）通过LoRA微调，在心脏病文本嵌入任务上性能更佳且资源消耗更低，有助于提升医疗信息学领域NLP系统的开发效率与效果。


<details>
  <summary>Details</summary>
Motivation: 在临床自然语言处理（NLP）领域，特定领域的文本嵌入至关重要，但对于不同模型架构的系统评测尚缺失。

Method: 对十种经LoRA微调的、适应心脏病学领域的Transformer文本嵌入模型进行评估，使用来源于权威医学教科书的106,535组心脏病学文本对。

Result: 编码器架构（尤其是BioLinkBERT）在领域特定任务上表现优越（分离分数0.510），且所需计算资源远低于更大的解码器模型。

Conclusion: 挑战了“大模型必优”的普遍假设，为临床NLP系统开发提供了实际指导，相关模型和数据已开源促进重复性研究。

Abstract: Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.

</details>


### [22] [What does it mean to understand language?](https://arxiv.org/abs/2511.19757)
*Colton Casto,Anna Ivanova,Evelina Fedorenko,Nancy Kanwisher*

Main category: cs.CL

TL;DR: 本文提出，深入的语言理解依赖核心语言系统与其他脑区的信息交互，支持感知、运动、知识和记忆等功能。认知神经科学的最新进展为验证这一假设提供了基础和方法，并推动语言理解机制的研究。


<details>
  <summary>Details</summary>
Motivation: 探讨语言理解不仅包括识别表层意义，还需要构建丰富的情境心理模型。

Method: 回顾认知神经科学领域的相关证据，提出理论假设，并讨论可用于直接验证该假设的研究方法。

Result: 提出深度语言理解需要将信息从核心语言系统迁移至其他脑区，以实现感知、运动表征、心理模型构建以及知识和记忆的存储。认知神经科学的进展为理论奠定基础并提供新方法。

Conclusion: 理解语言不仅依赖核心语言系统，更需全脑协同，包括与感知、动作、记忆等相关脑区的信息交流。认知神经科学的最新进展为研究语言理解的认知和神经机制开拓新方向。

Abstract: Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.

</details>


### [23] [Gender Bias in Emotion Recognition by Large Language Models](https://arxiv.org/abs/2511.19785)
*Maureen Herbert,Katie Sun,Angelica Lim,Yasaman Etesam*

Main category: cs.CL

TL;DR: 论文评估LLM在情绪心理推理任务中是否有性别偏见，结果表明，降低偏见需依赖训练阶段的干预，而不是仅靠提示工程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用于日常生活，提高其公平性极为重要，尤其是在处理情感推理和涉及性别的场景中。

Method: 分析LLM对描述人物及其情境后，回答“这个人感觉如何？”时是否出现性别偏见，同时提出并检验多种去偏策略，包括提示工程和训练干预。

Result: 仅用推理阶段的提示工程无法有效降低性别偏见，训练阶段的去偏方法能更有意义地减少偏见。

Conclusion: 要实现语言模型公平性，尤其在情绪心理领域减少性别偏见，必须采用训练阶段的干预方法，并非仅靠推理阶段的提示工程。

Abstract: The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, "How does this person feel?". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.

</details>


### [24] [Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions](https://arxiv.org/abs/2511.19816)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 作者推出了覆盖度更广、带有详细人类情感评分的NRC VAD情感词典v2，首次系统覆盖英语多词表达（如习语、复合名词等），为情感计算和多学科研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 现有情感词典主要涵盖单词，对多词表达和新词覆盖不足，影响语言情感分析精度，亟需更大覆盖面和更详细评分的人类词典以推动NLP等多领域研究。

Method: 收集英语多词表达（MWE）及其成分词的人类VAD（愉悦、唤醒、支配）评分，大幅扩展词表词条，包括新常见词；对评分可靠性进行检验，并分析多词表达的情感特征与成分构成。

Result: 新发布的NRC VAD Lexicon v2包括1万条MWE和2.5万条单词的VAD评分，覆盖率显著提升，评分高度可靠。利用新词典发现MWE显著表现出情感特征，并分析了其情感可组合性。

Conclusion: NRC VAD Lexicon v2是情感分析领域的重要工具，能更全面准确捕捉复杂语言中的情感信息，将促进NLP、心理学、数字人文等领域的研究和应用。

Abstract: Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html

</details>


### [25] [Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana](https://arxiv.org/abs/2511.19818)
*Koena Ronny Mabokela,Tim Schlippe,Mpho Raborife,Turgay Celik*

Main category: cs.CL

TL;DR: 本研究提出并验证了一种基于表情符号和单词的自动情感标注方法，能有效提升对南非多语种推文的标注效率。该方法减少了人工工作量，有助于低资源语言的情感分析研究。


<details>
  <summary>Details</summary>
Motivation: 许多非英语的非洲语言由于缺乏带有情感标签的数字资源，被归类为低资源语言。手动标注这些数据既耗时又昂贵，因此需要一种自动化、高效的标注方法来减少人工工作量。

Method: 提出了一种语言无关的自动情感标注方法，利用含有情感信息的表情符号和词语，对推文进行情感自动标注。实验基于SAfriSenti数据集中的英语、Sepedi语和Setswana语推文进行。

Result: 所提出的标注方法在英语推文中的准确率为66%、Sepedi语推文为69%、Setswana语推文为63%。平均来看，仅有34%的自动生成标签需要人工纠正。

Conclusion: 该方法能有效减少手动情感标注的工作量，提高了低资源语言数据集的标注效率。

Abstract: Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.

</details>


### [26] [Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs](https://arxiv.org/abs/2511.19852)
*Shi-Wei Dai,Yan-Wei Shie,Tsung-Huan Yang,Lun-Wei Ku,Yung-Hui Li*

Main category: cs.CL

TL;DR: 本文提出了PersonaPulse框架，通过动态优化提示词显著提升了LLMs的人格表达能力，并证明提示词优化对个性建模作用重大。


<details>
  <summary>Details</summary>
Motivation: 个性化大语言模型（LLMs）能够提升用户与AI的交互体验，但现有方法在通过提示词激发模型人格特质方面，未能优化提示词以最大化人格表达。

Method: 提出PersonaPulse框架，动态优化角色扮演提示词，并使用情境反应基准作为评分工具，引导优化流程。该方法利用LLMs对人格特质的内在知识，迭代提升人格表达的真实感与情境契合度。

Result: PersonaPulse生成的提示词在定量评估中显著优于基于心理学描述的传统方法。实验还探讨了模型规模与人格建模的关系，并发现某些人格特质的展现程度可通过暂停优化过程来调控。

Conclusion: 提示词优化对塑造LLMs的人格表达至关重要，为未来适应性AI交互研究提供了新思路。

Abstract: Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.

</details>


### [27] [A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction](https://arxiv.org/abs/2511.19858)
*Farzad Ahmed,Joniel Augustine Jerome,Meliha Yetisgen,Özlem Uzuner*

Main category: cs.CL

TL;DR: 该论文系统比较了三种大语言模型提示策略在医学文档错误检测和纠正中的有效性，发现检索增强动态提示显著优于其他方法，提升了检测召回率和准确率，并有效减少误报。


<details>
  <summary>Details</summary>
Motivation: 临床文档存在事实、诊断及管理错误，这些错误可能影响患者安全。大语言模型（LLMs）被认为有潜力帮助发现和纠正这些错误，但针对不同提示策略下LLM表现尚不明确，因此需系统评估这些方法在医学错误处理中的有效性。

Method: 采用MEDEC数据集，评估了九种经过指令微调的大语言模型（包括GPT、Claude、Gemini及OpenAI o-series），分别测试零样本提示、静态随机示例提示以及检索增强动态提示在医疗错误检测三子任务中的表现。通过准确率、召回率、误报率（FPR）及ROUGE-1、BLEURT和BERTScore聚合分数评估纠错效果，并分析模型输出与临床医生推理的差异。

Result: 零样本提示在检测任务中召回率较低，尤其容易遗漏缩写或非典型错误。静态随机示例提示提升了召回率但增加了误报。检索增强动态提示在所有模型中能显著降低误报率约15%、提升召回率5-10%，并生成更具上下文准确性的错误修正结果。

Conclusion: 检索增强动态提示在各种LLM上均优于零样本和静态提示，能够提升检测准确性、减少误报，并增强医学错误纠正的可靠性。

Abstract: Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.

</details>


### [28] [AppSelectBench: Application-Level Tool Selection Benchmark](https://arxiv.org/abs/2511.19957)
*Tianyi Chen,Michael Solodko,Sen Wang,Jongwoo Ko,Junheng Hao,Colby Banbury,Sara Abdali,Saeed Amizadeh,Qing Xiao,Yinheng Li,Tianyu Ding,Kamran Ghasedi Dizaji,Suzhen Zheng,Hao Fan,Justin Wagle,Pashmina Cameron,Kazuhito Koishida*

Main category: cs.CL

TL;DR: 本文提出了AppSelectBench，一套用于评测智能体应用选择能力的新基准，涵盖100个应用和超过10万个真实用户任务。实验显示现有大模型在该任务上的表现尚有明显不足，AppSelectBench为推动智能体应用推理研究提供了平台和挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的评测基准主要关注于细粒度的API选择，难以反映智能体是否具备跨应用程序选择与推理能力。而应用选择对于智能体初始化正确环境、避免操作混乱、聚焦相关上下文具有基础性作用，因此需要一个更全面、更具代表性的评测框架。

Method: 作者提出了AppSelectBench，这是一个专为评测智能体应用选择能力而设计的基准。它包含自动化的大规模用户任务生成流程，覆盖100个常用应用和超过10万个真实语义任务，提供统一的评测协议（如随机、启发式、零样本、小样本和检索增强设置）。

Result: 通过在闭源和开源大语言模型上的广泛实验证明，现有模型在跨应用推理任务上表现出系统性的优缺点，甚至能力最强的模型在一致性应用选择方面依然存在较大挑战。

Conclusion: AppSelectBench为研究和推动智能体应用级别推理能力奠定了基础，有助于发掘和提升CUA尚未充分开发的重要能力。

Abstract: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.

</details>


### [29] [$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers](https://arxiv.org/abs/2511.19987)
*Xinyu Wang,Hanwei Wu,Qingchen Hu,Zhenghan Tai,Jingrui Tian,Lei Ding,Jijun Chi,Hailin He,Tung Sum Thomas Kwok,Yufei Cui,Sicheng Lyu,Muzhi Li,Mingze Li,Xinyue Yu,Ling Zhou,Peng Lu*

Main category: cs.CL

TL;DR: 本论文提出 R2R 框架，通过动态专家路由与反捷径实体抽象训练，有效解决高专业领域 reranker 模型的表面过拟合和灾难性遗忘问题。在法律、金融、医学等领域测试优于通用和单领域微调模型，表现出很强的跨领域泛化能力与鲁棒性，方案通用可模块化集成。


<details>
  <summary>Details</summary>
Motivation: 解码器仅 reranker 在 Retrieval-Augmented Generation（RAG）中起关键作用，但通用模型在金融、法律等高度专业化领域难以捕捉领域特有细节，而直接微调容易导致表面特征过拟合及灾难遗忘。

Method: 提出 R2R 框架，结合动态专家路由与两阶段训练策略。通过 Entity Abstraction for Generalization（EAG），采用反捷径机制，遮蔽最具预测性的实体表面线索，促进模型学习领域不变的相关性模式。同时，引入 Latent Semantic Router 从冻结的主干解码器内部表征中选择最优 LoRA 专家，以高效激活领域专家。

Result: 在不同 reranker 主干和多个领域（法律、医学、金融）上的广泛实验表明，R2R 的表现优于通用模型和单领域微调基线。

Conclusion: R2R 是一种对模型算法无关且模块化的领域专用方法，具有很强的跨领域鲁棒性。

Abstract: Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.

</details>


### [30] [Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test](https://arxiv.org/abs/2511.19997)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 即使排除语义和语料特征，仅靠架构本身，Transformer依然对逆序学习表现出固有困难；论文提出新基准揭示该偏差，推动机制解释。


<details>
  <summary>Details</summary>
Motivation: Transformer模型理论上对序列方向是无偏的，但实际应用中却常出现“方向性诅咒”，即对逆序预测比正序预测表现差。科学界尚不清楚这种现象是由语言数据本身，还是模型结构引起。本文旨在消除数据统计因素影响，纯粹考察架构对方向性的影响。

Method: 提出全合成、熵可控的基准测试，利用可调分支因子K生成随机字符串映射，分设零条件熵的正向任务，以及熵下限已知的逆向任务。通过对GPT-2及MLP模型从零训练与预训练初始条件的测试，系统分析其方向性学习表现。

Result: 研究发现，即便消除了语义、频率和时间先验，仅在合成环境中训练，GPT-2依然存在明显的方向性优化差距（如K=5时，逆序多1.16 nats损失），高于MLP。预训练只能部分缓解，LoRA在高熵逆任务上很快遇到容量瓶颈。表明方向性摩擦是Transformer训练的内在特性。

Conclusion: 论文首次在严格受控合成任务下，证实了Transformer模型逆序学习更难的方向性偏差即使不依赖语义和语料特点也普遍存在，这一现象与模型训练机制相关。该基准为剖析序列模型方向性偏差及其机制研究提供了实验工具和动机。

Abstract: Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.

</details>


### [31] [A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media](https://arxiv.org/abs/2511.20001)
*Edward Ajayi,Martha Kachweka,Mawuli Deku,Emily Aiken*

Main category: cs.CL

TL;DR: 本研究提出多分类检测框架，针对社交媒体心理健康与网络欺凌，MentalBERT表现最佳。系统强调可解释性，并作为人工辅助工具，推动未来高质量数据集建设。


<details>
  <summary>Details</summary>
Motivation: 数字空间中心理健康问题和网络欺凌现象日益严重，现有检测系统可扩展性和可解释性不足，亟需高效且可解释的多类别识别方法。

Method: 作者提出统一多分类框架，检测社交媒体中的十类心理健康和网络欺凌类别。数据来源为Twitter和Reddit，采用“split-then-balance”数据处理流程，在平衡数据集上训练并在不平衡真实测试集上评估。模型评估包含传统词汇、混合方法以及端到端微调的Transformer模型。引入了SHAPLLM可解释性框架，以及集成预测与解释的原型仪表盘。

Result: 端到端微调Transformer性能最佳，MentalBERT模型准确率达0.92，Macro F1为0.76，优于通用模型和零样本大语言模型。系统作为人工辅助筛查工具而非诊断工具。

Conclusion: 文中方法为网络安全与计算心理健康交叉领域提供了坚实基线，并强调未来需多标签、临床验证的数据集。

Abstract: Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous "split-then-balance" pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conducted a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAPLLM explainability framework and present a prototype dashboard ("Social Media Screener") designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.

</details>


### [32] [Online-PVLM: Advancing Personalized VLMs with Online Concept Learning](https://arxiv.org/abs/2511.20056)
*Huiyu Bai,Runze Wang,Zhuoyun Du,Yiyang Zhao,Fengji Zhang,Haoyu Chen,Xiaoyong Zhu,Bo Zheng,Xuejiao Zhao*

Main category: cs.CL

TL;DR: 论文提出了无需训练、在线生成个性化视觉语言模型概念嵌入的新框架Online-PVLM，并开发大规模评测集OP-Eval，在真实场景下实现高效和领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有个性化视觉语言模型（VLMs）在用户专属概念交互效率上受到限制，特别是在大规模场景中，实时生成和检索概念嵌入较为困难。

Method: 提出Online-PVLM框架，采用超球面表示，实现测试阶段无需训练即可生成概念嵌入，从而提升VLM的可扩展性和效率。同时开发了OP-Eval基准，包括1292个概念和3万多个高质量实例，用于真实场景在线概念学习的评估。

Result: 在线和大规模条件下，Online-PVLM展现了领先性能，显著优于现有方法。

Conclusion: Online-PVLM克服了现有个性化VLM实现实时自适应的瓶颈，通过无训练生成、高效扩展和基准测试，推动了个性化视觉语言模型的落地与发展。

Abstract: Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.

</details>


### [33] [MTA: A Merge-then-Adapt Framework for Personalized Large Language Model](https://arxiv.org/abs/2511.20072)
*Xiaopeng Li,Yuanjin Zheng,Wanyu Wang,wenlin zhang,Pengyue Jia,Yiqi Wang,Maolin Wang,Xuetao Wei,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 本文提出了无需为每位用户分别微调的个性化大模型方法，通过共享与动态合成的LoRA模块，高效实现了可扩展且针对少样本更友好的个性化大模型，在多个任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前个人化大模型的训练方式需要为每个用户分别微调一个模块，导致存储成本随用户数线性增长，且对于数据稀疏用户效果不佳，难以扩展。

Method: 提出MTA（Merge-then-Adapt）框架。包括三步：（1）通过锚点用户预训练meta-LoRA，构建共享的Meta-LoRA Bank；（2）通过Adaptive LoRA Fusion，从Bank中检索和动态合并相关meta-LoRA，实现无用户特定存储的动态个性化；（3）少样本条件下再叠加一个超低秩LoRA进行轻量微调。

Result: 在LaMP基准任务上广泛实验证明，所提方法在多个任务上优于现有最优方法。

Conclusion: 通过Merge-then-Adapt框架，解决了个性化大模型在存储和数据稀疏场景下的扩展与效果难题，实现了更高效且灵活的个性化模型输出。

Abstract: Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.

</details>


### [34] [More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering](https://arxiv.org/abs/2511.20086)
*Duc Anh Vu,Thong Nguyen,Cong-Duy Nguyen,Viet Anh Nguyen,Anh Tuan Luu*

Main category: cs.CL

TL;DR: 该论文提出通过生成并综合每个选项理由的新推理框架BiasPrompting，显著提升大模型在选择题任务中的推理能力和表现。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在多项选择题任务中虽然表现提升，但回答选项缺乏上下文和理由说明，导致推理不充分、准确率受影响。

Method: 提出BiasPrompting推理框架，分为两个阶段：首先生成每个选项的支持性理由，再综合所有理由来选择最合理答案。

Result: 在五个主流多项选择题基准测试中显著提升模型表现，尤其是在复杂问题或现有方法效果较差的场景下提升大模型推理能力。

Conclusion: BiasPrompting能够有效增强大语言模型的推理能力，为解决复杂选择题和提升答案可靠性提供了坚实基础。

Abstract: With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.

</details>


### [35] [SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space](https://arxiv.org/abs/2511.20102)
*Zhenyi Shen,Junru Lu,Lin Gui,Jiazheng Li,Yulan He,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: 本文针对大语言模型注意力机制计算瓶颈，提出了SSA框架，结合稀疏和全注意力训练，解决了梯度不足问题。实验表明SSA显著提升了稀疏注意力性能和长文本处理能力，并可灵活调整推理计算量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于大模型的全注意力（full attention）具有二次复杂度，导致处理长文本变得低效。虽然稀疏注意力（sparse attention）能够降低计算成本，但目前无训练的稀疏注意力方法性能下降明显，而原生稀疏注意力方法又存在注意力稀疏度不足的悖论。作者分析这种悖论是梯度更新不足导致的，必须解决此问题以提升长上下文处理效率。

Method: 作者提出了一种统一训练框架SSA（Sparse Sparse Attention），结合了稀疏与全注意力，通过每一层的双向对齐，保持所有令牌的梯度流动。该方法让稀疏注意力的输出更接近全注意力，同时实现更强稀疏性。

Result: SSA框架在多种常识基准测试下，稀疏和全注意力推理都达到了最新水平。模型能够灵活适应不同稀疏预算，令性能与推理计算量可平衡。同时，原生稀疏训练显著提升了长文本外推能力，SSA表现最优。

Conclusion: SSA训练框架不仅提升了稀疏注意力的性能和稀疏性，还能灵活调节推理时的计算与性能权衡，极大改善了长上下文处理和外推能力。

Abstract: The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.

</details>


### [36] [EM2LDL: A Multilingual Speech Corpus for Mixed Emotion Recognition through Label Distribution Learning](https://arxiv.org/abs/2511.20106)
*Xingfeng Li,Xiaohan Shi,Junjie Li,Yongwei Li,Masashi Unoki,Tomoki Toda,Masato Akagi*

Main category: cs.CL

TL;DR: EM2LDL是一个专为多语种混合情感识别设计的新型语音语料库，包含英语、普通话和粤语真实表达，支持分布式多情感标注，实验验证其在不同个体属性上的鲁棒性，有助于发展面向情感计算和心理健康等跨文化应用的智能系统。


<details>
  <summary>Details</summary>
Motivation: 当前情感语音语料库多为单语、单标签，无法有效刻画多语种混合情感及现实世界真实表达，限制了跨文化、个性化情感计算应用的发展。

Method: 构建包含英语、普通话和粤语的多语种语音语料库，收集来自网络平台的真实情感表达，并通过32类细粒度情感分类进行分布式标注。采用自监督学习模型（如HuBERT-large-EN）进行基线评估，实现说话人性别、年龄和人格的独立测试。

Result: 实验结果显示，EM2LDL在多维度基线评估中表现优越，尤其HuBERT-large-EN模型各项指标最佳。语料库支持复杂情感动态研究，为自适应和具备同理心的智能系统开发奠定了基础。

Conclusion: EM2LDL语料库在提升多语种混合情感识别和标签分布学习方面展现了强大潜力，尤其在生态有效性和语言多样性上突破了传统语料库的限制。

Abstract: This study introduces EM2LDL, a novel multilingual speech corpus designed to advance mixed emotion recognition through label distribution learning. Addressing the limitations of predominantly monolingual and single-label emotion corpora \textcolor{black}{that restrict linguistic diversity, are unable to model mixed emotions, and lack ecological validity}, EM2LDL comprises expressive utterances in English, Mandarin, and Cantonese, capturing the intra-utterance code-switching prevalent in multilingual regions like Hong Kong and Macao. The corpus integrates spontaneous emotional expressions from online platforms, annotated with fine-grained emotion distributions across 32 categories. Experimental baselines using self-supervised learning models demonstrate robust performance in speaker-independent gender-, age-, and personality-based evaluations, with HuBERT-large-EN achieving optimal results. By incorporating linguistic diversity and ecological validity, EM2LDL enables the exploration of complex emotional dynamics in multilingual settings. This work provides a versatile testbed for developing adaptive, empathetic systems for applications in affective computing, including mental health monitoring and cross-cultural communication. The dataset, annotations, and baseline codes are publicly available at https://github.com/xingfengli/EM2LDL.

</details>


### [37] [Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach](https://arxiv.org/abs/2511.20107)
*Huu Tuong Tu,Ha Viet Khanh,Tran Tien Dat,Vu Huan,Thien Van Luong,Nguyen Tien Cuong,Nguyen Thi Thu Trang*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的新型发音错误检测与诊断方法，利用预训练语音识别模型和检索技术实现高效高准确度，实验证明性能优越且系统简单。


<details>
  <summary>Details</summary>
Motivation: 传统的发音错误检测与诊断（MDD）方法依赖于建模、训练音素级别评分模型，过程复杂，对算力和数据要求高，因此需要更简便高效的方法。

Method: 提出了一种全新的、无需训练的MDD框架，利用预训练的语音识别（ASR）模型结合检索技术进行错误检测与诊断，无需针对音素建模或额外任务训练。

Result: 在L2-ARCTIC数据集上，所提方法实现了69.60%的F1分数，优于传统方法，并大大简化了系统的复杂度。

Conclusion: 通过结合预训练ASR与检索技术，不仅避免了繁琐的训练流程和模型复杂性，还实现了更准确的发音错误检测，为MDD任务提供了高效实用的新思路。

Abstract: Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.

</details>


### [38] ["When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings](https://arxiv.org/abs/2511.20120)
*Somsubhra De,Harsh Kumar,Arun Prakash A*

Main category: cs.CL

TL;DR: 本文通过prompt+LLM方法在多种印度语的语法纠错任务中大幅提升效果，实验证明现有大型模型即使在低资源环境下也有强大泛化能力，显著优于本地微调模型，具有实际推广价值。


<details>
  <summary>Details</summary>
Motivation: 尽管英语等高资源语言在语法纠错（GEC）任务上已取得显著进展，但大多数印度语种因资源匮乏、语言多样性和复杂形态变化，GEC任务仍然极具挑战。

Method: 采用最先进的大型语言模型（如GPT-4.1、Gemini-2.5和LLaMA-4），结合基础至少量学习的prompting方法（包括zero-shot和few-shot策略），以适应低资源语言环境，并与印度语种的微调模型进行对比。

Result: 实验证明，即使是基本prompting策略，也能让LLMs在多个印度语种GEC任务中大幅超越微调大型本地模型，如Sarvam-22B。经过轻量级prompt设计与适配后，在多种印度语的纠错质量上取得显著提升，并在GEC相关竞赛中排名突出（例如泰米尔语第1、印地语第1等）。

Conclusion: Prompting结合LLMs能极大提升多语种、低资源条件下的GEC效果，展示了当前大型模型的卓越多语种泛化能力，有望缩小多语种GEC领域的资源鸿沟。

Abstract: Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.

</details>


### [39] [SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models](https://arxiv.org/abs/2511.20143)
*Wen-Fang Su,Hsiao-Wei Chou,Wen-Yang Lin*

Main category: cs.CL

TL;DR: 该论文提出在格子标注命名实体识别模型中应用图像数据增强技术，有效解决了不连续实体的分割和识别难题，实验验证显著提升了识别效果。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别（NER）很难准确识别跨句子分布的不连续实体，主要挑战在于文本分段，传统方法在分割和识别不连续实体时容易出错或遗漏。

Method: 在已有的格子标注（grid-tagging）信息抽取方法基础上，结合图像数据增强技术（如裁剪、缩放、填充）对格子标注模型进行改进，以提升识别不连续实体的能力，尤其针对分段问题。

Result: 在CADEC、ShARe13和ShARe14数据集上，改进后的模型整体F1分数提升1-2.5%，对不连续实体F1分数提升3.7-8.4%。

Conclusion: 通过将图像增强技术融入格子模型，有效提高了对跨句子不连续实体的识别准确率，克服了传统分段方法的局限性。

Abstract: Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.

</details>


### [40] [KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP](https://arxiv.org/abs/2511.20182)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.CL

TL;DR: 该论文发布了首个吉尔吉斯语BERT模型KyrgyzBERT，并创建了情感分析基准，模型表现优异且资源全部公开，显著促进了吉尔吉斯语NLP发展。


<details>
  <summary>Details</summary>
Motivation: 吉尔吉斯语作为一种资源稀缺语言，缺乏基础的自然语言处理工具。当前针对吉尔吉斯语的NLP研究较少，推动相关工具和资源的建立对该语言的数字化至关重要。

Method: 本论文提出KyrgyzBERT，这是首个公开的吉尔吉斯语单语BERT语言模型，并采用针对吉尔吉斯语形态结构定制的分词器。为评估性能，作者构建了kyrgyz-sst2情感分析基准，通过翻译Stanford Sentiment Treebank并手动注释完整测试集来实现。

Result: KyrgyzBERT在kyrgyz-sst2数据集微调后取得了0.8280的F1分数，与参数量大五倍的mBERT模型性能持平。此外，作者公开了所有模型、数据和代码，促进吉尔吉斯语NLP发展。

Conclusion: KyrgyzBERT扩大了吉尔吉斯语NLP工具和资源，为该语言相关研究提供了坚实基础，其性能显示专用模型在资源稀缺语言任务中具有效率。

Abstract: Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.

</details>


### [41] [REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance](https://arxiv.org/abs/2511.20233)
*Chuyi Kong,Gao Wei,Jing Ma,Hongzhan Lin,Zhiyuan Fan*

Main category: cs.CL

TL;DR: REFLEX无需重度依赖外部知识，通过新颖对话式训练和内部激活提取，显著提升事实核查的准确性和解释能力，并能有效赋能其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的事实核查方法对外部知识依赖高，导致响应速度慢且容易产生幻觉，影响可靠性、可解释性和实时性。该文旨在消除这些问题，通过挖掘主干模型内部知识实现高质量解释和判决。

Method: 提出了REFLEX范式，一种用于自动事实核查的新方法，通过角色扮演对话的形式联合训练判决预测和解释生成，利用主干模型内部知识，并采用自适应激活对比提取构建指向真理的向量，抑制噪声解释，提升推理的准确性和效率。

Result: REFLEX在真实数据集上大幅优于现有方法，仅用465个自我优化样本即取得SOTA性能，且解释目标训练下的模型能提升无解释目标模型，最高提升7.57%。

Conclusion: REFLEX展现出丰富的内部知识挖掘与利用潜力，极大提升了事实核查系统的效率与可解释性，推动了事实核查技术向更切实可用和智能的发展。

Abstract: The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.

</details>


### [42] [Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios](https://arxiv.org/abs/2511.20340)
*Luohe Shi,Zuchao Li,Lefei Zhang,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: 作者提出了SpecFormer架构，实现了在低验证资源条件下依然有效的投机解码，加速了主流大模型推理，同时降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 主流的大模型推理系统普遍采用批量推理方式，导致闲置计算资源有限，现有投机解码方法难以适用。实现低验证资源和低调度成本下的投机解码成为关键挑战。

Method: 提出了一种新架构SpecFormer，将单向和双向注意力机制结合在一起，融合自回归模型与非自回归模型的优点，实现了无需大型前缀树的投机解码方法。

Result: SpecFormer可以在不同规模模型上进行无损的投机解码，加速大模型推理，并降低训练需求和计算成本。

Conclusion: SpecFormer架构解决了高并发与低空闲资源下投机解码效率低的问题，使得大批次情况下依然能实现稳定加速，为大语言模型推理扩展设立了新标准。

Abstract: Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.

</details>


### [43] [The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models](https://arxiv.org/abs/2511.20344)
*Taewhoo Lee,Minju Song,Chanwoong Yoon,Jungwoo Park,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本研究系统探讨了大语言模型在类比推理中的关系编码与应用能力，发现其可以初步捕捉和传播高阶关系，但在泛化到新情境时仍有限，揭示了其与人类认知的相似与不足之处。


<details>
  <summary>Details</summary>
Motivation: 类比推理是人类认知的核心，但尚不清楚大语言模型(LLM)是否能像人类一样编码高阶关系概念并在新情境下应用这些知识。该研究旨在填补机器与人类在类比推理能力上的差距。

Method: 论文通过使用比例类比和故事类比任务，分析大语言模型在中高层网络中对于归因信息和关联信息的编码方式，以及在新实体上的应用能力。同时，通过对隐藏表示的有针对性修补，观察信息转移过程，并系统评估成功和失败的类比推理案例。

Result: 大语言模型能够在一些情境下有效编码和传播类比对象间的关系信息，但在信息缺失或尝试对新实体应用时容易受阻，有时通过修补关键位置的隐藏表示可部分缓解。模型成功的类比推理表现为结构对齐良好，而失败则通常为结构对齐弱化或错位。

Conclusion: 大语言模型在高阶关系概念的编码与应用方面显示出初步但有限的能力。虽然其在某些方面与人类认知存在相似性，但仍有明显差距，特别是在对新情境下关系知识迁移方面。

Abstract: Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.

</details>


### [44] [BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali](https://arxiv.org/abs/2511.20399)
*Abdullah Al Sefat*

Main category: cs.CL

TL;DR: 本文提出了针对低资源语言孟加拉语的谜语挑战数据集BengaliFig，用于测试主流语言模型在具象和文化推理方面的能力，结果揭示了现有模型在这一领域的显著不足，为今后更具文化包容性的NLP研究和评测提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）虽然在多语言基准测试中表现优秀，但在处理具象和与文化相关的推理，尤其是在低资源语言环境下，尚缺乏系统性评估。为填补这一领域空白，研究者希望推动对低资源文化环境中LLMs推理能力的检测和提升。

Method: 提出BengaliFig数据集，包含435个独特的孟加拉语谜语，涵盖口头与文学传统，并沿五个维度详细注释（推理类型、陷阱类型、文化深度、答案类别和难度）。全部谜题通过AI辅助流程自动转化为多选题。评估了八个主流前沿LLMs，在零样本和少样本链式思维提示下，分析其在隐喻和文化相关推理方面的表现。

Result: 八种主流LLMs在隐喻性和文化特异性推理任务上均表现出一致的不足，展现了目前模型在低资源文化推理任务上的显著弱点。

Conclusion: BengaliFig不仅是评估LLMs在低资源文化语境下鲁棒性的有效工具，也为更加包容和注重文化传承的自然语言处理（NLP）评价奠定了基础。

Abstract: Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.

</details>


### [45] [A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines](https://arxiv.org/abs/2511.20409)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 本文提出了一套新的多维度词干器评估方法，发现高效词干未必有益于下游任务，语义保留同样重要，为评估与选型提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 现有的词干提取（stemming）评估方法有限，且未能充分反映“过度词干提取”可能造成的危害，亟需更全面和实用的评估方式。

Method: 提出了一种新颖的、任务导向的词干提取方法评估框架，综合考虑了词干效果（SES）、对下游任务影响（MPD）和词义保持（ANLD）三个方面。并以Bengali和英文词干器为例应用该评估框架进行比较分析。

Result: Bangla词干器在词干效果评分（SES）中最高，但在词义保持方面（ANLD分数）表现不佳，导致下游性能下降；而英文词干器尽管SES较低，但在词义保持上表现良好，并促进了下游任务表现提升。

Conclusion: SES指标不能单独反映词干器好坏，须结合语义距离（ANLD）等安全性指标综合评价，才能更有效地区分效率提升和语义保留。该框架为今后的词干器评估与选择提供了可靠工具。

Abstract: Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).

</details>


### [46] [Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts](https://arxiv.org/abs/2511.20459)
*Mosab Rezaei,Mina Rajaei Moghadam,Abdul Rahman Shaikh,Hamed Alhoori,Reva Freedman*

Main category: cs.CL

TL;DR: 该论文提出用大语言模型和AI检测器自动生成及评价19世纪知名小说家风格文本，无需人工配对评价。方法通过微调和可解释AI识别文体特征，实现了可靠的自动化文体学研究，所有研究产出已公开。


<details>
  <summary>Details</summary>
Motivation: 近期的大型语言模型进展为文体学（写作风格与作者研究）带来了新机遇，但面对两大主要挑战：1）在无配对数据情况下如何训练生成模型；2）如何在不完全依赖人工判断的前提下评价文体文本。

Method: 提出了一个既可生成又可评价19世纪小说家文体句子的框架。通过最小化、单词提示微调大型语言模型，使其能生成狄更斯、奥斯汀、马克·吐温、奥尔科特、梅尔维尔等作家的文本。评估方法为：用基于transformer的检测器在真实句子上训练，并作为分类器和风格解释工具；结合句法对比和可解释AI（attention和梯度分析）方法，识别驱动文体模仿的语言线索。

Result: 生成文本能够体现这些作者独特的风格特征。AI自动评价在可靠性方面是人工评价的替代方案。所有的研究产出均已公开发布。

Conclusion: 通过微调大模型和AI检测技术，生成并评价19世纪小说家风格文本变得可行，AI方法在文体研究中展现出较好表现。

Abstract: Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.

</details>


### [47] [Adversarial Confusion Attack: Disrupting Multimodal Large Language Models](https://arxiv.org/abs/2511.20494)
*Jakub Hoscilowicz,Artur Janicki*

Main category: cs.CL

TL;DR: 该论文提出了针对多模态大语言模型（MLLM）的新型系统性干扰攻击方法，通过扰乱模型输出使其产生不连贯或自信但错误的回答。方法简单但迁移性强，适用于多类模型，揭示了MLLM安全方面的新隐患。


<details>
  <summary>Details</summary>
Motivation: 目前针对多模态大语言模型（MLLMs）的攻击主要集中在越狱或定向误判等有限目标，但缺少对系统性扰乱模型稳定性的研究。作者旨在提出新的攻击方式，提升安全防护意识。

Method: 作者提出了一种新的攻击方式——Adversarial Confusion Attack，通过最大化下一个token的熵，利用开源MLLMs的小型集成，通过PGD进行白盒攻击。采用嵌入式对抗图像来扰乱模型在网站等场景下的正常运行。

Result: 在白盒环境下，一个对抗性图像能同时扰乱集成中的所有模型，无论完整图片还是对抗型验证码场景均有效。生成的扰动还可迁移到未见过的开源模型（如Qwen3-VL）和闭源模型（如GPT-5.1）。

Conclusion: Adversarial Confusion Attack可以显著提升对MLLM的干扰性，并具备广泛的迁移能力，体现了当前防护体系下的安全隐患。

Abstract: We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Applications include embedding adversarial images into websites to prevent MLLM-powered agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.

</details>


### [48] [The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models](https://arxiv.org/abs/2511.20507)
*Nathan Roll,Jill Kries,Flora Jin,Catherine Wang,Ann Marie Finley,Meghan Sumner,Cory Shain,Laura Gwilliams*

Main category: cs.CL

TL;DR: 本文提出了用于评估大型语言模型（LLM）类似失语症语言缺陷的标准化文本性测试工具TAB，并通过自动化协议验证其可靠性，以支持大规模LLM语言障碍研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在模拟和研究人类语言，特别是语言障碍（如失语症）的计算基础上展现出巨大潜力，但传统的临床评估方法并不适合直接应用于LLM。本文旨在弥补这一空白。

Method: 本文提出了文本失语症测试（TAB），这是一个以文本为基础，对现有Quick Aphasia Battery(QAB)失语症快速测试进行改编的基准工具，包含四项子测试（连贯文本、词汇理解、句子理解和复述），并详细阐述了评分标准。为实现大规模自动评估，作者还设计并验证了基于Gemini 2.5 Flash模型的自动化评分协议。

Result: 自动化协议的可靠性与专家人工评分接近（模型与共识评分者的加权Cohen's kappa为0.255，人工评分者间为0.286）。TAB基准被发布为临床基础的、可扩展分析人工系统语言障碍的新框架。

Conclusion: TAB为研究和量化LLM中的类失语症缺陷提供了科学、标准化和可扩展的评估工具，促进了AI在神经语言科学和认知研究中的应用。

Abstract: Large language models (LLMs) have emerged as a candidate "model organism" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.

</details>


### [49] [Bridging the Language Gap: Synthetic Voice Diversity via Latent Mixup for Equitable Speech Recognition](https://arxiv.org/abs/2511.20534)
*Wesley Bian,Xiaofeng Lin,Guang Cheng*

Main category: cs.CL

TL;DR: 提出了一种创新的数据增强方法，显著提升了低资源语言语音识别效果，优于现有方法，推动了低资源语言语音技术的发展。


<details>
  <summary>Details</summary>
Motivation: 当前语音任务中的机器学习模型在英语等资源丰富的语言上效果优异，主要由于训练数据充足，而低资源语言因数据稀缺，表现存在明显差距。

Method: 作者提出了一种新颖的语音语料库数据增强方法，旨在提升低资源语言的表现。

Result: 实验结果显示，该数据增强方法显著提升了低资源语言自动语音识别系统的性能，且优于现有的增强方法。

Conclusion: 该方法为提升欠代表性语言社区的语音技术提供了有效的解决方案。

Abstract: Modern machine learning models for audio tasks often exhibit superior performance on English and other well-resourced languages, primarily due to the abundance of available training data. This disparity leads to an unfair performance gap for low-resource languages, where data collection is both challenging and costly. In this work, we introduce a novel data augmentation technique for speech corpora designed to mitigate this gap. Through comprehensive experiments, we demonstrate that our method significantly improves the performance of automatic speech recognition systems on low-resource languages. Furthermore, we show that our approach outperforms existing augmentation strategies, offering a practical solution for enhancing speech technology in underrepresented linguistic communities.

</details>


### [50] [From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding](https://arxiv.org/abs/2511.20547)
*Farjana Sultana Mim,Shuchin Aeron,Eric Miller,Kristen Wendell*

Main category: cs.CL

TL;DR: 作者建立了一个教育对话注释数据集，关注学生知识建构与任务执行两种话语。利用GPT-3.5和Llama-3.1进行自动检测基准实验，结果一般，说明现有模型在该领域表现不足，有待进一步改进。


<details>
  <summary>Details</summary>
Motivation: 手动分析学生对话中的话语特征效率低且耗时，限制了研究的规模。利用NLP技术能够实现自动检测，提高分析的可扩展性和数据驱动能力，但目前针对教育领域对话的话语检测研究较少。

Method: 建立了包含知识建构与任务生产话语的学生对话注释数据集，并基于预训练大语言模型（GPT-3.5和Llama-3.1）进行了基线模型实验，用于自动预测每次发言的话语属性。

Result: 实验显示，当前主流大语言模型（GPT-3.5和Llama-3.1）在该任务中的表现有待提升，揭示了未来改进和研究的潜力。

Conclusion: 现有大语言模型（GPT-3.5和Llama-3.1）在自动检测学生对话中的话语特征任务上表现一般，说明还有进一步改进的空间。

Abstract: Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.

</details>


### [51] [On Evaluating LLM Alignment by Evaluating LLMs as Judges](https://arxiv.org/abs/2511.20604)
*Yixin Liu,Pengfei Liu,Arman Cohan*

Main category: cs.CL

TL;DR: 本文发现LLM的生成能力和评价能力强相关，并提出利用评价能力而非生成输出的新对齐度评价基准AlignEval，其效果优越、效率更高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）需要与人类偏好对齐，包括有用性、诚实性、安全性和精确执行人类指令。现有对齐评估往往需人工注释或借助强大评测模型，成本较高，且效率有限。因此，探索一种无需对生成结果直接评估的新对齐评测方式，有现实需求和研究价值。

Method: 本文系统分析了多种LLM的生成-评价一致性（GE-consistency），即其生成能力与评价能力之间的相关性。基于强LLM偏好“神谕者”评测这一相关性，并提出基于评价能力而非直接评估生成输出的新型对齐基准AlignEval。

Result: AlignEval表现良好，在对齐性排名能力上达到或超过了现有知名自动评价基准（如AlpacaEval和Arena-Hard），并能更好地捕捉人类偏好。

Conclusion: LLM的生成能力和评价能力紧密相关。利用LLM自身评价能力，可以高效、准确评估对齐程度，无需直接人工评估输出。AlignEval为LLM对齐评估提供了新思路。

Abstract: Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.

</details>


### [52] [Latent Collaboration in Multi-Agent Systems](https://arxiv.org/abs/2511.20639)
*Jiaru Zou,Xiyuan Yang,Ruizhong Qiu,Gaotang Li,Katherine Tieu,Pan Lu,Ke Shen,Hanghang Tong,Yejin Choi,Jingrui He,James Zou,Mengdi Wang,Ling Yang*

Main category: cs.CL

TL;DR: LatentMAS让多个大语言模型直接在隐空间协作，相比传统文本通信MAS，准确率提升明显，效率大幅提高，无需再训练，效果优越。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）多智能体系统（MAS）主要通过文本通信和推理，存在信息损失和效率低下等问题，亟需寻找更高效的信息交换方式，以提升系统级智能。

Method: 提出LatentMAS框架，使多个LLM智能体直接在连续隐空间内协作，实现无损的信息表达和交换。该框架采用无训练方式，智能体通过最后一层的隐藏嵌入进行自回归潜在思维生成，并利用共享隐空间记忆实现信息保存与转移。

Result: 理论分析证实LatentMAS比传统文本MAS有更高的表达能力和更低的复杂度。实证测试（9个基准任务）显示LatentMAS在准确率上提升最高14.6%，输出token减少70.8%-83.7%，推理速度提升4-4.3倍，性能全面优于单模型和文本MAS。

Conclusion: LatentMAS无需额外训练，通过潜在空间合作显著提升多智能体LLM系统的推理能力和效率，为系统级智能奠定了新基础。

Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [53] [ChemicHull: an online tool for determining extremal chemical graphs of maximum degree at most 3 for any degree-based topological indices](https://arxiv.org/abs/2511.20249)
*Sébastien Bonte,Gauvain Devillez,Valentin Dusollier,Alain Hertz,Hadrien Mélot,David Schindl*

Main category: cs.DM

TL;DR: 本文提出了通过多面体优化理论及在线工具ChemicHull，高效寻找各类度基拓扑指数的极值化学图，揭示只有部分图能实现极值，并用新工具纠正了既有理论错误。


<details>
  <summary>Details</summary>
Motivation: 拓扑指数在数学化学领域用于描述分子结构特征，并能预测其物理化学性质。度基拓扑指数是一类常用的指标，仅依赖于图中顶点的度数。传统方法难以系统性地找出这些指数的极值化学图。为此，需要一个高效工具与理论方法。

Method: 将每个化学图（顶点度最大为3）映射到三维空间的多面体，然后通过线性优化方法确定所有可能的度基拓扑指数在该多面体上的极值。开发了名为ChemicHull的在线工具，可自动计算、展现任意度基拓扑指数的极值化学图。

Result: 分析显示，仅有少数化学图族能够获得度基拓扑指数的极值，部分化学图永远无法成为极值图。ChemicHull工具可以有效、便捷地恢复已知结果，还适用于分析化学树和单环化学图。该工具还发现了关于Randić指数的一个反例，修正了先前的结论。

Conclusion: 本文提出了基于多面体理论的系统方法与在线工具ChemicHull，实现了极值化学图的自动化探索与可视化。新发现修正了关于某些度基拓扑指数极值的已知结论，极高效地服务于化学图分析。

Abstract: Topological indices are graph-theoretic descriptors that play a crucial role in mathematical chemistry, capturing the structural characteristics of molecules and enabling the prediction of their physicochemical properties. A widely studied category of topological indices, known as degree-based topological indices, are calculated as the sum of the weights of a graph's edges, where each edge weight is determined by a formula that depends solely on the degrees of its endpoints.
  This work focuses exclusively on chemical graphs in which no vertex has a degree greater than 3, a model for conjugated systems. Within a polyhedral framework, each chemical graph is mapped to a point in a three-dimensional space, enabling extremal values of any degree-based topological index to be determined through linear optimization over the corresponding polyhedron. Analysis within this framework reveals that extremality is limited to a small subset of chemical graph families, implying that certain chemical graphs can never attain extremality for any degree-based topological index.
  The main objective of this paper is to present ChemicHull, an online tool we have developed to determine and display extremal chemical graphs for arbitrary degree-based topological indices. To illustrate the power of this tool, we easily recover established results, emphasizing its effectiveness for chemically significant graph classes such as chemical trees and unicyclic chemical graphs. This tool also enabled the identification of a counterexample to a previously published extremal result concerning the Randić index.

</details>


### [54] [Enumeration With Nice Roman Domination Properties](https://arxiv.org/abs/2511.20367)
*Kevin Mann*

Main category: cs.DM

TL;DR: 本文提出并推广了一种统一的多项式延迟枚举算法，使得多种罗马支配变体极小支配函数的高效枚举成为可能，覆盖了包括极大、连通、全体等多种类型，以及不同特殊图类，并分析了方法的不足之处。


<details>
  <summary>Details</summary>
Motivation: 罗马支配及其变体在图论和最优化等领域具有重要意义。然而，相关问题（如扩展完美罗马支配）判定问题为NP完全，但对其极小支配函数的枚举效率不高。作者希望发展一般化方法提升枚举效率，同时扩展适用范围到更多支配函数类型与图类。

Method: 作者提出一种基于极小完美罗马支配函数与罗马支配函数之间的双射，以及极小罗马支配函数的多项式延迟枚举方法，并将该思路推广到所谓的“良性罗马支配性质”，借此设计多项式延迟枚举算法。同时针对不同类别的支配函数与图类（如cobipartite图、区间图等）分析和讨论实现细节。

Result: 证明并实现了所有极小极大罗马支配函数在O(1.9332^n)时间内可多项式延迟枚举；在cobipartite图上，极小连通/全体罗马支配函数可多项式延迟枚举；在区间图上，极小连通罗马支配函数也可多项式延迟枚举。同时分析了该方法的不足。

Conclusion: 尽管扩展完美罗马支配问题判定很困难，但通过推广双射与枚举方法，可对若干重要变体实现高效极小支配函数的枚举。这为图论中的罗马支配相关问题的算法研究和应用提供了新思路，也揭示了方法的适用边界。

Abstract: Although Extension Perfect Roman Domination is NP-complete, all minimal (with respect to the pointwise order) perfect Roman dominating functions can be enumerated with polynomial delay. This algorithm uses a bijection between minimal perfect Roman dominating functions and Roman dominating functions and the fact that all minimal Roman dominating functions can be enumerated with polynomial delay. This bijection considers the set of vertices with value 2 under the functions. In this paper, we will generalize this idea by defining so called nice Roman Domination properties for which we can employ this method. With this idea, we can show that all minimal maximal Roman Dominating functions can be enumerated with polynomial delay in O(1.9332^n) time. Furthermore, we prove that enumerating all minimal connected/total Roman dominating functions on cobipartite graphs can be achieved with polynomial delay. Additionally, we show the existence of a polynomial-delay algorithm for enumerating all minimal connected Roman dominating function on interval graphs. We show some downsides to this method as well.

</details>


### [55] [3-colorable planar graphs have an intersection segment representation using 3 slopes](https://arxiv.org/abs/2511.20368)
*Daniel Gonçalves*

Main category: cs.DM

TL;DR: 作者通过借用与同胚三角形相关的方法，证明了任意3-可着色的平面图都可以被表示为只使用三种斜率线段的交集图，从而实现了Scheinerman的猜想。


<details>
  <summary>Details</summary>
Motivation: 此前已经证明了所有平面图是线段交集图；该文关注于将这种交集限制在仅三种斜率，回应3-可着色平面图相关的猜想。

Method: 作者采用了S. Felsner提出的用于处理与同胚三角形接触表示相关的平面图的方法。

Result: 证明了3-可着色平面图是仅用三种斜率线段的交集图。

Conclusion: 作者证明了3-可着色平面图可以只用三种斜率的线段作为交集图，从而证实了E.R. Scheinerman的猜想。

Abstract: In his PhD Thesis, E.R. Scheinerman conjectured that planar graphs are intersection graphs of line segments in the plane. This conjecture was proved with two different approaches by J. Chalopin and the author, and by the author, L. Isenmann, and C. Pennarun. In the case of 3-colorable planar graphs E.R. Scheinerman conjectured that it is possible to restrict the set of slopes used by the segments to only 3 slopes. Here we prove this conjecture by using an approach introduced by S. Felsner to deal with contact representations of planar graphs with homothetic triangles.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [56] [Softmax Transformers are Turing-Complete](https://arxiv.org/abs/2511.20038)
*Hongjian Jiang,Michael Hahn,Georg Zetzsche,Anthony Widjaja Lin*

Main category: cs.FL

TL;DR: 本文解决了软注意力链式思维transformer是否图灵完备的公开问题，证明其在一定扩展下可对任意语言实现图灵完备，理论与实验均得到验证。


<details>
  <summary>Details</summary>
Motivation: 目前已知硬注意力的链式思维(CoT) transformer是图灵完备的，但对于软注意力的CoT transformer是否也具备图灵完备性仍是一个未解问题。探索这一属性有助于理解transformer模型的表达能力和泛化能力。

Method: 作者通过将Counting RASP (C-RASP)扩展到链式思维(CoT)，并引入因果掩码和相对位置编码，理论上证明了具备长度泛化特性的软max CoT transformer的图灵完备性。此外，开展了针对需要复杂非线性算术推理的语言的实证训练，以验证理论。

Result: 证明了带因果掩码、适用于一元字母表（以及更一般的字母界定语言）的软max CoT transformer具有图灵完备性。对于任意语言，原始模型并不完备，但扩展相对位置编码后则成为图灵完备。实验验证了复杂算术推理任务下该理论的正确性。

Conclusion: 本研究首次证明了具备长度泛化能力的软max注意力CoT transformer在引入因果掩码及相对位置编码后对任意语言是图灵完备的，推进了对transformer表达能力的理论认知，并得到实证支持。

Abstract: Hard attention Chain-of-Thought (CoT) transformers are known to be Turing-complete. However, it is an open problem whether softmax attention Chain-of-Thought (CoT) transformers are Turing-complete. In this paper, we prove a stronger result that length-generalizable softmax CoT transformers are Turing-complete. More precisely, our Turing-completeness proof goes via the CoT extension of the Counting RASP (C-RASP), which correspond to softmax CoT transformers that admit length generalization. We prove Turing-completeness for CoT C-RASP with causal masking over a unary alphabet (more generally, for letter-bounded languages). While we show this is not Turing-complete for arbitrary languages, we prove that its extension with relative positional encoding is Turing-complete for arbitrary languages. We empirically validate our theory by training transformers for languages requiring complex (non-linear) arithmetic reasoning.

</details>
