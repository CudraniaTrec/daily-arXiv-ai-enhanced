<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 51]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants](https://arxiv.org/abs/2508.18587)
*Barış Bayazıt,Yao Li,Xujie Si*

Main category: cs.PL

TL;DR: 本文以两个成熟验证项目为案例，系统分析了大型语言模型（LLMs）在自动化生成证明方面的效果。发现上下文、依赖信息有助提高质量；LLMs善于生成小型证明，能应对复杂任务但表现依赖具体项目；同时LLMs生成证明既有创新简洁之处，也会犯奇怪错误。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自动化助手证明中的实际效果，尤其是验证任务上的应用能力和限制。

Method: 以hs-to-coq工具和Verdi两个成熟Rocq项目为案例，采用定量与定性分析评估LLMs自动生成证明的有效性。

Result: 实验结果显示，上下文信息、外部依赖显著提升证明生成，LLMs能智能生成高质量证明但仍会犯错。

Conclusion: 1. 外部依赖和同源文件中的上下文信息对自动生成证明非常有帮助；2. LLMs在生成小型证明方面表现优异，并能生成大型证明；3. LLMs在不同验证项目中的表现存在差异；4. LLMs能生成简洁而聪明的证明，应用经典技巧到新定义，但也可能出现奇特错误。

Abstract: Large language models (LLMs) can potentially help with verification using
proof assistants by automating proofs. However, it is unclear how effective
LLMs are in this task. In this paper, we perform a case study based on two
mature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the
effectiveness of LLMs in generating proofs by both quantitative and qualitative
analysis. Our study finds that: (1) external dependencies and context in the
same source file can significantly help proof generation; (2) LLMs perform
great on small proofs but can also generate large proofs; (3) LLMs perform
differently on different verification projects; and (4) LLMs can generate
concise and smart proofs, apply classical techniques to new definitions, but
can also make odd mistakes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: 本论文提出CTF-Dojo和CTF-Forge，大幅简化和扩展LLM可执行任务的训练环境，并证明执行反馈对提升模型能力至关重要，开源模型性能已接近业界顶尖水平。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在可执行运行环境中表现优异，特别是在软件工程任务中，但通用且可扩展的执行环境仍然稀缺，限制了更强大的机器学习代理的训练发展。

Method: 提出了CTF-Dojo，这是首个大规模可执行运行环境，内含658个可重现的CTF风格挑战，并容器化在Docker中。同时研发了CTF-Forge这一自动化流水线，可以将公开资源转换为可用的运行环境，无需人工干预，大幅提高扩展效率。

Result: 在CTF-Dojo环境下，仅使用486条高质量轨迹训练LLM代理，在三个权威基准测试中实现了最多11.6%的绝对增益。最佳的32B模型达到了31.9%的Pass@1，创造了新的开源权重的最优成绩，表现媲美最先进的前沿模型。

Conclusion: 通过CTF-Dojo，将CTF任务作为可执行代理学习的基准，验证了基于执行反馈的训练信号对于提升ML代理性能极为有效且关键，可以不依赖高昂的专有系统实现突破。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


### [3] [DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting](https://arxiv.org/abs/2508.18431)
*Kérian Fiter,Louis Malassigné-Onfroy,Bentley Oakes*

Main category: cs.SE

TL;DR: 本文提出DTInsight，实现对数字孪生系统的架构可视化、特性摘要生成及报告自动集成，有效支持系统演进中利益相关方的理解和决策。


<details>
  <summary>Details</summary>
Motivation: 数字孪生系统随着时间不断发展，利益相关方需要始终能理解其当前特征和架构，但缺乏持续化、自动化报告工具。

Method: 提出DTInsight工具，结合DT描述框架（DTDF），自动可视化DT架构、生成本体特性摘要，并将结果集成到CI/CD的报告页面中。

Result: DTInsight实现了DT架构的交互式可视化、特性自动摘要与报告集成，提升了报告自动化和理解效率。

Conclusion: DTInsight通过自动化工具和方法，持续生成数字孪生（DT）系统报告，提升了利益相关方对系统状态和特性理解能力。

Abstract: With Digital Twin (DT) construction and evolution occurring over time,
stakeholders require tools to understand the current characteristics and
conceptual architecture of the system at any time. We introduce DTInsight, a
systematic and automated tool and methodology for producing continuous
reporting for DTs. DTInsight offers three key features: (a) an interactive
conceptual architecture visualization of DTs; (b) generation of summaries of DT
characteristics based on ontological data; and (c) integration of these outputs
into a reporting page within a continuous integration and continuous deployment
(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT
Description Framework (DTDF), DTInsight enables up-to-date and detailed reports
for enhanced stakeholder understanding.

</details>


### [4] [Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling](https://arxiv.org/abs/2508.18452)
*Pierre-Emmanuel Goffi,Raphaël Tremblay,Bentley Oakes*

Main category: cs.SE

TL;DR: 本文报告了在啤酒发酵工业场景下工程互动型数字孪生的完整方法与成果，所建系统可持续采样并极大减少人工操作时间。提出了三阶段工程方法及多层安全措施，实现了高压实时双向控制，并总结了跨学科集成的关键经验，对安全关键型DT开发具参考价值。


<details>
  <summary>Details</summary>
Motivation: 目前工业环境下数字孪生(DT)系统通常仅用于被动监控，难以实现实时交互和控制，尤其是在安全关键型场景中，工程实现复杂。因此，需要探索可行系统性工程方法以推动工业DT从被动监控向双向交互与控制转变。

Method: 采用三阶段工程方法，将被动监控系统转化为支持实时控制的互动型二类数字孪生。具体包括多层安全协议设计、硬软一体化（Arduino与Unity可视化集成）、实时同步方案，以及基于星座报告框架促进跨领域协作。全过程记录工程挑战与应对实践。

Result: 通过所述方法成功开发适用于高压啤酒发酵环节的安全关键型互动数字孪生系统，实现持续采样，并将人工采样时间减少了91%。系统在多领域集成和安全设计方面效果良好，技术路线具备可复制性。

Conclusion: 实践证明，安全优先设计、仿真驱动开发与渐进式实施策略对于安全关键型双向控制数字孪生至关重要；给出对类似领域工程实践的具体方法指导。

Abstract: Successfully engineering interactive industrial DTs is a complex task,
especially when implementing services beyond passive monitoring. We present
here an experience report on engineering a safety-critical digital twin (DT)
for beer fermentation monitoring, which provides continual sampling and reduces
manual sampling time by 91%. We document our systematic methodology and
practical solutions for implementing bidirectional DTs in industrial
environments. This includes our three-phase engineering approach that
transforms a passive monitoring system into an interactive Type 2 DT with
real-time control capabilities for pressurized systems operating at seven bar.
We contribute details of multi-layered safety protocols, hardware-software
integration strategies across Arduino controllers and Unity visualization, and
real-time synchronization solutions. We document specific engineering
challenges and solutions spanning interdisciplinary integration, demonstrating
how our use of the constellation reporting framework facilitates cross-domain
collaboration. Key findings include the critical importance of safety-first
design, simulation-driven development, and progressive implementation
strategies. Our work thus provides actionable guidance for practitioners
developing DTs requiring bidirectional control in safety-critical applications.

</details>


### [5] [How do Humans and LLMs Process Confusing Code?](https://arxiv.org/abs/2508.18547)
*Youssef Abdelsalam,Norman Peitek,Anna-Maria Maurer,Mariya Toneva,Sven Apel*

Main category: cs.SE

TL;DR: 本研究发现，LLM 和人类程序员对代码困惑区域的反应高度一致，基于此，可以用 LLM 辅助发现可能令程序员困惑的代码区域。


<details>
  <summary>Details</summary>
Motivation: 人类与基于大语言模型的编程助手已在日常编程任务中协作，探究 LLM 和人类之间代码理解困惑的一致性，有助于改进 LLM 并优化软件开发流程。

Method: 通过对比 LLM 的 perplexity（困惑度）与人类程序员的神经生理反应（如基于 EEG 的注视相关电位），实证性分析他们在阅读清晰和晦涩代码时的表现。

Result: LLM 的 perplexity 与人类困惑时的神经生理信号在位置和幅值上高度相关，即他们在代码的同一部分感到困惑。由此设计了一种基于 LLM、数据驱动的方法，用于发现令程序员困惑的代码区域。

Conclusion: LLMs 和人类程序员在理解代码时，对于哪些部分产生困惑高度一致，可以利用 LLM 辅助检测令程序员困惑的代码区域。

Abstract: Already today, humans and programming assistants based on large language
models (LLMs) collaborate in everyday programming tasks. Clearly, a
misalignment between how LLMs and programmers comprehend code can lead to
misunderstandings, inefficiencies, low code quality, and bugs.
  A key question in this space is whether humans and LLMs are confused by the
same kind of code. This would not only guide our choices of integrating LLMs in
software engineering workflows, but also inform about possible improvements of
LLMs.
  To this end, we conducted an empirical study comparing an LLM to human
programmers comprehending clean and confusing code. We operationalized
comprehension for the LLM by using LLM perplexity, and for human programmers
using neurophysiological responses (in particular, EEG-based fixation-related
potentials).
  We found that LLM perplexity spikes correlate both in terms of location and
amplitude with human neurophysiological responses that indicate confusion. This
result suggests that LLMs and humans are similarly confused about the code.
Based on these findings, we devised a data-driven, LLM-based approach to
identify regions of confusion in code that elicit confusion in human
programmers.

</details>


### [6] [LaQual: A Novel Framework for Automated Evaluation of LLM App Quality](https://arxiv.org/abs/2508.18636)
*Yan Wang,Xinyi Hou,Yanjie Zhao,Weiguo Lin,Haoyu Wang,Junjun Si*

Main category: cs.SE

TL;DR: 本文针对LLM应用商店评测不足的问题，提出了自动化评估框架LaQual，显著提升了应用筛选准确性和用户体验，实验和用户研究均证明其有效性与先进性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM（大语言模型）应用商店仅依赖用户活跃度、收藏等静态指标进行应用排名和推荐，导致用户难以高效找到高质量应用，这对用户体验和应用商店生态都构成挑战。

Method: 提出了LaQual自动化框架，包括三个阶段：（1）分层标注与分类LLM应用，以匹配不同场景；（2）利用时序加权用户参与度和功能指标等静态信息筛选低质应用；（3）利用LLM自身生成针对不同场景的评价指标和任务，进行动态自适应的高质量评估。

Result: 在主流LLM应用商店实验发现，LaQual自动评分结果与人工判断高度一致（在法律咨询和旅行规划场景下，斯皮尔曼相关系数约为0.6，统计显著）；可筛除66.7%-81.3%的候选低质应用。用户研究也证实LaQual在决策信心、效率和评价报告价值感知等方面大幅优于基线系统。

Conclusion: LaQual为实际场景下LLM应用的发现和推荐提供了可扩展、客观且以用户为中心的高质量应用筛选与评估解决方案。

Abstract: LLM app stores are quickly emerging as platforms that gather a wide range of
intelligent applications based on LLMs, giving users many choices for content
creation, coding support, education, and more. However, the current methods for
ranking and recommending apps in these stores mostly rely on static metrics
like user activity and favorites, which makes it hard for users to efficiently
find high-quality apps. To address these challenges, we propose LaQual, an
automated framework for evaluating the quality of LLM apps. LaQual consists of
three main stages: first, it labels and classifies LLM apps in a hierarchical
way to accurately match them to different scenarios; second, it uses static
indicators, such as time-weighted user engagement and functional capability
metrics, to filter out low-quality apps; and third, it conducts a dynamic,
scenario-adaptive evaluation, where the LLM itself generates scenario-specific
evaluation metrics, scoring rules, and tasks for a thorough quality assessment.
Experiments on a popular LLM app store show that LaQual is effective. Its
automated scores are highly consistent with human judgments (with Spearman's
rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in
travel planning). By effectively screening, LaQual can reduce the pool of
candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual
significantly outperforms baseline systems in decision confidence, comparison
efficiency (with average scores of 5.45 compared to 3.30), and the perceived
value of its evaluation reports (4.75 versus 2.25). Overall, these results
demonstrate that LaQual offers a scalable, objective, and user-centered
solution for finding and recommending high-quality LLM apps in real-world use
cases.

</details>


### [7] [Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision](https://arxiv.org/abs/2508.18675)
*Xu Lu,Weisong Sun,Yiran Zhang,Ming Hu,Cong Tian,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出了基于多智能体和形式化需求开发的代码自动生成框架ReDeFo，通过引入正式规范和智能体协作，显著提升自动生成代码的可靠性和质量，为软件自动生成迈向实用化提供新思路。


<details>
  <summary>Details</summary>
Motivation: 自动化代码生成一直是软件工程领域的重要目标，而当前基于大型语言模型（LLM）的生成方法在生成代码的质量和对实际需求的满足方面仍存在不足。缺乏系统化的需求开发与建模策略是主要瓶颈之一。

Method: 该论文提出了一个多智能体框架（ReDeFo），以实现更可靠的代码自动生成。该框架引入了三种智能体，结合形式化方法的知识与技术，将需求开发、需求形式化与代码生成流程紧密结合。核心思想是用形式化规范将自然语言需求转化为准确的可执行代码。

Result: ReDeFo框架能够实现更严格的需求到代码的转换，通过形式化规范进行正确性推理、发现潜在的隐藏bug，并保证关键性质在开发全流程中的贯彻，显著提升自动生成代码的质量与可靠性。

Conclusion: 该工作推动了自动化代码生成领域的进步，通过多智能体+形式化需求方法增强代码质量和保障生成代码的可靠性，为自动生成可靠软件的长期目标迈出重要一步。

Abstract: Automated code generation has long been considered the holy grail of software
engineering. The emergence of Large Language Models (LLMs) has catalyzed a
revolutionary breakthrough in this area. However, existing methods that only
rely on LLMs remain inadequate in the quality of generated code, offering no
guarantees of satisfying practical requirements. They lack a systematic
strategy for requirements development and modeling. Recently, LLM-based agents
typically possess powerful abilities and play an essential role in facilitating
the alignment of LLM outputs with user requirements. In this paper, we envision
the first multi-agent framework for reliable code generation based on
\textsc{re}quirements \textsc{de}velopment and \textsc{fo}rmalization, named
\textsc{ReDeFo}. This framework incorporates three agents, highlighting their
augmentation with knowledge and techniques of formal methods, into the
requirements-to-code generation pipeline to strengthen quality assurance. The
core of \textsc{ReDeFo} is the use of formal specifications to bridge the gap
between potentially ambiguous natural language requirements and precise
executable code. \textsc{ReDeFo} enables rigorous reasoning about correctness,
uncovering hidden bugs, and enforcing critical properties throughout the
development process. In general, our framework aims to take a promising step
toward realizing the long-standing vision of reliable, auto-generated software.

</details>


### [8] [LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging](https://arxiv.org/abs/2508.18721)
*Yunrui Pei,Hongshu Wang,Wenjie Zhang,Yun Lin,Weiyu Kong,Jin song Dong*

Main category: cs.SE

TL;DR: RecovSlicing利用大模型推断部分记录程序轨迹下的变量动态定义，无需传统繁重插桩和复刻执行；实际准确率和召回率显著高于同类方法，并能发现更多程序回归缺陷，提高调试效率。


<details>
  <summary>Details</summary>
Motivation: 程序调试过程中，理解变量为何具有某个值（即动态数据依赖）很关键，然而在实际追踪变量的动态定义时，传统方法不仅需要大量插桩或重复执行（尤其当变量定义在库里或程序非确定性时更为昂贵或不可行），严重影响效率和可操作性。

Method: 提出了RecovSlicing方法，在仅部分插桩的单次运行基础上，通过LLM（大模型）对代码及部分执行轨迹进行推断，恢复缺失的执行信息，实现动态数据依赖分析。该方法支持如list.get(i)等隐式变量，关键技术点包括：（1）恢复运行时值与结构，（2）将恢复出的变量与记录的内存对齐，实现定义分析。

Result: 在三个切片基准测试、8300个数据依赖任务上，RecovSlicing在准确率（80.3%，91.1%，98.3%）和召回率（91.1%，91.1%，98.3%）均大幅超越最佳基线方法（准确率分别为39.0%，82.0%，59.9%；召回率分别为53.4%，79.1%，87.1%），且整合进回归缺陷定位工具后可多发现16%的回归缺陷。

Conclusion: RecovSlicing无需全面插桩或重复执行，仅借助大模型和代码上下文就能高效、高准确率推断变量定义，有效提升了程序调试与缺陷定位效率。

Abstract: Dynamic data dependency, answering "why a variable has this value?", is
critical for debugging. Given a program step `s` reading a variable `v`,
finding the dynamic definition of `v` is challenging. Traditional methods
require either (1) exhaustive instrumentation of all possible definitions of
`v` in one run or (2) replicating the run to re-examine reads/writes - both
costly. If `v` is defined in a library, instrumentation becomes expensive; for
non-deterministic programs, replication is infeasible.
  We propose RecovSlicing, which computes dynamic data dependency in a single
run with partial instrumentation. We leverage LLMs to infer program behavior
from a partially recorded trace and code context. Given a trace and a slicing
criterion (step `s` and variable `v`), RecovSlicing estimates the runtime
definition of `v` by recovering the missing execution.It also supports implicit
variables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:
(1) recovering runtime values and structures, and (2) aligning recovered
variables with recorded memory to analyze definitions.
  We evaluate RecovSlicing on 8,300 data dependencies across three slicing
benchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution
Slicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,
outperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall
(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug
localizer, it enables finding 16% more regressions.

</details>


### [9] [Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions](https://arxiv.org/abs/2508.18771)
*Kexin Sun,Hongyu Kuang,Sebastian Baltes,Xin Zhou,He Zhang,Xiaoxing Ma,Guoping Rong,Dong Shao,Christoph Treude*

Main category: cs.SE

TL;DR: 本研究大规模分析了AI代码审查工具在GitHub上的使用和实际影响，发现有效的评论有助于代码改进，并为未来工具优化提供参考建议。


<details>
  <summary>Details</summary>
Motivation: AI代码审查工具在自动化审查和提升代码质量方面日益流行，但其实际作用尚未被充分认识。本文旨在揭示这些工具对代码质量和开发流程的真正影响。

Method: 作者选取了GitHub上的16种主流AI代码审查工具，对178个仓库中的超过22,000条审查评论进行了大规模实证分析。研究采用了两阶段的LLM辅助框架来判断评论是否被采纳，并结合可解释机器学习方法识别有效性的影响因素。

Result: 研究发现，虽然这些工具的采用率在不断增长，但其有效性差异较大。简明扼要、包含代码片段、手动触发的评论，以及针对代码块的审查工具评论，更容易促成实际的代码更改。

Conclusion: AI代码审查工具需注重工具与评论设计，其有效性受多种因素影响。针对工具设计的深入优化，可进一步提升AI代码审查系统的质量和作用。

Abstract: AI-based code review tools automatically review and comment on pull requests
to improve code quality. Despite their growing presence, little is known about
their actual impact. We present a large-scale empirical study of 16 popular
AI-based code review actions for GitHub workflows, analyzing more than 22,000
review comments in 178 repositories. We investigate (1) how these tools are
adopted and configured, (2) whether their comments lead to code changes, and
(3) which factors influence their effectiveness. We develop a two-stage
LLM-assisted framework to determine whether review comments are addressed, and
use interpretable machine learning to identify influencing factors. Our
findings show that, while adoption is growing, effectiveness varies widely.
Comments that are concise, contain code snippets, and are manually triggered,
particularly those from hunk-level review tools, are more likely to result in
code changes. These results highlight the importance of careful tool design and
suggest directions for improving AI-based code review systems.

</details>


### [10] [Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study](https://arxiv.org/abs/2508.18816)
*Sabato Nocera,Davide Fucci,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 本文系统分析了GitHub项目使用和定制SonarQube Cloud的现状，发现既有大量依赖默认配置，也有大量针对具体需求进行定制，未来可将质量门配置与软件成果关联，为相关工具配置提供建议。


<details>
  <summary>Details</summary>
Motivation: 尽管静态代码分析（SCA）工具被广泛用于提升代码质量，但目前关于开源项目实际如何使用和定制这些工具的研究较少，特别是针对SonarQube Cloud的定制和使用情况。

Method: 本文采用挖掘式研究方法，分析通过GitHub Actions关联到SonarQube Cloud的GitHub项目，考察这些项目是否正确连接、质量门的配置和定制情况。

Result: 在321个使用SonarQube Cloud的GitHub项目中，81%正确连接，其他存在配置错误或访问受限。在265个可访问的项目中，75%使用组织默认的质量门，55%采用内建质量门，45%则根据自身需求进行了定制。定制内容多聚焦在安全、可维护性、可靠性、覆盖率等方面，符合SonarQube Cloud的“Clean as You Code”原则。

Conclusion: 许多项目依赖预设配置，但相当一部分会根据自身质量目标进行定制。未来应进一步研究质量门配置与软件实际成果的关联，为不同语境下SCA工具配置提供循证建议。

Abstract: Background: Static Code Analysis (SCA) tools are widely adopted to enforce
code quality standards. However, little is known about how open-source projects
use and customize these tools. Aims: This paper investigates how GitHub
projects use and customize a popular SCA tool, namely SonarQube Cloud. Method:
We conducted a mining study of GitHub projects that are linked through GitHub
Actions to SonarQube Cloud projects. Results: Among 321 GitHub projects using
SonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud
projects, while others exhibit misconfigurations or restricted access. Among
265 accessible SonarQube Cloud projects, 75% use the organization's default
quality gate, i.e., a set of conditions that deployed source code must meet to
pass automated checks. While 55% of the projects use the built-in quality gate
provided by SonarQube Cloud, 45% of them customize their quality gate with
different conditions. Overall, the most common quality conditions align with
SonarQube Cloud's "Clean as You Code" principle and enforce security,
maintainability, reliability, coverage, and a few duplicates on newly added or
modified source code. Conclusions: Many projects rely on predefined
configurations, yet a significant portion customize their configurations to
meet specific quality goals. Building on our initial results, we envision a
future research agenda linking quality gate configurations to actual software
outcomes (e.g., improvement of software security). This would enable
evidence-based recommendations for configuring SCA tools like SonarQube Cloud
in various contexts.

</details>


### [11] [Interleaving Large Language Models for Compiler Testing](https://arxiv.org/abs/2508.18955)
*Yunbo Ni,Shaohua Li*

Main category: cs.SE

TL;DR: 本论文提出将LLM生成的功能丰富的小代码片段，分离到离线生成并在线灵活组合的新测试框架LegoFuzz，有效发现了主流C编译器中的大量高价值bug，并大幅提升了效率，为AI辅助软件测试开辟新思路。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLMs）的编译器测试方法面临两个挑战：生成的测试程序通常过于简单且广泛、深入的测试消耗大量算力。

Method: 提出一种新颖的编译器测试框架，将测试过程分为离线与在线两个阶段。离线阶段通过LLMs生成大量小而复杂的代码片段；在线阶段则通过策略性组合这些片段生成高质量的测试程序，对编译器进行测试。此外将此框架实现为名为LegoFuzz的工具，用于测试C编译器。

Result: LegoFuzz在测试GCC和LLVM时发现了66个bug，其中近一半为严重且难以发现的错误编译（miscompilation）bug。这些是现有LLMs工具未能检测到的问题。

Conclusion: 这种高效的设计极大提升了AI模型在软件测试，尤其是C编译器测试中的实际应用价值，并有潜力推广至更广泛的软件测试领域。

Abstract: Testing compilers with AI models, especially large language models (LLMs),
has shown great promise. However, current approaches struggle with two key
problems: The generated programs for testing compilers are often too simple,
and extensive testing with the LLMs is computationally expensive. In this
paper, we propose a novel compiler testing framework that decouples the testing
process into two distinct phases: an offline phase and an online phase. In the
offline phase, we use LLMs to generate a collection of small but feature-rich
code pieces. In the online phase, we reuse these code pieces by strategically
combining them to build high-quality and valid test programs, which are then
used to test compilers.
  We implement this idea in a tool, LegoFuzz, for testing C compilers. The
results are striking: we found 66 bugs in GCC and LLVM, the most widely used C
compilers. Almost half of the bugs are miscompilation bugs, which are serious
and hard-to-find bugs that none of the existing LLM-based tools could find. We
believe this efficient design opens up new possibilities for using AI models in
software testing beyond just C compilers.

</details>


### [12] [GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging](https://arxiv.org/abs/2508.18993)
*Ziyi Ni,Huacan Wang,Shuo Zhang,Shuo Lu,Ziyang He,Wang You,Zhenheng Tang,Yuntao Du,Bill Sun,Hongzhang Liu,Sen Hu,Ronghao Chen,Bo Li,Xin Li,Chen Hu,Binxing Jiao,Daxin Jiang,Pin Lyu*

Main category: cs.SE

TL;DR: 该工作提出GitTaskBench基准，系统评估代码智能体在真实仓库任务中的表现，并用新经济效益指标量化性能。当前模型表现仍有限，关键难点在环境与依赖管理。基准开放，促进代码AI真实应用。


<details>
  <summary>Details</summary>
Motivation: 现有代码代理的基准测试很少考虑真实的软件开发场景中对大型代码库（如GitHub）的实际任务利用，这与实际开发需求存在差距。作者希望提出一种新的评测方法，能更贴合工作流驱动的实际任务。

Method: 作者提出了GitTaskBench，一个包含54个现实任务、覆盖7种模态和7个领域的基准，任务结合有代表性的代码仓库和自动化、人工审核的评估工具，明确了实用的成功标准。同时引入了alpha-value指标，用以量化代理性能的经济效益（综合任务成功率、模型调用成本及开发者薪资平均值）。并用三个最前沿代理框架与多种LLM进行对比实验。

Result: 实验结果显示，目前先进系统在利用仓库解决复杂任务方面仍存在较大难度，如OpenHands+Claude 3.7仅解决了48.15%的任务。超过半数的失败归因于环境搭建和依赖解析等看似简单却至关重要的步骤，暴露了工作流管理和超时应对方面的不足。

Conclusion: GitTaskBench能引导社区关注和提升代码仓库感知的自动推理、执行和部署能力，推动代码智能体真实场景下的复杂任务解决。基准与代码已开源，助力后续研究。

Abstract: Beyond scratch coding, exploiting large-scale code repositories (e.g.,
GitHub) for practical tasks is vital in real-world software development, yet
current benchmarks rarely evaluate code agents in such authentic,
workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a
benchmark designed to systematically assess this capability via 54 realistic
tasks across 7 modalities and 7 domains. Each task pairs a relevant repository
with an automated, human-curated evaluation harness specifying practical
success criteria. Beyond measuring execution and task success, we also propose
the alpha-value metric to quantify the economic benefit of agent performance,
which integrates task success rates, token cost, and average developer
salaries. Experiments across three state-of-the-art agent frameworks with
multiple advanced LLMs show that leveraging code repositories for complex task
solving remains challenging: even the best-performing system, OpenHands+Claude
3.7, solves only 48.15% of tasks. Error analysis attributes over half of
failures to seemingly mundane yet critical steps like environment setup and
dependency resolution, highlighting the need for more robust workflow
management and increased timeout preparedness. By releasing GitTaskBench, we
aim to drive progress and attention toward repository-aware code reasoning,
execution, and deployment -- moving agents closer to solving complex,
end-to-end real-world tasks. The benchmark and code are open-sourced at
https://github.com/QuantaAlpha/GitTaskBench.

</details>


### [13] [A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs](https://arxiv.org/abs/2508.19056)
*S. Panda,D. Munjal,D. P. Mohapatra*

Main category: cs.SE

TL;DR: 本文提出一种基于静态分析的测试用例优先级排序方法，通过分析对象程序中受影响组件的耦合度，优先执行可能更易出错部分的测试用例，实验证明该方法能更早发现错误，效果优于部分现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的测试用例优先级排序方法多依赖于动态信息或历史数据，缺乏有效静态分析手段，且提前发现程序错误可节省回归测试时间和成本，因此需要更高效且可行的静态优先级排序方法。

Method: 采用静态分析方法，通过构建受影响片段图（ASG），并计算其节点的受影响组件耦合度（ACC），据此为测试用例分配优先级。

Result: 实证分析显示，通过优先执行覆盖高ACC节点的测试用例，可更早发现程序中的错误。针对七个案例的结果表明，该方法在性能和可行性上均具备优势。

Conclusion: 本文方法在实际案例中表现良好，对于揭示程序早期错误表现出较高的性能，且在性能上可与现有方法相媲美。

Abstract: Test case prioritization focuses on finding a suitable order of execution of
the test cases in a test suite to meet some performance goals like detecting
faults early. It is likely that some test cases execute the program parts that
are more prone to errors and will detect more errors if executed early during
the testing process. Finding an optimal order of execution for the selected
regression test cases saves time and cost of retesting. This paper presents a
static approach to prioritizing the test cases by computing the affected
component coupling (ACC) of the affected parts of object-oriented programs. We
construct a graph named affected slice graph (ASG) to represent these affected
program parts.We determine the fault-proneness of the nodes of ASG by computing
their respective ACC values. We assign higher priority to those test cases that
cover the nodes with higher ACC values. Our analysis with mutation faults shows
that the test cases executing the fault-prone program parts have a higher
chance to reveal faults earlier than other test cases in the test suite. The
result obtained from seven case studies justifies that our approach is feasible
and gives acceptable performance in comparison to some existing techniques.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [14] [Decidability of Extensions of Presburger Arithmetic by Hardy Field Functions](https://arxiv.org/abs/2508.19206)
*Hera Brown,Jakub Konieczny*

Main category: cs.LO

TL;DR: 只要用成长速度不低于某些多项式的Hardy场函数扩展Presburger算术，大多数情况下，这类理论都是不可判定的。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过引入Hardy场函数扩展Presburger算术，特别关注这些扩展后的理论在可判性方面的表现。

Method: 分析$	ext{Th}(	ext{Z}; <, +, floor f ceil)$理论，其中$f$是Hardy场函数，使用最接近整数的运算符，对不同增长速度的Hardy场函数进行可判性的理论证明。

Result: 当$f$的增长速度多项式快于$x$时，该理论不可判定。当$f$增长速度次线性但依然达到某种多项式速度时，理论同样不可判定。

Conclusion: 大多数由Hardy场函数扩展的Presburger算术理论是不可判定的，尤其是在$f$增长速度达到多项式级别时。

Abstract: We study the extension of Presburger arithmetic by the class of
sub-polynomial Hardy field functions, and show the majority of these extensions
to be undecidable. More precisely, we show that the theory
$\mathrm{Th}(\mathbb{Z}; <, +, \lfloor f \rceil)$, where $f$ is a Hardy field
function and $\lfloor \cdot \rceil$ the nearest integer operator, is
undecidable when $f$ grows polynomially faster than $x$. Further, we show that
when $f$ grows sub-linearly quickly, but still as fast as some polynomial, the
theory $\mathrm{Th}(\mathbb{Z}; <, +, \lfloor f \rceil)$ is undecidable.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 本文提出基于复杂值张量和语义吸引子的AGI理论框架，突破现有概率语言模型的局限，强调语义应通过递归收敛而生成，为实现具有主动表达和理解能力的通用人工智能提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 当前主流的基于transformer模型的语言系统，侧重统计概率的下一个词预测，仍缺乏对复杂语义（如讽刺、多义、模糊等）的本质建模。论文旨在突破现有模型的局限，提出更具表达性和稳定性的语义生成方法。

Method: 引入复杂值语义空间中的'语义吸引子'(semantic attractor)理论，通过递归的张量变换、虚数单位i的循环操作，构建旋转语义结构。该结构通过梯度流、张量变形和矩阵动态，实现对语义的非概率化、内在导向的收敛建模。

Result: 提出了一种以语义吸引子为核心的新型理论模型，可有效描述讽刺、多义性和歧义等语言现象，为语义的稳定性、清晰度及表达深度提供了数学和哲学基础。

Conclusion: 语义的真正生成源自递归性地趋向于语义一致性，而非简单模拟或概率预测，实现这一点需设计全新的认知架构，让模型主动塑造语言而非仅预测。

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [16] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: 本文提出KAIROS基准，系统研究LLMs在多智能体社会动态下的决策、信任和鲁棒性，发现GRPO方法虽提升表现但影响抗社会影响能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地被用于多智能体系统（MAS）中，作为协作智能的组成部分。但过往研究主要关注从众偏见，较少系统性探索信任建立、抗假信息能力以及群体互动下的社会动态关键因素。

Method: 提出KAIROS基准测试，模拟包含不同可靠性个体（专家、新手、敌对者、噪声）的答题竞赛环境。通过操纵历史互动与当前同伴反馈，有系统地研究信任、自信和同伴行为对LLM决策的影响。并评估了三种缓解策略：提示工程（prompting）、有监督微调、强化学习（GRPO）等。

Result: GRPO结合多智能体上下文、基于结果的奖励和无约束推理时，整体性能最佳，但相较于基础模型，对社会影响的鲁棒性降低。

Conclusion: LLMs在多智能体协作环境下的信任形成、信息整合及决策机制复杂。GRPO可改善整体表现，但需权衡其对社会影响的敏感性。相关代码和数据集已公开。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [17] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 本论文构建了全球首个聚焦非拉丁文字多语种网站的大型数据集LangCrUX，并发现当前网页无障碍提示普遍不准确，影响屏幕阅读器使用。为此，作者提出了Kizuki扩展工具，旨在提高多语种网页的无障碍访问体验。


<details>
  <summary>Details</summary>
Motivation: 尽管网络上逐渐支持多语种内容，但对于视觉障碍者来说，现有辅助技术如屏幕阅读器对非拉丁文字支持不足，影响其获取信息。当前缺乏针对多语种网页内容的大规模数据集，阻碍对相关无障碍问题的研究。

Method: 作者构建了一个名为LangCrUX的大型数据集，涵盖120,000个主要采用非拉丁文字的热门网站，涉及12种语言，并利用该数据集系统分析多语种网页的无障碍状况。

Result: 数据分析发现，网页普遍忽视无障碍提示，且这些提示通常不能反映页面可见内容的语言多样性，导致屏幕阅读器效果受限，降低无障碍性。

Conclusion: 多语种网站普遍存在无障碍提示缺失或不准确问题，严重影响视觉障碍用户的访问体验。针对这一问题，作者提出了Kizuki，一种语言感知的自动无障碍测试扩展，以提升多语种内容的辅助访问效果。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [18] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 论文发现LVLMs多语言理解和浅层神经元激活密切相关，提出PLAST方法通过精确浅层微调显著提升多语言能力且仅需14%的参数，并在低资源与复杂任务中展现优异性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在视觉信息处理和自然语言理解方面表现卓越，但在多语言能力上仍存在明显不平衡。本文旨在探索LVLMs多语言处理的工作机制，并提升其多语言表现。

Method: 提出了一种名为PLAST的训练方法，通过精确识别和微调与多语言理解相关的特定浅层神经元，实现高效的多语言能力增强。具体做法包括监测神经元激活以定位关键层，随后利用问题翻译对进行多语言对齐微调。

Result: 在MM-Bench和MMMB多语言基准上，PLAST显著提升了LVLMs的多语言处理能力，仅需微调14%的参数即可获得有效提升。此外，PLAST在低资源和复杂视觉推理任务上表现良好，证明其泛化性强。

Conclusion: PLAST方法能够高效提升大型视觉语言模型的多语言能力，且参数开销低，具有良好的适用性和扩展性，尤其适用于浅层信息的语言特定处理。

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [19] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 本文提出一种名为backprompting的新方法，通过生成仿真真实LLM输出并配合人工聚类进行标注，用于训练更鲁棒的健康建议检测器。实验证明，该检测器在准确率上优于参数远多于自身的GPT-4o等方案，为LLM安全落地提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 解决企业环境下LLM输入/输出文本守护检测器难以获得高质量、真实标注数据的问题，尤其是在健康建议检测这样难度高且细微的场景中，现有数据采集和模型效果均存在较大挑战。

Method: 提出了一种称为backprompting的数据生成与标注方法，用于为大型语言模型的健康建议守护检测器生成更接近真实产出的训练样本，并采用稀疏人工参与的聚类方式标注这些样本，最后用于训练检测器并与现有方法进行对比。

Result: 通过将新方法与现有主流方法（如GPT-4o）对比，提出的检测器提升了高达3.73%的检测准确率，且只需1/400的参数量，显示出明显的优势。

Conclusion: 提出的backprompting方法结合稀疏人工参与聚类，实现了对生成数据的标注，并由此开发出的检测器在健康建议检测任务中表现优于现有方法，尤其是在参数远小于GPT-4o的情况下，性能提升明显。

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [20] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 本文提出Integral Transformer机制，能有效去除自注意力中的噪声同时保留对特殊token的作用。实验结果显示其在多项语言任务上优于现有方法，并对不同层注意力机制的搭配给出新见解。


<details>
  <summary>Details</summary>
Motivation: 现有的Softmax自注意力容易对无语义信息的token（如特殊符号和标点）分配过多权重，导致注意力噪声。以往引入负注意力分数的方法则有舍弃有用信息的风险，需找到更好的去噪方案。

Method: 提出了一种新的自注意力机制Integral Transformer，通过对logit分布采样信号进行积分来去噪，同时保留对特殊token的有效贡献。还分析了不同层使用不同注意力机制的效果。

Result: 在多项知识与推理基准数据集上，Integral Transformer的表现优于vanilla、Cog与Differential等注意力机制。同时分析表明底层Transformer使用vanilla注意力更佳，高层Integral Transformer能有效平衡分布并减少秩坍塌。

Conclusion: Integral Transformer能够更好地平衡注意力分布并减少高层的秩坍塌，比现有多种注意力机制效果更好。

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [21] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: 本文提出Latent Self-Consistency（LSC）算法，通过学习型Token嵌入选择语义一致答案，在各种基准测试中超越现有方法，并保持高效、易部署和良好的输出置信度评估。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理复杂或长文本问题时，基于概率解码会产生不一致的输出。虽然Self-Consistency等方法能缓解短文本问题，但在长文本或所有格式下准确性和一致性均存在缺陷。

Method: 提出了Latent Self-Consistency（LSC）方法，利用可学习的Token嵌入，挑选语义最一致的答案。同时，仅需通过轻量级的摘要Token生成，保持推理效率，无需修改模型结构。

Result: 在6个短文本和5个长文本推理基准（如MATH、MMLU和TruthfulQA）上，LSC在所有任务平均表现优于SC、USC和WUCS，且计算开销极低。此外，LSC能提供校准良好的置信度估计，使预期校准误差在各种答案格式下都较低。

Conclusion: LSC是一个实用且高效的答案一致性选择方法，无论面对短文本还是长文本问题都表现稳定可靠，还能输出可靠的置信度评估。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [22] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 论文发现分布外（OOD）评估难以准确反映模型泛化与现实中真正的失败模式，建议采用更细致的方法提升泛化评估的健壮性。


<details>
  <summary>Details</summary>
Motivation: 质疑目前AI模型泛化能力评估的一项核心假设，即分布外数据集能够反映模型在真实部署中的可能失效，并希望提出更健全的评估方法。

Method: 将现有分布外评估结果与问答系统中的具体失败模式（尤其是模型对虚假特征或预测捷径的依赖）进行对比分析。

Result: 发现不同的OOD数据集评估模型的鲁棒性质量悬殊，有些甚至比简单的分布内评估更差。这部分是因为虚假捷径在分布内和分布外的数据集中可能共享，且训练和评估数据集质量可能脱钩。

Conclusion: 常用的基于OOD（分布外）数据集的泛化能力评估方法在实际应用中存在局限性，未必能准确反映模型的真实失效模式。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [23] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 本文探讨大语言模型用于内容重排序时的可解释性与语义理解，发现训练方法的选择对模型的透明度和推理能力影响显著，部分方法能提升模型解释排序原因的能力，但整体可靠性和透明度仍需加强。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）语义理解能力的提升，其对人类价值观的对齐度增强，但模型决策过程透明度下降，用户难以理解其重新排序（re-ranking）背后的原因。特别是在用户数据匮乏的新系统中，内容准确排序变得更加困难。

Method: 本文分析了不同训练方法对LLMs在重新排序任务中的语义理解影响，并检验这些模型能否通过文本推理增强透明度和可解释性。作者使用来自环境与地球科学领域的较小排序数据集进行实验，重点考察模型解释排序原因的能力。

Result: 部分训练方法在可解释性方面表现优异，显示出模型并非都学到了准确的语义理解，而是通过抽象知识优化排序评价。这说明LLMs的可靠性存在疑问，其透明度与训练方式密切相关。

Conclusion: LLMs在重新排序任务中的语义理解和可解释性受训练方法显著影响，有些方法能提升模型推理和解释能力，但并不能全部实现真正的语义理解，可靠性仍有提升空间。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [24] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 本文针对波兰语大模型中的性别偏见问题，利用性别包容性校对和指令进行微调，显著提升了模型生成文本的性别平等性。


<details>
  <summary>Details</summary>
Motivation: 波兰语的语法性别体系因历史和政治原因导致男性形式主导，造成社会性别不公。这种偏见会被基于大规模文本训练的语言模型继承和放大，亟需解决。

Method: 利用IPIS数据集——包含性别包容性校对和波兰语到英语翻译指令，对多语言和波兰语专用大模型进行微调；结合理论语言学框架，设计系统性提示，明确定义性别包容性规范。

Result: 通过IPIS微调，所选模型（Llama-8B、Mistral-7B、Mistral-Nemo、Bielik和PLLuM）在生成波兰语文本时表现出明显改善的性别包容性，能够有效减少性别偏见。

Conclusion: 本文提出的IPIS微调方法有效地降低了波兰语大型语言模型中的性别偏见，实现了更具性别包容性的语言生成。

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [25] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 本文将大模型幻觉检测建模为假设检验问题，并提出了基于多重检验的新方法，实验证明该法优于现有方法，能更有效检测幻觉输出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在处理多种任务中表现突出，但它们常常出现幻觉现象，即生成自信但实为错误或无意义的内容。鉴于幻觉问题严重影响LLM的实际应用，亟需有效的检测方法。

Method: 本文将幻觉检测问题形式化为假设检验问题，并类比于机器学习中的分布外检测。进一步，作者提出了一种基于多重检验的新方法，以提升幻觉检测的效果。

Result: 通过丰富的实验证明，所提出的方法在鲁棒性上优于现有的主流技术。

Conclusion: 本文提出的幻觉检测方法有效提升了检测精度和鲁棒性，有助于实际应用中更好地识别LLM的幻觉输出。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [26] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文针对机器翻译自动评估指标只参考单一译文的问题，提出了综合多译文信息的新方法，显著提升评价一致性，并公开了相关新模型。


<details>
  <summary>Details</summary>
Motivation: 目前自动化机器翻译评估指标通常只考虑源句和单一译文，与人工评估时参考多个译文的做法不同，这种评估设置可能限制自动化指标的表现。

Method: 提出了两种新型自动化评估指标：COMET-polycand，通过对比同一源句的多个备选译文，提升评估质量；COMET-polyic，借鉴检索式in-context learning，参考与当前源句类似的其它源句翻译及其人工质量得分进行评估。

Result: 在COMET-polycand中，仅加入一个额外译文，分段级评价性能即从Kendall's tau-b相关性0.079提升到0.118，添加更多译文后效果进一步增强。COMET-polyic采用检索示例同样带来改进（相关性从0.079提升到0.116）。

Conclusion: 为机器翻译自动评估的指标注入多译文或多例信息后，显著提升了其与人工评价的一致性。所提出模型已公开发布。

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [27] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 提出了用于隐喻对齐的自评估视觉隐喻生成框架，通过结构化提示和带奖励的训练方案，不需大规模重训也能优于主流封闭源方法，但与人类审美仍有差距。


<details>
  <summary>Details</summary>
Motivation: 视觉隐喻生成要求语言理解和视觉协同，当前方法难以精准对齐隐喻的意义与视觉表达，因此需要新的框架和评价机制提升隐喻对齐表现。

Method: 提出了自评估视觉隐喻生成框架，包括一个训练自由的S-T-M分解管线和一个结合自评估奖励机制的训练管线，并引入新的隐喻分解分数和意义对齐指标进行评估。

Result: 在保留测试集上，训练自由管线在分解、CLIP和意义对齐指标上优于强闭源基线（GPT-4o、Imagen），训练管线表现接近。用户研究表明，GPT-4o整体偏好较高，训练自由管线在开源方法中领先并在抽象隐喻上优于Imagen。分析发现S-T-M提示对较长或抽象隐喻帮助显著，闭源模型在短小具体隐喻表现更优，采样参数对表现有敏感性。

Conclusion: 结构化提示和轻量级强化学习在适度计算资源下能够很好地实现视觉隐喻的对齐，但与人类偏好的差距主要体现在美学和采样方面。

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [28] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 本文认为大型语言模型主要反映训练语料库的统计和结构，而不是人类认知；作者通过分析变换器架构的计算特点，说明其与人脑处理语言的方式存在本质不同，并提出LLMs和人类都能创新语言表达，但方式截然不同。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型到底模拟了什么，澄清它们与人类认知能力之间的关系，以及解释它们在语言上的本质。

Method: 通过比较变换器（transformer）架构的计算能力与人类语言处理能力的格式，重点讨论架构不变性，并引用相关研究（如Liu等2022年）对“捷径自动机”进行分析。

Result: 变换器架构本质上处理的是线性格式，而人类语言能力涉及超线性计算格式；LLMs和人类学习生成语言的路径和机制显著不同，但都能利用语言作为“话语机器”创造新的语境和表达。

Conclusion: 作者认为大型语言模型（LLMs）主要是训练语料库的模型，而不是直接反映人类认知能力，尽管这并不是一个贬低它们价值的说法。

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [29] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: 该文提出了NOOV神经翻译系统，通过融合自动学习的双语词典和生物医学短语查找表，显著提升英文EHR到西班牙文的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏并行对齐语料库及大量未登录词，电子健康记录（EHR）文本从英文到西班牙文的翻译非常具有临床意义但充满挑战。

Method: 提出了NOOV（No OOV）神经机器翻译（NMT）系统，融合从并行对齐语料自动学习的双语词典与从大型生物医学知识库提取的短语查找表，以解决未登录词和词重复等NMT常见问题。

Result: NOOV系统能够显著提升EHR文本翻译的准确性和流畅性，生成更优质的翻译结果。

Conclusion: NOOV系统有效缓解了领域内并行语料不足和未知词汇问题，对EHR翻译具有临床应用价值。

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [30] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 该论文深入揭示了后训练量化（PTQ）对大语言模型知识能力的影响，提出了任务分层的量化扩展定律。发现知识记忆能力对量化相关参数极为敏感，知识利用能力则更为稳健，为未来量化策略优化提供了理论依据和实践参考。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）因其庞大规模在部署上面临诸多挑战，后训练量化（PTQ）成为一种有效的压缩方法，但目前对PTQ对LLM多样知识能力影响的理解不充分，已有的关于量化模型的扩展定律也忽视了PTQ相关参数和任务敏感性。

Method: 综合实证研究，建立以任务分层的扩展定律。将LLM知识能力区分为记忆能力（memorization）和利用能力（utilization），并构建统一的量化框架，涵盖模型大小、有效比特宽度、校准集大小和分组大小。

Result: 研究发现，相比更鲁棒的知识利用能力，知识记忆能力对有效比特宽度、校准集大小和模型大小的变化表现出更强的敏感性。

Conclusion: 研究获得了PTQ影响的细粒度理解，可为开发更好地保留目标认知功能的知识感知量化策略提供指导。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [31] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: 本文提出通过插入主动生成的洞见来改善LLM在复杂数学推理中的表现，并构建了TBYS框架与自动化数据收集方法，实验证明TBYS在数学推理方面效果优越。


<details>
  <summary>Details</summary>
Motivation: 传统LLM训练数据中缺乏人类实际推理过程中关键的内在思考步骤，尤其是在复杂数学问题中，人类通常不会表达所有推理细节，导致模型推理能力不足。

Method: 提出了在连续推理步骤之间插入insight（洞见），并设计了自动化流程收集和筛选in-context示例，无需大量人工标注或微调。

Result: 在具有挑战性的数学数据集上，TBYS显著提升了模型推理表现，减少了人工和微调工作量。

Conclusion: TBYS框架能够有效提升LLM在复杂数学推理任务中的表现。

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [32] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 提出一种新的解码方法CoDe，能动态结合有无外部知识的概率输出，有效提升大模型回复的忠实性而不损失表达性，并通过实验证明了其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在融合外部知识以减少幻觉时，难以同时保证回复的忠实性（faithfulness）和表达性（expressiveness），造成生成结果要么缺乏外部知识支持，要么表达冗长且不自然。

Method: 引入Collaborative Decoding（CoDe）方法，通过分布散度与模型置信度，动态融合有无外部知识的输出概率；同时设计知识感知重排序机制，防止过度依赖模型内部知识，并适当利用外部信息。

Result: 提出的Collaborative Decoding（CoDe）方法有效提升了生成内容的忠实性，同时不牺牲表达性。实验结果显示，在多种大模型和评测指标上，CoDe都比现有方法表现更优。

Conclusion: CoDe框架凭借分布散度和模型置信度引导动态融合，配合知识感知重排序机制，可以提升大模型在整合外部知识时的表现，既保证忠实性又保留表达性，具有良好通用性。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [33] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文提出了Emotion Omni，一个可在有限数据和低算力条件下理解和回复用户情感的语音LLM，并构建了20万条情感对话数据集，极大提升了语音助手的情感交互能力。


<details>
  <summary>Details</summary>
Motivation: 当前的语音大语言模型（Speech LLMs）虽然可以直接通过语音与用户交互，但还未能充分理解用户语音中丰富的情感和副语言信息。由于情感理解对提升人机交互体验至关重要，且现有同理心语音LLM对大规模数据和计算资源需求极高，如何在有限数据下生成有同理心的语音回复成为主要挑战。

Method: 提出了一种新颖的模型架构Emotion Omni，能够理解用户语音输入的情感内容并生成具备同理心的语音回复。同时，基于开源TTS框架设计了数据生成流程，构建了包含20万条情感对话的数据集，用于同理心语音助手的开发。

Result: Emotion Omni模型能够在有限数据和无需大规模训练的条件下，实现对情感内容的理解和同理心回复的生成。同时，自建的20万情感对话数据集为模型训练提供了支持，并且相关演示已上线展示。

Conclusion: Emotion Omni有效提升了语音大语言模型对用户情感的理解与回复能力，减小了对大规模数据与算力的依赖，为构建情感化人机交互带来了新方案。

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [34] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 该文针对多模态链式思考提示样本选择的不足，提出将提示选取视为课程设计问题，并引入双重难度信号进行难度平衡采样，大幅提升多模态模型推理表现的稳定性和效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态链式思考(MCoT)在提示样本选择上存在局限，随机或人工选取的样本未兼顾模型本身的知识分布和任务复杂度，导致效果不佳且波动大。

Method: 受教学理论“因材施教、难易平衡”启发，将提示选择重新定义为提示课程设计问题：根据模型当前能力有序生成适宜的训练样本。方法综合两类难度标志：1) 基于主动学习预测分歧，刻画模型主观感知难度；2) 评估样本本身复杂性，刻画样本固有难度。通过两维度分析，提出难度平衡采样策略，保证样本在这两个维度的多样性。

Result: 在5个挑战性基准和多个流行多模态大模型上，实验证明该方法显著提升和稳定了性能，大幅减少了随机采样带来的性能波动。

Conclusion: 提出了一种以难度平衡课程为核心的MCoT提示选择新框架，通过融合模型感知难度和样本本身复杂性，实现了更稳健且优化的多模态推理能力提升。

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [35] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文揭示现有Med-VLMs在处理语义等价问题时答案不一致，影响医疗诊断可靠性。作者构建RoMed数据集并提出CCL方法，提高了模型一致性和鲁棒性，实现了领先性能。代码将公开。


<details>
  <summary>Details</summary>
Motivation: 当前医疗视觉-语言模型（Med-VLMs）在高风险医学应用中，面对语义等价但表达方式不同的医学问题时，其答案不一致，影响诊断的可靠性。动机是解决模型对不同问题表述脆弱、答复不稳定的问题。

Method: 构建RoMed数据集，包含144,000个医疗视觉问答，覆盖词级、句级与语义级扰动；提出一致性与对比学习（CCL）方法，包括基于医学知识的一致性学习和偏差感知的对比学习，从而提升模型对表达变体的鲁棒性。

Result: 原有SOTA模型在RoMed上的表现显著下降（如召回率下降40%），暴露出鲁棒性缺陷。CCL方法在三大主流VQA数据集上取得SOTA结果，并在RoMed测试集上将答案一致性提升50%，显著增强鲁棒性。

Conclusion: 医疗视觉-语言模型在面对表述变体时存在严重一致性问题。构建的新数据集和一致性-对比学习方法，有效提升了模型的答复一致性和鲁棒性，为医疗AI诊断提供更可靠技术基础。

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [36] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出Attention2Probability方法，有效提升语音识别和翻译系统的领域术语识别准确率，显著优于传统方法，并提供术语干预语音数据集，为后续相关研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前语音大模型在通用领域取得了进步，但在处理特定领域术语或新词时仍然存在准确性挑战。解决这个难题对于提升语音识别和翻译系统的实际应用价值至关重要。

Method: 提出了一种名为Attention2Probability的注意力驱动术语概率估计算法，将语音与术语之间的交叉注意力权重转化为术语出现概率，并结合课程学习提升检索准确性。此外，作者还构建并公开了一个包含术语的语音数据集用于相关任务评测。

Result: 实验结果显示，Attention2Probability方法在术语检索准确性方面显著优于传统的VectorDB方法，中文最大召回率为92.57%，英文为86.83%，单次查询延迟仅8.71毫秒。该方法提升了SLM在识别和翻译任务中术语的准确率6%-17%。同时揭示当前SLM对术语利用仍有局限性。

Conclusion: Attention2Probability不仅在术语识别效率和准确性上取得较大进步，还通过新的数据集推动语音到文本领域术语干预研究发展。该方法为提升实际语音系统中的领域术语处理能力提供了有效方案和新的研究方向。

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [37] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文提出AOF提示框架，通过语义相似性过滤与新颖性约束提升多语言谜题生成的多样性和原创性，在多种语言和模型上显著减少了内容冗余并提高了创造性。


<details>
  <summary>Details</summary>
Motivation: 多语言谜题生成需要大型语言模型（LLM）既具备文化适应力，又要能进行创造性抽象，但传统提示策略常常导致模型只是复用记忆中的谜题或进行浅层次改写。

Method: 提出了自适应原创性过滤（AOF）提示框架，利用基于余弦相似度的过滤方法，剔除冗余生成内容，并强制要求词汇新颖性和跨语言一致性。

Result: AOF增强的GPT-4o在日语测试中达到0.177的Self-BLEU和0.915的Distinct-2，表现出更高的词汇多样性和更低的冗余度；超越了其他提示方法和语言对。

Conclusion: 语义拒绝机制无需针对具体任务微调，也能引导模型产生具备文化基础和创造性的内容。

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [38] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: 该文提出了解释先行的MGT检测新框架EMMM，兼顾延迟、准确率和面向非专家的可解释性，实验表明其易于理解且表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在客服领域的快速应用，恶意行为者可以利用其生成的机器文本进行用户冒充，现有检测方法在在线对话场景下可靠性和可解释性不足，难以被非专业客服人员信赖使用。

Method: 提出解释-后检测（explanation-then-detection）框架EMMM，优化以非专家易懂解释为优先，结合准确性与低延迟，提升用户对AI检测系统的信任度。

Result: EMMM提供了适合非专家用户理解的解释，在人工评测中有70%的评审更喜欢其输出。同时，其准确率与最先进模型持平，且输出延迟低于1秒。

Conclusion: EMMM方案有效提升了在线客服场景下MGT检测的可解释性和效率，适合实际应用，并已开源相应代码和数据集。

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [39] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: DIVER框架通过联合优化多样性和质量，借助大模型、SFT和RL，能在实际平台上显著提升广告标题的多样性和点击效果。


<details>
  <summary>Details</summary>
Motivation: 现有的广告标题生成方法通常只优化于标题质量或点击率（CTR），忽视了多样性，导致生成结果同质化。广告行业急需既有高质量又有多样性的标题生成方法，以吸引更多受众群体。

Method: 提出了DIVER，一个基于大型语言模型（LLMs）的生成框架，通过多阶段多目标优化结合有监督微调（SFT）和强化学习（RL），联合优化标题的多样性和质量。同时，设计了一套自动生成包含多样化高质量标题的数据生成流程。

Result: DIVER在真实工业数据集上的实验表明，该框架能有效平衡标题的质量与多样性。在一个拥有数亿用户的大规模内容平台上线后，广告主价值（ADVV）提升4.0%，点击率（CTR）提升1.4%。

Conclusion: DIVER框架有效解决了广告标题生成中过度同质化的问题，显著提升了标题的多样性及广告整体效果，为广告主带来更大收益。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [40] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 本文构建了首个多场景多模态MECTEC数据集，并提出基于异构图的新模型M3HG，实现情感因果三元组抽取效果提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态情感因果三元组抽取（MECTEC）研究受到数据集稀缺和场景单一问题的限制，缺乏能支持模型发展的多样化对话数据。

Method: 作者构建了首个多模态、多场景MECTEC数据集MECAD，包含来自56部电视剧的989个会话，并提出了M3HG模型，通过多模态异构图结构显式建模情感与因果语境，同时在话语间与话语内多层级融合语义信息。

Result: 实验结果表明，M3HG模型在新数据集上优于现有主流方法，显著提升了MECTEC任务的性能。

Conclusion: 该研究推动了MECTEC领域的进展，解决了数据集稀缺和场景单一问题，同时验证了新模型的有效性和先进性。

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [41] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出ChronoRAG方法，通过整合和排序文档片段，显著提升了对叙事长文本问答中时序关系和上下文理解的表现，效果超越现有RAG方法。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在处理叙事类长文本问答时效果有限，因为叙事文本的理解需要更广泛的上下文以及片段间的时序关系，而不仅仅是孤立的信息片段。

Method: 提出了ChronoRAG，一个专为叙事文本设计的新型RAG框架，通过将零散信息整合为结构化片段，并在检索到的信息中显式捕捉与保持时间顺序。

Result: 在NarrativeQA数据集上，ChronoRAG在事实识别和复杂时序关系理解方面均显著优于现有方法。

Conclusion: ChronoRAG强调时序推理对于叙事类QA的重要性，并有效提升了长文本叙事问答的表现。

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [42] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: 本文提出了ThinkDial，这是首个开源框架，实现了类似gpt-oss的可控推理模式。它通过多模式预算控制和端到端训练，实现了在大幅减少token情况下性能损失极小，展现了高效、泛化性强的LLM推理资源调控新方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在通过Chain-of-thought推理展现出强大的问题解决能力，但在实际应用中，如何有效控制其计算资源消耗仍是关键难题。尽管专有系统（如OpenAI的gpt-oss系列）已经引入了离散操作模式来直观调控推理过程，开源社区却尚未有类似开源解决方案。

Method: 提出了ThinkDial，这是首个开源、端到端的框架，实现了类似gpt-oss的可控推理。该系统通过三种独立的推理模式（高、中、低）实现推理计算的动态管控。方法核心包括预算模式的有监督微调（将可控推理能力直接植入训练过程）和分两阶段的预算感知强化学习（结合自适应奖励调整）。

Result: ThinkDial在三种推理模式下，能灵活平衡推理能力和计算资源消耗：Medium模式实现50% token减少且性能下降小于10%，Low模式减少75% token而性能下降小于15%。实验验证了其在响应长度显著缩短的同时，依然达到了既定性能阈值，并在分布外任务上展现了良好的泛化能力。

Conclusion: ThinkDial突破了开源LLM推理控制的瓶颈，通过端到端连续集成预算感知训练，成功实现了以响应长度和计算消耗为核心的多模式动态推理能力，并保持了较强的性能和泛化能力。它为实际部署提供了重要的技术基础。

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [43] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出基于规则的强化学习方法引导LLMs进行中文语法纠错，超越传统微调方法，在召回率和整体表现上达到SOTA，展现出RL机制的优势和未来可控性。


<details>
  <summary>Details</summary>
Motivation: 目前GEC领域主要依赖传统的编码器-解码器模型和对大型语言模型(LLMs)的监督微调，但这种方式限制了模型的推理能力。

Method: 提出了一种基于规则的强化学习(Rule-Based RL)新框架，用以引导LLMs进行语法纠错，而不是直接用监督细调生成纠正后的句子。

Result: 在中文数据集上，该框架取得了目前最佳的性能，尤其是在召回率方面有显著提升。

Conclusion: 使用强化学习的规则引导方法能更好地激发LLMs的潜力，为GEC带来更可控和可靠的新范式。

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [44] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: 该文提出了在对话分析中进行可控主题检测的方法，并通过公开比赛DSTC12验证相关技术及成果，推动了领域发展和技术创新。


<details>
  <summary>Details</summary>
Motivation: 面对对话数据分析领域日益增长的数据量与复杂性问题，自动检测和总结对话主题可替代繁琐的人工分析过程，提升效率，满足用户多样化和定制化需求，尤其适用于客户支持与销售等场景。

Method: 将主题检测问题定义为对话语句的联合聚类和主题标签分配，通过用户偏好数据实现主题粒度的可控性。组织了公开比赛，提供数据集、自动化和人工评价指标，并分析参赛队伍方案。

Result: 本次公开赛促进了主题检测技术的发展，推动了联合聚类与可控主题标签分配方法的研究。公开的数据和代码资源，为社区带来了新的数据集和标准，实现了创新的实用方案。

Conclusion: 提出了一种在对话分析中进行可控主题检测的新方法，并通过公开比赛（DSTC12）验证了其实用性和创新性。

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [45] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: LaTeXTrans提出多智能体系统，显著提升LaTeX文档的格式和内容翻译质量，优于现有主流机器翻译方法。


<details>
  <summary>Details</summary>
Motivation: 尽管现代机器翻译系统在通用文本的翻译上取得了显著进展，但面对结构化的LaTeX格式文档仍存在很大挑战。这类文档中自然语言与数学公式、表格、图像和交叉引用等领域特定语法混合，要求在翻译中准确保留格式和语义一致性。

Method: 本文提出了LaTeXTrans，这是一个协作式多智能体系统，包含六个特定功能的智能体：解析器用于将LaTeX拆分为易于翻译的单元；翻译器、验证器、摘要器和术语提取器协作以实现自我校正和专业术语一致性翻译；生成器用于重构结构化的LaTeX文档。

Result: 实验结果表明，LaTeXTrans在翻译准确性与结构保真度方面均优于主流机器翻译系统，能有效且实用地翻译LaTeX文档。

Conclusion: LaTeXTrans多智能体系统有效解决了LaTeX结构化文档的翻译难题，为专业文档的编译与语义一致性提供了更优的自动化翻译工具。

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [46] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: 本文提出了结合AMR语义和传播特征的自监督虚假新闻检测方法，通过LLM生成负样本并采用图对比学习，在少量标注数据情况下表现优异，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 数字时代虚假信息泛滥对社会带来严重挑战，现有方法难以捕捉长距离依赖、复杂语义关系及社会动态，同时对标注数据集依赖大、资源消耗高。

Method: 提出一种自监督的虚假信息检测框架，结合AMR表示的复杂语义关系和新闻传播动态。创新性地引入LLM生成的负锚点的图对比损失（LGCL），并通过多视图图掩码自编码器学习社交传播特征，无需大量标注数据。

Result: 实验结果表明，所提出的自监督方法在有限标注数据下，效果优于当前主流方法，并提升了泛化能力。

Conclusion: 本文证明了结合语义和社交传播特征的自监督检测框架在虚假信息识别上的有效性和优越性，特别适用于标注数据稀缺场景。

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [47] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 作者提出一种通过程序辅助自动化生成高质量数学训练数据的新方法，并保证数据多样性和正确性。使用该数据微调后的大模型，在数学推理任务中取得了最先进的表现。


<details>
  <summary>Details</summary>
Motivation: 目前提升大语言模型（LLM）数学推理能力面临训练数据昂贵、可扩展性和数据可靠性的难题。

Method: 提出了一种全新的程序辅助生成框架，通过整合数学知识系统和领域特定工具，自动生成可执行程序，并转化成自然语言的题目和解答对。利用双向验证机制保证数据的正确性和一致性。

Result: 生成了1230万个高质量的数学题目-解答三元组。模型在这些数据上微调后，在多个基准数据集上推理能力有显著提升，达到当前最优水平。

Conclusion: 程序辅助生成高质量数学语料的方法有效提升了LLM的数学推理及泛化能力，具备很强的扩展性与实用价值。

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [48] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: 大模型常出现高置信度却错误的“过度自信”现象，影响可靠性。本文提出简单高效的ConfTuner方法，通过tokenized Brier score损失函数微调，大幅提升模型置信度校准，泛化能力强，有助于打造可信大模型系统。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLMs）在高风险领域（如科学、法律和医疗）应用中往往出现“过度自信”问题，即模型即使错误也会以高置信度回答，影响其可靠性和可信度。现有校准方法（如提示工程或用启发式置信度微调）普适性有限，急需更简单高效且普适的模型置信度校准方法。

Method: 本文提出ConfTuner方法，即通过一个新颖的损失函数——tokenized Brier score，对LLM进行简单高效的微调，不依赖于真实或代理置信度分数。该损失函数被理论证明为一种正确的评分规则，可以合理激励模型报告真正的置信概率。

Result: 在多种推理任务中，ConfTuner显著提升了大模型置信度表达的校准能力，并能泛化到黑盒大模型（如GPT-4o）。实验还显示，更好校准的置信度提升了模型的自我纠错和模型级联等下游能力。

Conclusion: ConfTuner为LLM置信度校准提供了一种有效、通用、理论有保证的方法，有助于构建更可靠和可信赖的大模型系统。同时该方法适用于主流开源/闭源大模型，具有较强的实际应用价值。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [49] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文提出ReflectivePrompt，通过自反进化机制改进自动提示算法，在多项任务大幅优于现有方法，成为自动提示领域的最新高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 自动生成高效提示（prompt）在大型语言模型（LLM）领域越来越重要，现有的进化算法自动提示方法还存在改进空间，需要更精细和全面的提示搜索策略。

Method: 提出了ReflectivePrompt，一种基于进化算法（evolutionary algorithms）的自反式进化方法。通过在交叉和精英变异操作前引入短期与长期反思操作，积累并动态更新进化过程中的知识，从而提升所得提示的质量。

Result: ReflectivePrompt在33个分类和文本生成数据集上使用开放大语言模型（如t-lite-instruct-0.1和gemma3-27b-it）进行了测试。实验结果显示，ReflectivePrompt在几乎所有指标上都优于现有SOTA方法（如EvoPrompt），如BBH任务上提升达28%。

Conclusion: ReflectivePrompt作为进化算法驱动的自动提示新方法，在自动提示任务中实现了显著的性能提升，是当前最有效的相关方法之一。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [50] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: 本文提出了利用大语言模型辅助内容分析（LACA）的方法，帮助计算教育研究者高效地分析大量文本数据，提升研究严谨性和普适性，对学科发展具有推动作用。


<details>
  <summary>Details</summary>
Motivation: 计算教育研究（CER）往往由一线教学实践者推动，旨在改进自身和整个学科的教学方法，但普遍面临缺乏同事、资源或能力，难以开展具备广泛适用性和严谨性的研究，难以推动学科进步。

Method: 提出一种结合内容分析与大语言模型的内容分析新方法，即LLM辅助内容分析（LACA），用于处理和分析大量文本数据。通过一个计算教育相关的数据集，演示了该方法的可重复性及严谨性。

Result: 展示了LACA方法能够帮助研究人员在不显著增加研究负担的情况下，高效、严谨地处理和分析海量文本数据，从而更好地提炼和推广研究成果。

Conclusion: LACA这种方法在计算教育研究（CER）中具有广阔应用前景，可提升研究的普遍性和质量，对于推动该领域的教学实践与研究水平进步具有积极意义。

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [51] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: 通过自动化分析六国议会发言，发现欧洲议会普遍存在敌意化、极化交流，与议员活跃度无关，互惠机制助推极化。


<details>
  <summary>Details</summary>
Motivation: 情感极化，即对反对集团的负面情绪和敌意上升，已成为全球政治话语的重要特征。该研究旨在考察欧洲议会中是否也存在类似的情感极化现象。

Method: 研究利用六个欧洲国家议会的全面发言语料库，采用自然语言处理技术自动评估议员情感，通过比较议员对对立集团成员与本集团成员的负面言辞，分析情感极化互动模式。

Result: 分析显示，六个国家议会中都存在持续的情感极化现象。虽然议员的活跃度与负面情绪呈相关，但高活跃与低活跃议员在情感极化程度上并无显著差异。此外，互惠机制在议员间情感极化中起到了重要作用。

Conclusion: 欧洲多数议会中普遍存在情感极化，且与议员活跃度关系不大，互惠机制是影响情感极化的重要因素。

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [52] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 本文提出一种多智能体方法，自动生成兼具多样性和隐私保护的RAG系统评测问答数据集，在多项实验中明显优于现有方法，为更合规、更安全的RAG评测提供新思路。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统的评价标准大多关注性能指标，忽视了用于评价的数据集设计和质量，尤其在大模型实际应用时面临如隐私保护等现实约束。因此需开发能兼顾多样性和隐私合规性的新型RAG系统评价数据集生成方案。

Method: 设计了一个多智能体框架，包含：1）多样性智能体利用聚类技术提升主题覆盖和语义变化，2）隐私智能体跨领域检测并屏蔽敏感信息，3）问答策划智能体结合前两者生成适合RAG评价的高质量问答集。并通过实验证明其有效性。

Result: 实验结果表明，该方法生成的数据集在语义多样性和隐私屏蔽效果上显著优于传统方法，尤其在领域专属数据集上表现突出。

Conclusion: 本研究提出的多智能体合成问答数据集生成方法，在语义多样性和隐私保护方面均优于现有基线方法，为未来更安全、合规的RAG系统评价奠定了基础。

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [53] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 本文提出在神经网络内部原生植入符号推理能力的AI Mother Tongue框架，实验证明其同时具备高准确率与可追踪解释性，有望统一提升神经网络的直觉推理与符号解释能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络虽然表现优异，但其黑盒性质导致难以理解其推理过程，解释性较弱。因此，提升神经网络的可解释性与符号推理能力成为重要研究方向。该论文旨在提出一种在神经网络内部植入符号化可解释能力的方法。

Method: 提出了一种AI母语(Symbolic Mother Tongue)的框架，将符号语言原生内嵌到神经模型的表示中。该方法通过符号的方式追踪推理路径、利用门控归纳机制进行选择性关注，并结合符号纯度和决策稀疏性等互补训练目标。训练过程分为两阶段，先获得广泛符号能力后逐步细化为直觉判断能力。

Result: 在多个AI任务上通过实验验证，所提出的方法兼具竞争性准确率和可验证的推理轨迹，显示出在神经模型中统一实现可解释性、直觉性和符号推理的潜力。

Conclusion: AI Mother Tongue框架为神经网络推理过程的可解释性和符号推理能力提供了有效解决方案，有望成为新一代可解释神经模型的核心机制。

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [54] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: DistillPrompt通过多阶段的提示词优化显著提升了自动提示工程的效果，是目前最优的非梯度方法之一。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和prompt工程的迅速发展，自动化挑选最优提示词成为研究热点，传统方法存在探索不充分等问题。

Method: 提出DistillPrompt方法，基于大模型，通过蒸馏、压缩和聚合操作分多阶段将任务相关信息融合进提示词，并利用训练数据优化提示。

Result: 在不同文本分类和生成任务的测试中，DistillPrompt相比现有方法（如Grips）在关键指标上平均提升了20.12%。

Conclusion: DistillPrompt方法在自动化选择优化的大模型提示词（autoprompting）领域表现卓越，是最有效的非梯度方法之一。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [55] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: 本文提出了MovieCORE，一个具有更高认知要求的视频问答数据集，通过多大语言模型生成问题，开发认知评测体系，并引入推理增强模块ACE，有效提升视频-语言模型对电影深层次内容的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答（VQA）数据集通常只涉及表层理解，难以评估模型对电影内容的深层次认知和推理能力，因此需要新的数据集和方法来推动视频理解向更高阶认知任务发展。

Method: 提出了MovieCORE数据集，通过创新的agentic brainstorming方法，使用多个大语言模型（LLMs）协同生成和优化高质量问答对。同时，开发了认知测试体系，用于评估问题的深度、思维激发性和句法复杂度，并提出了针对深层认知任务的VQA模型综合评估方案。此外，提出了Agentic Choice Enhancement（ACE）模块，作为模型推理能力的后训练增强。

Result: MovieCORE数据集能够更好地评测和激发模型在电影内容理解上的深层推理能力。ACE模块使现有视频-语言模型在推理任务上的表现提升最高可达25%。

Conclusion: MovieCORE推动了AI对电影内容的深层理解，为视频问答模型的能力评估和改进提供了新工具，并揭示了现有模型在复杂认知任务上的局限。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [56] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: HiPlan通过分层规划和实时指导，结构化复用专家经验，使得LLM智能体在复杂、长周期任务上的决策能力大幅提升，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的智能体在应对复杂、长周期的规划任务时表现不佳，其问题主要在于缺乏宏观指导，导致任务迷失和失败，同时在执行过程中监督不足，无法及时响应环境变化、容易偏离目标。

Method: 提出了HiPlan，一个分层规划框架。该方法将复杂任务分解为里程碑式的行动指导（宏观方向）和逐步提示（微观行动），包括：1）离线阶段，通过专家演示构建里程碑库，实现结构化经验复用；2）执行阶段，结合过去里程碑的轨迹，根据当前观察动态生成与里程碑目标一致的逐步提示。

Result: 在两个具有挑战性的基准测试中，HiPlan在任务表现上显著超越了已有的强基线方法，并通过消融实验证明分层组件各自的互补性。

Conclusion: HiPlan有效解决了LLM智能体在复杂规划任务中目标迷失和监督不足的问题，通过分层的规划和提示机制，大幅提升了决策表现和鲁棒性。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [57] ["Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
*Tom Röhr,Soumyadeep Roy,Fares Al Mohamad,Jens-Michalis Papaioannou,Wolfgang Nejdl,Felix Gers,Alexander Löser*

Main category: cs.CL

TL;DR: 本研究提出医生意图精细化分类体系，标注大量医患对话，用以评测医疗意图分类模型表现，揭示对话结构转变及其模式，并证实意图特征有助于对话摘要性能提升。数据与代码全部公开。


<details>
  <summary>Details</summary>
Motivation: 尽管医生在对话中有明确的诊断及治疗导向，现有研究鲜有关注医生意图随对话发展而变化的轨迹，缺少细粒度数据和分析方法制约了自动化医学对话系统的发展。

Method: 以Aci-bench数据集为基础，与医学专家合作基于SOAP框架提出医生意图分类体系，对5000余轮医患对话进行大规模标注，并用以评测生成式和编码式模型在医疗意图分类任务中的表现。

Result: 模型对整体医疗对话结构的理解准确率高，但对SOAP意图类别转换的识别仍存在不足。此外，首次揭示了医疗对话中常见的结构模式，为自动化辅助诊断系统设计提供了基础。医生意图过滤用于医疗对话摘要任务时，显著提升了性能。

Conclusion: 本研究首次分析了医患对话中医生意图的动态轨迹，对医生意图的精细化分类能够促进算法揭示对话结构并提升医疗摘要性能。

Abstract: In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.

</details>


### [58] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
*Yue Li,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 大模型对极低资源、尤其书写体系罕见的语言支持薄弱。对比ICL和PEFT两大类适配方式，发现在极低资源场景下，加入语言对齐信号的零样本ICL效果最好，而PEFT和少样本ICL更适合较常见低资源语言。建议针对目标语言类型选择合适适配策略，避免用PEFT处理未见书写体系。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对极低资源语言（尤其是稀有书写体系）支持极度有限，主要由于缺乏训练数据。研究希望揭秘ICL和PEFT等方案在这类场景下的有效性及应用边界。

Method: 系统性地在三种最先进的多语种大模型上，对20种代表性极低资源语言进行了对比评估，涵盖纯ICL（有无辅助对齐信号）、PEFT以及多种零样本/少样本配置。

Result: PEFT在极低资源且书写体系罕见时效果受限；带语言对齐信号的零样本ICL在极低资源语言上尤其有效；较高代表性的低资源语言则更适合采用少样本ICL或PEFT。

Conclusion: 对于极低资源语言，尤其是罕见书写体系的语言，参数高效微调（PEFT）表现有限，而基于语言对齐的零样本上下文学习（ICL）效果更显著。对于稍微更有代表性的低资源语言，少样本ICL或PEFT收益更明显。针对这类语言的实践建议包括：避免将多语种模型用于未见书写体系的微调。

Abstract: Extremely low-resource languages, especially those written in rare scripts,
as shown in Figure 1, remain largely unsupported by large language models
(LLMs). This is due in part to compounding factors such as the lack of training
data. This paper delivers the first comprehensive analysis of whether LLMs can
acquire such languages purely via in-context learning (ICL), with or without
auxiliary alignment signals, and how these methods compare to
parameter-efficient fine-tuning (PEFT). We systematically evaluate 20
under-represented languages across three state-of-the-art multilingual LLMs.
Our findings highlight the limitation of PEFT when both language and its script
are extremely under-represented by the LLM. In contrast, zero-shot ICL with
language alignment is impressively effective on extremely low-resource
languages, while few-shot ICL or PEFT is more beneficial for languages
relatively better represented by LLMs. For LLM practitioners working on
extremely low-resource languages, we summarise guidelines grounded by our
results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning
a multilingual model on languages of unseen scripts.

</details>


### [59] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
*Mathew Henrickson*

Main category: cs.CL

TL;DR: 本研究提出RAG框架，突破了碎片化、多语种艺术品溯源数据高效检索的难题，对档案大规模检索和分析具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 艺术品溯源研究对于验证其真实性、法律维权和理解历史文化背景至关重要，但目前由于档案信息碎片化且多语种，检索效率低下，并且现有平台对元数据要求高，限制了探索性检索。

Method: 提出了检索增强生成（RAG）框架，采用语义检索和上下文摘要的方法，支持自然语言与多语种检索，降低对严格元数据结构的依赖。

Result: 使用Getty Provenance Index（德国销售记录）中的1万条拍卖数据样本验证了方法，结果显示该方案能高效检索和总结拍卖记录，实现可扩展的数据导航。

Conclusion: RAG框架为艺术品市场档案的检索和摘要提供了有效且可扩展的工具，能够更好地支持历史学者和文化遗产专业人士的敏感性研究。

Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for
art provenance studies, focusing on the Getty Provenance Index. Provenance
research establishes the ownership history of artworks, which is essential for
verifying authenticity, supporting restitution and legal claims, and
understanding the cultural and historical context of art objects. The process
is complicated by fragmented, multilingual archival data that hinders efficient
retrieval. Current search portals require precise metadata, limiting
exploratory searches. Our method enables natural-language and multilingual
searches through semantic retrieval and contextual summarization, reducing
dependence on metadata structures. We assess RAG's capability to retrieve and
summarize auction records using a 10,000-record sample from the Getty
Provenance Index - German Sales. The results show this approach provides a
scalable solution for navigating art market archives, offering a practical tool
for historians and cultural heritage professionals conducting historically
sensitive research.

</details>


### [60] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
*Thomas Compton*

Main category: cs.CL

TL;DR: 作者提出结合词汇与语义的新型透明QDA方法，克服现有黑盒工具的不足。方法通过开源Python工具实现，提升了研究可控性、复现性与多层解析能力，为话语分析提供广泛适用且透明的范式。


<details>
  <summary>Details</summary>
Motivation: 量化话语分析（QDA）越来越依赖大型语言模型和计算工具，但过于依赖黑盒软件如MAXQDA和NVivo，可能损害方法透明性和研究目标的一致性。作者因此希望提出更透明且能与研究目标对齐的方法。

Method: 提出了一个结合词汇与语义方法的混合框架。方法包括自定义的Python流程，利用NLTK、spaCy和Sentence Transformers进行数据预处理、词形还原和嵌入生成。使用BERTopic建模，结合UMAP降维、HDBSCAN聚类和c-TF-IDF关键词提取，进行了参数优化及多次运行以提升主题一致性。通过词汇精确搜索与语义聚类相结合，实现多层次分析。

Result: 该方法通过案例研究展示，能够实现对历史政治话语数据的细粒度控制、高方法透明性和可复现性。多层次的分析方式提升了话语主题分析的稳健性与解释性，其透明的代码和材料也公开分享，支持他人复现与应用。

Conclusion: 自定义的、透明的QDA方法能够克服黑盒软件的局限，通过方法三角验证提升研究可靠性，强调研究者主导权和代码层透明性。为计算话语研究提供了可复用和解释性强的工作流。

Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of
Large Language Models and computational tools. However, reliance on black box
software such as MAXQDA and NVivo risks undermining methodological transparency
and alignment with research goals. This paper presents a hybrid, transparent
framework for QDA that combines lexical and semantic methods to enable
triangulation, reproducibility, and interpretability. Drawing from a case study
in historical political discourse, we demonstrate how custom Python pipelines
using NLTK, spaCy, and Sentence Transformers allow fine-grained control over
preprocessing, lemmatisation, and embedding generation. We further detail our
iterative BERTopic modelling process, incorporating UMAP dimensionality
reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised
through parameter tuning and multiple runs to enhance topic coherence and
coverage. By juxtaposing precise lexical searches with context-aware semantic
clustering, we argue for a multi-layered approach that mitigates the
limitations of either method in isolation. Our workflow underscores the
importance of code-level transparency, researcher agency, and methodological
triangulation in computational discourse studies. Code and supplementary
materials are available via GitHub.

</details>


### [61] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
*Zhikai Ding,Shiyu Ni,Keping Bi*

Main category: cs.CL

TL;DR: 本文分析了大型视觉-语言模型对自身知识边界的感知能力，发现其置信度表现尚可但需改进。提出的新校准方法有效提升了模型的自知能力，表现优于仅用语言模型。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）虽然表现出强大的视觉问答能力，但存在幻觉现象。一个可靠的模型应该知道自身知识的边界，即“知之为知之，不知为不知”，因此需要研究LVLM模型对自身知识边界感知的能力。

Method: 论文评估了三种信心信号：概率型信心、答案一致性型信心和口头化信心，并在三个LVLM模型和三个VQA数据集上进行实验。还引入了几种来自大语言模型（LLM）的置信度校准方法，并提出三种有效的新方法。

Result: 研究发现LVLMs具备一定知识边界感知能力，但提升空间仍大。其中概率和一致性型信号是更可靠的信心指标，而口头化信心易导致过度自信。将LLM相关校准方法用于LVLM能提升感知表现。视觉与文本输入的联合处理虽然略降问答表现，但提高了模型自信判断的准确性。

Conclusion: LVLM对自身知识边界有初步感知能力，但现有信心信号需改进。提出的方法可提升其知之为知之能力。联合视觉和文本输入可增加感知能力。

Abstract: Large vision-language models (LVLMs) demonstrate strong visual question
answering (VQA) capabilities but are shown to hallucinate. A reliable model
should perceive its knowledge boundaries-knowing what it knows and what it does
not. This paper investigates LVLMs' perception of their knowledge boundaries by
evaluating three types of confidence signals: probabilistic confidence, answer
consistency-based confidence, and verbalized confidence. Experiments on three
LVLMs across three VQA datasets show that, although LVLMs possess a reasonable
perception level, there is substantial room for improvement. Among the three
confidences, probabilistic and consistency-based signals are more reliable
indicators, while verbalized confidence often leads to overconfidence. To
enhance LVLMs' perception, we adapt several established confidence calibration
methods from Large Language Models (LLMs) and propose three effective methods.
Additionally, we compare LVLMs with their LLM counterparts, finding that
jointly processing visual and textual inputs decreases question-answering
performance but reduces confidence, resulting in an improved perception level
compared to LLMs.

</details>


### [62] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
*Alan Li,Yixin Liu,Arpan Sarkar,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出统一科学推理评测套件SciReas与分析框架KRUX，系统分析LLM科学推理中的知识与推理角色，发现知识检索是瓶颈，外部知识与提升语言化推理可有效提升模型表现，并发布有竞争力的基线模型SciLit01。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在科学问题解决方面面临挑战，既需要深厚领域知识，也要具备复杂推理能力，而现有评测基准过于零散，难以系统分析知识与推理的不同作用。

Method: 提出SciReas，一个综合科学推理任务的多样性评测套件，以及更注重复杂推理的子集SciReas-Pro。同时，设计KRUX框架，用以剖析知识与推理在科学任务中的作用，并结合两者进行深入分析。最后，将数据集与其他最新方法如长链式推理SFT进行对比，推出强力基线模型SciLit01。

Result: 发现：（1）模型参数中检索相关知识是科学推理的瓶颈；（2）在上下文中加入外部知识能持续提升推理模型表现；（3）提升语言化推理有助于模型获取任务相关知识。并发布了SciLit01作为科学推理领域的强基线模型。

Conclusion: 完善而系统化的科学推理评测与分析框架有助于揭示大型语言模型科学推理的瓶颈和提升路径，未来发展应进一步考虑知识检索与推理能力的协同优化。

Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both
deep domain knowledge and the ability to apply such knowledge through complex
reasoning. While automated scientific reasoners hold great promise for
assisting human scientists, there is currently no widely adopted holistic
benchmark for evaluating scientific reasoning, and few approaches
systematically disentangle the distinct roles of knowledge and reasoning in
these tasks. To address these gaps, we introduce SciReas, a diverse suite of
existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a
selective subset that requires more complex reasoning. Our holistic evaluation
surfaces insights about scientific reasoning performance that remain hidden
when relying on individual benchmarks alone. We then propose KRUX, a probing
framework for studying the distinct roles of reasoning and knowledge in
scientific tasks. Combining the two, we conduct an in-depth analysis that
yields several key findings: (1) Retrieving task-relevant knowledge from model
parameters is a critical bottleneck for LLMs in scientific reasoning; (2)
Reasoning models consistently benefit from external knowledge added in-context
on top of the reasoning enhancement; (3) Enhancing verbalized reasoning
improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct
a lightweight analysis, comparing our science-focused data composition with
concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline
for scientific reasoning.

</details>


### [63] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: VibeVoice通过新型分词器和next-token diffusion方法，能高效且高质量地生成多角色、长时间的语音，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成模型在处理多说话人和长时序语音时受限于压缩率和计算资源，影响效率和音质，亟需更有效方法提升长文本、多角色语音合成能力。

Method: 采用next-token diffusion进行自回归潜在向量生成，并设计了连续语音分词器，比Encodec模型提升了80倍数据压缩效率，同时保证音频质量。

Result: VibeVoice可在最长64K窗口（约90分钟）与最多四名说话人条件下生成真实对话语境的高质量长篇语音，音频保真且计算效率优异，优于目前开源/商用对话模型。

Conclusion: VibeVoice能高效合成多位说话者的长篇语音，并在保持高保真音质的同时大幅提升数据压缩与计算效率，超越现有开源和商用对话模型。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [64] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
*Isabel Cachola,Daniel Khashabi,Mark Dredze*

Main category: cs.CL

TL;DR: 该文发现传统可读性指标难以准确反映PLS摘要的易读性，语言模型评价结果与人类一致性更高，建议改用语言模型衡量摘要可读性，并公开了相关代码和数据。


<details>
  <summary>Details</summary>
Motivation: 鉴于当前PLS领域普遍采用传统可读性指标进行摘要评价，但尚未有研究比较这些指标与人工判断的一致性，作者希望填补这一空白，并探索更优的评价方法。

Method: 作者评估了8种可读性指标，并将它们的结果与人工可读性判断进行对比，同时分析了语言模型在PLS数据集上的表现，并考察其是否能更好地衡量深层次的可读性因素（如背景知识需求）。

Result: 实验结果表明，多数传统指标与人工可读性判断的相关性很低，而表现最佳的语言模型与人工判断的皮尔逊相关系数为0.56，说明后者在捕捉可读性（包括深层因素）上更加准确，也能引出不同于传统指标的结论。

Conclusion: 作者发现，传统可读性评价指标（如FKGL）与人工可读性判断相关性较低，而基于语言模型的评价指标与人工判断相关性更高，推荐在PLS领域采用语言模型来评估摘要的可读性，并公开了分析代码和数据。

Abstract: Plain Language Summarization (PLS) aims to distill complex documents into
accessible summaries for non-expert audiences. In this paper, we conduct a
thorough survey of PLS literature, and identify that the current standard
practice for readability evaluation is to use traditional readability metrics,
such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in
other fields, these metrics have not been compared to human readability
judgments in PLS. We evaluate 8 readability metrics and show that most
correlate poorly with human judgments, including the most popular metric, FKGL.
We then show that Language Models (LMs) are better judges of readability, with
the best-performing model achieving a Pearson correlation of 0.56 with human
judgments. Extending our analysis to PLS datasets, which contain summaries
aimed at non-expert audiences, we find that LMs better capture deeper measures
of readability, such as required background knowledge, and lead to different
conclusions than the traditional metrics. Based on these findings, we offer
recommendations for best practices in the evaluation of plain language
summaries. We release our analysis code and survey data.

</details>


### [65] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 本文提出让大语言模型主动生成任务相关界面的范式，并通过多维度评估验证，其在用户体验和交互效率上显著优于传统对话式系统，用户在大多数场景中更喜欢生成式界面。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）多作为助手，支持多种任务，但依然受限于线性请求-响应格式，特别在多轮、信息密集和探索性任务中，互动效率较低。因此，亟需提升人机交互效率和体验。

Method: 提出一种Generative Interfaces for Language Models范式，让LLM主动为用户生成交互界面（UI），通过结构化的界面特定表示和迭代优化，将用户查询转化为特定任务的界面。系统性引入多维评估体系，比较生成式界面与传统聊天界面在多种任务、交互和查询类型下的表现。

Result: 实验结果表明，生成式界面在功能、交互和情感体验上均优于传统对话式界面，且在70%以上的案例中被用户偏好选择。

Conclusion: 生成式界面为提升LLM人机交互体验提供新思路，明确了用户偏好其的场景与原因，为人机协作与UI未来发展铺路。

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [66] [CASP: An evaluation dataset for formal verification of C code](https://arxiv.org/abs/2508.18798)
*Niclas Hertzberg,Merlijn Sevenhuijsen,Liv Kåreborn,Anna Lokrantz*

Main category: cs.FL

TL;DR: 本文针对自动生成代码缺乏正确性保障的问题，构建了 506 个 C 代码与形式化规范（ACSL）的高质量配对数据集 CASP，并确认每一对正确无误，为后续结合代码生成与正式验证的研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型虽然能自动生成代码，但生成程序缺乏严格的正确性保障，而形式化验证虽能解决此问题，却需要专业知识且耗时。目前尚无已验证的 C 代码与形式化规范配对的数据集用于系统性基准测试。本文旨在填补该空白。

Method: 开发了多阶段筛选流程，从 The Stack 1 和 The Stack 2 精心提取 506 对 C 代码与形式化规范。流程包括：识别带有形式化语言注释的 C 文件；确保这些文件可被正式验证，并利用 LLMs 改进未能验证的文件；对剩余文件进行后处理，形成每一对代码与 ACSL 规范都通过 Frama-C 形式化验证的配对。最终用人工检查确保配对质量和正确性。

Result: 成功建立了 506 对 C-ACSL 规范配对的 CASP 数据集。每个配对都通过 Frama-C 形式化验证，并经人工检查确认正确性。这个数据集为后续自动生成代码与正确性验证结合的研究和基准测试提供了基础。

Conclusion: 本文首次发布了配对形式化规范和已验证 C 代码的高质量数据集 CASP，为自动化代码生成和正确性验证领域的系统性研究和评测提供了重要支撑。

Abstract: Recent developments in Large Language Models (LLMs) have shown promise in
automating code generation, yet the generated programs lack rigorous
correctness guarantees. Formal verification can address this shortcoming, but
requires expertise and is time-consuming to apply. Currently, there is no
dataset of verified C code paired with formal specifications that enables
systematic benchmarking in this space. To fill this gap, we present a curated
evaluation dataset of C code paired with formal specifications written in
ANSI/ISO C Specification Language (ACSL). We develop a multi-stage filtering
process to carefully extract 506 pairs of C code and formal specifications from
The Stack 1 and The Stack 2. We first identify C files annotated with formal
languages. Then, we ensure that the annotated C files formally verify, and
employ LLMs to improve non-verifying files. Furthermore, we post-process the
remaining files into pairs of C code and ACSL specifications, where each
specification-implementation pair is formally verified using Frama-C. To ensure
the quality of the pairs, a manual inspection is conducted to confirm the
correctness of every pair. The resulting dataset of C-ACSL specification pairs
(CASP) provides a foundation for benchmarking and further research on
integrating automated code generation with verified correctness.

</details>
