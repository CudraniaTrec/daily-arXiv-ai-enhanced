<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Efficient Cost Bounds with Linear Maps](https://arxiv.org/abs/2509.22982)
*David M Kahn,Jan Hoffmann,Thomas Reps,Jessie Grosen*

Main category: cs.PL

TL;DR: 通过线性映射与矩阵不等式，本文首次实现在多种复杂度下对AARA类型高效并自动化地推断，其方法显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 当前AARA推断cost-free类型算法效率低，且仅适用于多项式复杂度，急需更高效且能处理非多项式复杂度的解决方案。

Method: 用线性映射替代传统的递归方式描述cost-free类型，然后利用线性规划工具求解矩阵不等式，从而高效推断和验证资源复杂度类型。

Result: 原型实现表明该方法在适用场景下推断效率比最先进算法高出指数级。

Conclusion: 用线性映射表示函数的cost-free类型，并通过矩阵不等式进行代数化推理，可以更高效地推断并校验类型，尤其在处理非多项式复杂度的情况下更具优势。

Abstract: The Automatic Amortized Resource Analysis (AARA) derives program-execution
cost bounds using types. To do so, AARA often makes use of cost-free types,
which are critical for the composition of types and cost bounds. However,
inferring cost-free types using the current state-of-the-art algorithm is
expensive due to recursive dependence on additional cost-free types.
Furthermore, that algorithm uses a heuristic only applicable to polynomial cost
bounds, and not, e.g., exponential bounds. This paper presents a new approach
to these problems by representing the cost-free types of a function in a new
way: with a linear map, which can stand for infinitely many cost-free types.
Such maps enable an algebraic flavor of reasoning about cost bounds (including
non-polynomial bounds) via matrix inequalities. These inequalities can be
solved with off-the-shelf linear-programming tools for many programs, so that
types can always be efficiently checked and often be efficiently inferred. An
experimental evaluation with a prototype implementation shows that-when it is
applicable-the inference of linear maps is exponentially more efficient than
the state-of-the-art algorithm.

</details>


### [2] [Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification](https://arxiv.org/abs/2509.23061)
*Xu Xu,Xin Li,Xingwei Qu,Jie Fu,Binhang Yuan*

Main category: cs.PL

TL;DR: 本文提出了DafnyCOMP基准，专注于多函数组合规范生成。实验表明，LLM在此类任务上远逊于单函数场景，存在显著推理瓶颈。该基准有助于检测和推进LLM在高阶代码组合及验证领域的进步。


<details>
  <summary>Details</summary>
Motivation: 现有的评测基准主要关注于单一函数任务，无法完整衡量大语言模型在多函数、复杂数据依赖和跨组件边界推理下的表现。为此，作者提出了新的基准以更好诊断和评估模型在组合性规范生成方面的能力。

Method: 提出了DafnyCOMP基准，包括300个自动合成的多函数程序。利用该基准评估了多种主流LLM模型，在组合性规范生成任务上进行系统性分析。

Result: 结果显示，现有LLM在单函数验证任务上表现良好，但在多函数复杂组合任务上的表现急剧下降，尤其是在跨函数推理方面存在系统性失败，体现在规范脆弱、实现与证明不一致等方面。

Conclusion: DafnyCOMP基准能够作为诊断工具，合理揭示当前LLM在组合性代码生成方面的不足，推动模型向高可靠、可验证、可组合方向发展。

Abstract: We introduce DafnyCOMP, a benchmark for evaluating large language models
(LLMs) on compositional specification generation in Dafny. Unlike prior
benchmarks that focus on single-function tasks, DafnyCOMP targets programs
composed of multiple interacting functions with data dependencies, requiring
reasoning across component boundaries. The benchmark consists of 300
automatically synthesized multi-function programs. We evaluate several
state-of-the-art LLM families and find that, while they perform well on
single-function verification, their performance drops sharply on compositional
tasks. Analysis reveals systematic failures in cross-functional reasoning,
including fragile specifications, misalignment between implementations and
proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for
measuring progress toward reliable, verifiable, and compositional code
generation with LLMs.

</details>


### [3] [Fine-Grained Reasoning About Container-Internal Pointers with Logical Pinning](https://arxiv.org/abs/2509.23229)
*Yawen Guan,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: 提出了一种新的分离逻辑扩展方法，使容器内部指针的临时暴露可以被精确规范和验证，该方法兼容当前主流分离逻辑，简化证明过程、拓展适用场景，并已在实际工具中实现。


<details>
  <summary>Details</summary>
Motivation: 大多数分离逻辑为了模块化，会隐藏容器内部指针，这导致在API需要临时暴露这些指针时难以描述和验证使用这些接口的程序。作者希望解决这个痛点，实现更灵活和精确的容器接口规范。

Method: 提出了一种逻辑锁定（logical pinning）方法，是轻量级的借用模型，可以在逻辑层面选择性地跟踪容器内部指针，并将其推广到magic-wand操作符，从而更容易编写和证明属性。该方法改变了表示谓词和规范的写法，但与大多数分离逻辑变种兼容。

Result: 利用该方法验证了一些典型的指针操作程序，并更精确地推导出容器规范。结果显示：逻辑锁定方法可以涵盖一些著名的证明模式、简化部分复杂证明，还能支持传统规范不支持的程序模式。所有结果均在Rocq证明助手和CFML库中实现。

Conclusion: 逻辑锁定为分离逻辑中的容器指针暴露问题提供了通用且轻量的解决方案，提升了规范精度和可验证性，同时具备较强的实践兼容性和可扩展性。

Abstract: Most separation logics hide container-internal pointers for modularity. This
makes it difficult to specify container APIs that temporarily expose those
pointers to the outside, and to verify programs that use these APIs. We present
logical pinning, a lightweight borrowing model for sequential programs that
allows users to selectively track container-internal pointers at the logical
level. Our model generalizes the magic-wand operator, making it easy to write
and prove precise specifications, including pointer-stability properties.
Because it only changes how representation predicates and specifications are
written, our approach is compatible with most separation logic variants. We
demonstrate the practicality of logical pinning by verifying small but
representative pointer-manipulating programs, and deriving more precise
versions of common container specifications. In doing so, we show that our
approach subsumes some well-known proof patterns, simplifies some complex
proofs, and enables reasoning about program patterns not supported by
traditional specifications. All of our results are mechanized in the Rocq proof
assistant, using the CFML library.

</details>


### [4] [From Affine to Polynomial: Synthesizing Loops with Branches via Algebraic Geometry](https://arxiv.org/abs/2509.25114)
*Erdenebayar Bayarmagnai,Fatemeh Mohammadi,Rémi Prébet*

Main category: cs.PL

TL;DR: 该论文提出了一种利用代数几何和SMT技术，由多项式不变量合成复杂循环结构的通用方法，突破了现有方法对循环类型的限制，实现了更高效和广泛的合成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从不变量合成循环时局限于无判别条件的仿射循环，而实际应用需要支持多项式更新、复杂判别式和任意形态不变量，因此亟需更泛化的合成方法。

Method: 利用代数几何工具，设计并实现了通过求解有限多项式方程组寻找满足不变量的所有非确定性分支循环的算法，并引入了一个新的不变量类别以及更高效的合成算法，最后利用SMT求解器处理求解环节。

Result: 提出了一种能处理多项式更新映射、复杂guard条件和任意形式不变量的循环合成算法，并展示了对某些新型不变量更高效的特定算法。相关方法已实现为软件工具，能以可行时间找到所有满足条件的循环结构。

Conclusion: 通过将循环合成问题转化为多元多项式方程组求解，并利用SMT求解器解决相关问题，实现了从多项式不变量合成更一般性的循环结构。

Abstract: Ensuring software correctness remains a fundamental challenge in formal
program verification. One promising approach relies on finding polynomial
invariants for loops. Polynomial invariants are properties of a program loop
that hold before and after each iteration. Generating such invariants is a
crucial task in loop analysis, but it is undecidable in the general case.
Recently, an alternative approach to this problem has emerged, focusing on
synthesizing loops from invariants. However, existing methods only synthesize
affine loops without guard conditions from polynomial invariants. In this
paper, we address a more general problem, allowing loops to have polynomial
update maps with a given structure, inequations in the guard condition, and
polynomial invariants of arbitrary form.
  We use algebraic geometry tools to design and implement an algorithm that
computes a finite set of polynomial equations whose solutions correspond to all
nondeterministic branching loops satisfying the given invariants. Furthermore,
we introduce a new class of invariants for which we present a significantly
more efficient algorithm. In other words, we reduce the problem of synthesizing
loops to find solutions of multivariate polynomial systems with rational
entries. This final step is handled in our software using an SMT solver.

</details>


### [5] [Guard Analysis and Safe Erasure Gradual Typing: a Type System for Elixir](https://arxiv.org/abs/2408.14345)
*Giuseppe Castagna,Guillaume Duboc*

Main category: cs.PL

TL;DR: 本文提出了Elixir的新类型系统，无需更改编译和运行时架构，结合渐进类型和语义子类型，通过创新机制实现类型健全与表达能力并重，效果已在工业级项目中验证。


<details>
  <summary>Details</summary>
Motivation: Elixir是一种流行且动态类型的函数式编程语言，缺乏静态类型分析工具。本论文旨在解决在不改变Elixir编译或运行环境的前提下，提供精准、健全且实用的静态类型分析能力。

Method: 提出一种新的类型系统，将渐进类型（gradual typing）与语义子类型（semantic subtyping）结合，依靠Erlang VM的隐式检查和开发者自定义守卫（guards）的显式检查保证类型健全性。创新点包括“strong functions”概念和对守卫的细粒度分析，实现了case表达式和守卫函数定义的精确类型细化。

Result: 新类型系统可以被集成到Elixir的新版本中，对大型工业代码库进行了实验验证，显示性能与兼容性没有受到影响，并实现了类型健全性与表达能力的提升。

Conclusion: 本论文为Elixir构建了理论完善的新类型系统，实现了健全、兼容、实用且精准的静态类型分析，为该语言的类型安全和大型开发项目的可靠性打下基础。

Abstract: We formalize a new type system for Elixir, a dynamically typed functional
programming language of growing popularity that runs on the Erlang virtual
machine. Our system combines gradual typing with semantic subtyping to enable
precise, sound, and practical static type analysis, without requiring any
changes to Elixir's compilation pipeline or runtime. Type soundness is ensured
by leveraging runtime checks -- both implicit, from the Erlang VM, and
explicit, via developer-written guards.
  Central to our approach are two key innovations: the notion of "strong
functions", which can be assigned precise types even when applied to inputs
that may fall outside their intended domain; and a fine-grained analysis of
guards that enables accurate type refinement for case expressions and guarded
function definitions. While type information is erased before execution and not
used by the compiler, our "safe erasure" gradual typing strategy maintains
soundness and expressiveness without compromising compatibility or performance.
This work lays the theoretical foundation for Elixir's new type system,
outlines its integration into recent versions of the language, and demonstrates
its effectiveness on large-scale industrial codebases.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908)
*Sergiu Bursuc,Theodore Ehrenborg,Shaowei Lin,Lacramioara Astefanoaei,Ionel Emilian Chiosa,Jure Kukovec,Alok Singh,Oliver Butterley,Adem Bizid,Quinn Dougherty,Miranda Zhao,Max Tan,Max Tegmark*

Main category: cs.SE

TL;DR: 论文提出了最大规模的vericoding基准，验证主流LLM在自动形式化代码生成上的能力，Dafny表现最佳，其他体系仍有提升空间，公开基准促进社区发展。


<details>
  <summary>Details</summary>
Motivation: 论文针对当前LLM生成形式化验证代码（vericoding）的需求，提出和测试迄今最大规模的vericoding基准，用以区别于生成可能含有bug的代码（vibe coding）。

Method: 构建了包含12,504条形式化规范的大型基准库，涵盖Dafny、Verus/Rust、Lean三种体系，并在主流LLM上进行形式化验证代码生成测试。统计不同语言下的成功率，并探究自然语言描述对结果的影响。

Result: 在现有LLM下，不同体系的vericoding成功率分别为Lean 27%、Verus/Rust 44%、Dafny 82%；自然语言描述对成功率提升不大。同时Dafny的验证率在一年内由68%提升至96%。共收录6,174道新问题。

Conclusion: 主流LLM已能高效完成部分形式化代码自动验证，但不同体系表现差异显著，自然语言描述提升有限。论证了基准公开对推动领域进步的价值。

Abstract: We present and test the largest benchmark for vericoding, LLM-generation of
formally verified code from formal specifications - in contrast to vibe coding,
which generates potentially buggy code from a natural language description. Our
benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in
Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find
vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny
using off-the-shelf LLMs. Adding natural-language descriptions does not
significantly improve performance. We also find that LLM progress has improved
progress on pure Dafny verification from 68% to 96% over the past year. The
benchmark and vericoding results are shared at
https://github.com/Beneficial-AI-Foundation/vericoding-benchmark

</details>


### [7] [Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer](https://arxiv.org/abs/2509.22978)
*Teeradaj Racharak,Chaiyong Ragkhitwetsagul,Chayanee Junplong,Akara Supratak*

Main category: cs.SE

TL;DR: 本研究提出用大语言模型作为代码克隆检测器的后置解释器，使用ChatGPT-4对GraphCodeBERT输出结果进行解释，取得高准确率和实用性，并总结了可供未来优化的启示。


<details>
  <summary>Details</summary>
Motivation: 当前ML-based代码克隆检测器虽能准确识别语义克隆，但决策过程不透明，且现有后置解释工具依赖白盒访问或计算成本高，亟需高效的新型后置解释方法。

Method: 研究通过使用ChatGPT-4解释由GraphCodeBERT检测出的代码克隆结果，评估LLM在解释准确性和实用性上的表现。实验还分析LLM参数（如temperature值）的调整对解释效果的影响。

Result: 提出的LLM解释方法准确率高达98%，解释质量达到95%。部分情况下LLM生成的解释和代码例子非常有用，调整参数（如temperature）有助于提升解释准确性，并提出未来改进方向。

Conclusion: 利用大语言模型（LLM）通过上下文学习机制为机器学习代码克隆检测器提供后置解释是可行且有效的方法。综合性能表现优异，能够解释代码克隆结果，并对未来相关研究提供指导。

Abstract: Recent studies highlight various machine learning (ML)-based techniques for
code clone detection, which can be integrated into developer tools such as
static code analysis. With the advancements brought by ML in code
understanding, ML-based code clone detectors could accurately identify and
classify cloned pairs, especially semantic clones, but often operate as black
boxes, providing little insight into the decision-making process. Post hoc
explainers, on the other hand, aim to interpret and explain the predictions of
these ML models after they are made, offering a way to understand the
underlying mechanisms driving the model's decisions. However, current post hoc
techniques require white-box access to the ML model or are computationally
expensive, indicating a need for advanced post hoc explainers. In this paper,
we propose a novel approach that leverages the in-context learning capabilities
of large language models to elucidate the predictions made by the ML-based code
clone detectors. We perform a study using ChatGPT-4 to explain the code clone
results inferred by GraphCodeBERT. We found that our approach is promising as a
post hoc explainer by giving the correct explanations up to 98% and offering
good explanations 95% of the time. However, the explanations and the code line
examples given by the LLM are useful in some cases. We also found that lowering
the temperature to zero helps increase the accuracy of the explanation. Lastly,
we list the insights that can lead to further improvements in future work. This
study paves the way for future studies in using LLMs as a post hoc explainer
for various software engineering tasks.

</details>


### [8] [The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution](https://arxiv.org/abs/2509.23261)
*Fei Gu,Zi Liang,Hongzong LI,Jiahao MA*

Main category: cs.SE

TL;DR: AI辅助编程更倾向于支持热门语言和框架，这将加剧主流工具的集中，降低编程生态的多样性与创新空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注于AI辅助编程的提示设计和代码生成质量，但AI如何影响软件工程迭代和生态系统尚未深入探讨。本研究旨在揭示AI编程与软件生态之间的互动，并分析其更广泛影响。

Method: 作者进行了大规模实验，涵盖数千个算法编程任务和数百个框架选择任务，系统性分析AI辅助编程与软件生态系统的关系。

Result: AI生成代码在流行的编程语言和框架下成功率更高，呈现明显的马太效应，即热门技术越受AI支持，进一步强化主流地位，可能抑制生态多样性和创新。

Conclusion: AI驱动的编程方式有可能加速主流工具的集中化发展，影响未来编程生态系统的多样性和创新能力。这一现象应引起业界和研究界关注，避免技术单一化。

Abstract: AI-assisted programming is rapidly reshaping software development, with large
language models (LLMs) enabling new paradigms such as vibe coding and agentic
coding. While prior works have focused on prompt design and code generation
quality, the broader impact of LLM-driven development on the iterative dynamics
of software engineering remains underexplored. In this paper, we conduct
large-scale experiments on thousands of algorithmic programming tasks and
hundreds of framework selection tasks to systematically investigate how
AI-assisted programming interacts with the software ecosystem. Our analysis
reveals \textbf{a striking Matthew effect: the more popular a programming
language or framework, the higher the success rate of LLM-generated code}. The
phenomenon suggests that AI systems may reinforce existing popularity
hierarchies, accelerating convergence around dominant tools while hindering
diversity and innovation. We provide a quantitative characterization of this
effect and discuss its implications for the future evolution of programming
ecosystems.

</details>


### [9] [Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics](https://arxiv.org/abs/2509.23297)
*Anthony Savidis,Christos Vasilopoulos*

Main category: cs.SE

TL;DR: 本文提出了一套集合分组、度量及交互特性的可视化工具，更好地帮助开发者理解大型软件系统。


<details>
  <summary>Details</summary>
Motivation: 软件可视化旨在提升开发者对源代码的理解、分析、维护和演化能力，通过图形方式展现软件结构和演变历史，解决从文本难以察觉的系统行为和复杂性问题。

Method: 该工作提出了三项方法学创新：1) 可配置的分组机制，根据任意关系灵活组织代码元素；2) 结合细粒度与粗粒度指标，从多个层级展现系统属性；3) 交互式可视化引擎，支持开发者动态调整可视化渲染参数。

Result: 提出的机制和工具能够提升大规模系统的代码理解和洞察，增强了可视化的适应性和信息价值。

Conclusion: 综合各项创新，该研究为源代码理解提供了更灵活、深入的可视化手段，有助于软件工程师高效分析和维护复杂系统。

Abstract: Software visualization seeks to represent software artifacts graphical-ly in
two or three dimensions, with the goal of enhancing comprehension, anal-ysis,
maintenance, and evolution of the source code. In this context, visualiza-tions
employ graphical forms such as dependency structures, treemaps, or time-lines
that incorporate repository histories. These visualizations allow software
engineers to identify structural patterns, detect complexity hotspots, and
infer system behaviors that are difficult to perceive directly from source
text. By adopting metaphor-based approaches, visualization tools provide
macroscopic overviews while enabling focused inspection of specific program
elements, thus offering an accessible means of understanding large-scale
systems. The contri-bution of our work lies in three areas. First, we introduce
a configurable group-ing mechanism that supports flexible organization of code
elements based on arbitrary relationships. Second, we combine fine-grained and
coarse-grained software metrics to provide a multi-level perspective on system
properties. Third, we present an interactive visualization engine that allows
developers to dynamically adjust rendering attributes. Collectively, these
advances provide a more adaptable and insightful approach to source code
comprehension.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [10] [Structural Separation and Semantic Incompatibility in the P vs. NP Problem: Computational Complexity Analysis with Construction Defining Functionality](https://arxiv.org/abs/2509.22995)
*Yumiko Nishiyama*

Main category: cs.LO

TL;DR: 本文通过CDF框架分析SAT问题，提出其结构特性可能是导致NP类问题难解的根本原因，重新定义了SAT在复杂性理论中的作用，对P与NP问题给出新的结构性理解。


<details>
  <summary>Details</summary>
Motivation: 传统上SAT被视为NP完备性的标准代表，论文希望通过反思SAT结构本身，探索其是否是NP复杂性产生的结构根源，而不只是一个判定标准。

Method: 采用CDF框架对SAT问题结构进行分析，重点研究3SAT形式的句法结构与计算复杂性之间的关系。

Result: CDF框架下发现3SAT的递归和非局部结构很可能定义了可解和不可解问题的边界，因此SAT的结构性对P vs NP问题具有根本影响。

Conclusion: 论文提出，3SAT的结构性特点可能是NP问题计算不可解性的本源，即SAT不仅仅是NP完备的基准，更可能是诱发NP复杂性的结构模型。

Abstract: The Boolean satisfiability problem (SAT) holds a central place in
computational complexity theory as the first shown NP-complete problem. Due to
this role, SAT is often used as the benchmark for polynomial-time reductions:
if a problem can be reduced to SAT, it is at least as hard as SAT, and hence
considered NP-complete. However, the CDF framework offers a structural
inversion of this traditional view. Rather than treating SAT as merely a
representative of NP-completeness, we investigate whether the syntactic
structure of SAT itself -- especially in its 3SAT form -- is the source of
semantic explosion and computational intractability observed in NP problems. In
other words, SAT is not just the yardstick of NP-completeness, but may be the
structural archetype that induces NP-type complexity. This reframing suggests
that the P vs NP question is deeply rooted not only in computational resource
limits, but in the generative principles of problem syntax, with 3SAT capturing
the recursive and non-local constructions that define the boundary between
tractable and intractable problems.

</details>


### [11] [Proceedings Twentieth International Symposium on Logical and Semantic Frameworks with Applications](https://arxiv.org/abs/2509.23739)
*Haniel Barbosa,Christophe Ringeissen*

Main category: cs.LO

TL;DR: LSFA 2025在巴西利亚召开，旨在理论与实践结合，涵盖逻辑和语义框架相关多领域，促进国际交流与技术发展。


<details>
  <summary>Details</summary>
Motivation: 该研讨会旨在推动逻辑与语义框架领域理论与实践的结合，促进交流与合作，提升研究和应用水平。

Method: 通过会议论文集形式，收录和展示了相关领域的最新理论成果与实际应用反馈。

Result: 成功举办了第20届逻辑与语义框架应用研讨会，汇集了证明与类型理论、方程推理、重写系统、自动推理和并发理论等领域的最新研究。

Conclusion: 本论文集通过促进理论与实践的交流，推动了逻辑和语义框架相关领域的发展。

Abstract: This volume contains the proceedings of the 20th Workshop on Logical and
Semantic Frameworks with Applications (LSFA 2025), which was held in Brasilia,
the capital of Brazil, from October 7 to October 8, 2025.
  The aim of the LSFA series of workshops is bringing together theoreticians
and practitioners to promote new techniques and results, from the theoretical
side, and feedback on the implementation and use of such techniques and
results, from the practical side. LSFA includes areas such as proof and type
theory, equational deduction and rewriting systems, automated reasoning and
concurrency theory.

</details>


### [12] [The Complexity of Defining and Separating Fixpoint Formulae in Modal Logic](https://arxiv.org/abs/2509.24583)
*Jean Christoph Jung,Jędrzej Kołodziejski*

Main category: cs.LO

TL;DR: 该论文系统分析了模态不动点逻辑公式的可分性、可插值性及其复杂度，并对分离器构造与推广做了详细讨论，对相关逻辑和自动化推理理论具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 模态不动点逻辑在理论计算机科学有重要应用，了解模态公式之间是否可以通过更简单公式分离及其复杂度，有助于理论研究与相关自动化工具的开发。特别地，不同模型结构下问题性质可能有根本性差异，值得系统研究。

Method: 基于复杂性理论和模态逻辑，作者在不同模型类别（如有界出度、任意模型、二叉树、字模型）下通过理论证明分析了模态可分性与可插值性问题的判定复杂度。同时提供了构造分离公式的算法，并通过扩展案例分析探讨了更一般情形下的分离问题。

Result: 主要结果为：（1）出度≤1的模型（words）下分离问题为PSpace-complete；（2）任意和二叉模型下为ExpTime-complete；（3）出度有穷且≥3时为TwoExpTime-complete，并且在该情形下Craig插值性质失效；（4）插值存在性问题为coNExpTime-complete，比普通有效性判定更复杂；（5）给出了分离器有效构造算法及带级数算子的推广案例。

Conclusion: 论文全面研究了模态不动点公式的可分性及其特殊情形下的可定义性，分别给出了不同模型类别下该问题的复杂度，并指出部分情形下Craig插值性质的失效及插值存在性问题的复杂度更高。除此之外，还给出了用于有效构造分离公式的算法，并探讨了推广到带有级数模态算子的情形。

Abstract: Modal separability for modal fixpoint formulae is the problem to decide for
two given modal fixpoint formulae $\varphi,\varphi'$ whether there is a modal
formula $\psi$ that separates them, in the sense that $\varphi\models\psi$ and
$\psi\models\neg\varphi'$. We study modal separability and its special case
modal definability over various classes of models, such as arbitrary models,
finite models, trees, and models of bounded outdegree. Our main results are
that modal separability is PSpace-complete over words, that is, models of
outdegree $\leq 1$, ExpTime-complete over unrestricted and over binary models,
and TwoExpTime-complete over models of outdegree bounded by some $d\geq 3$.
Interestingly, this latter case behaves fundamentally different from the other
cases also in that modal logic does not enjoy the Craig interpolation property
over this class. Motivated by this we study also the induced interpolant
existence problem as a special case of modal separability, and show that it is
coNExpTime-complete and thus harder than validity in the logic. Besides
deciding separability, we also provide algorithms for the effective
construction of separators. Finally, we consider in a case study the extension
of modal fixpoint formulae by graded modalities and investigate separability by
modal formulae and graded modal formulae.

</details>


### [13] [Generalization of Variadic Structures with Binders: A Tool for Structural Code Comparison](https://arxiv.org/abs/2509.25023)
*Alexander Baumgartner,Temur Kutsia*

Main category: cs.LO

TL;DR: 本文提出一种针对可变参结构（如AST）并带有绑定符号的新型反统一算法，能够精准比较和泛化复杂代码结构，并通过参数化方法兼顾灵活性与精度，适用于代码克隆检测和结构相似性分析等多种场景。


<details>
  <summary>Details</summary>
Motivation: 现有用于代码结构比较的方法在处理绑定变量和可变参数结构（如抽象语法树）时存在局限，尤其容易因变量名或代码碎片的插入、删除造成比较和泛化困难。需要一种能够更灵活、精确处理这种结构的数据泛化与比较工具。

Method: 提出了一种新颖的反统一算法，专门针对具有绑定符号的可变参结构泛化。方法结合了处理变量绑定的名义技术，并支持在抽象语法树等结构中常见的变长表达式。此外，算法区分原子、项变量与序列变量，并引入可参数化的刚性函数，实现对相似性评判标准的细粒度调控。

Result: 所提出的算法能够在最大程度保留结构相似性的同时，抽象出系统性差异，并能详细量化结构差异，为表达式重建和比较提供支持。在代码相似性检测、代码克隆识别、重构和程序分析等任务中表现出良好适应性与实用价值。

Conclusion: 新算法为含有绑定符号和可变长表达式的结构比较带来更灵活和精确的方案，显著提升了对于绑定变量和结构差异的处理能力，在多种相关领域具备广泛应用前景。

Abstract: This paper introduces a novel anti-unification algorithm for the
generalization of variadic structures with binders, designed as a flexible tool
for structural code comparison. By combining nominal techniques for handling
variable binding with support for variadic expressions (common in abstract
syntax trees and programming languages), the approach addresses key challenges
such as overemphasis on bound variable names and difficulty handling insertions
or deletions in code fragments. The algorithm distinguishes between atoms and
two kinds of variables (term and hedge variables) to compute best
generalizations that maximally preserve structural similarities while
abstracting systematic differences. It also provides detailed information to
reconstruct original expressions and quantify structural differences. This
information can be useful in tasks like code clone detection, refactoring, and
program analysis. By introducing a parametrizable rigidity function, the
technique offers fine-grained control over similarity criteria and reduces
nondeterminism, enabling flexible adaptation to practical scenarios where
trivial similarities should be discounted. Although demonstrated primarily in
the context of code similarity detection, this framework is broadly applicable
wherever precise comparison of variadic and binder-rich representations is
required.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
*Alessandra Urbinati,Mirko Lai,Simona Frenda,Marco Antonio Stranisci*

Main category: cs.CL

TL;DR: 提出用模型置信度（不确定性）作为衡量内容审查模型公平性的新方法，发现模型对弱势群体标签即使准确，置信度可能不足，这一分析有助于更好地去偏和提升公平性。


<details>
  <summary>Details</summary>
Motivation: 自动内容审查对社交媒体安全至关重要，但语言模型易在内容审查中延续种族和社会偏见。现有资源虽尝试挑战该问题，但如何衡量模型公平性仍未解决。该研究试图提出更有效的公平性评估途径。

Method: 采用无监督方法，以共形预测技术计算模型对少数群体（女性及非白人）注释消息的分类不确定性（置信度），并分析模型的偏见。将这种不确定性作为衡量公平性的指标，与传统的性能指标（如F1分数）进行比较。

Result: 有些预训练模型对少数群体标签预测准确，但置信度却很低。结果表明，通过置信度评估，可以揭示模型在不同群体上的代表与偏见，有利于后续模型去偏与公平优化。

Conclusion: 通过测量模型的预测置信度，可以更好地了解哪些弱势群体在预训练模型中的代表性，从而引导模型去偏过程，使其在内容审查任务中更公平地服务于不同群体。

Abstract: Automatic content moderation is crucial to ensuring safety in social media.
Language Model-based classifiers are being increasingly adopted for this task,
but it has been shown that they perpetuate racial and social biases. Even if
several resources and benchmark corpora have been developed to challenge this
issue, measuring the fairness of models in content moderation remains an open
issue. In this work, we present an unsupervised approach that benchmarks models
on the basis of their uncertainty in classifying messages annotated by people
belonging to vulnerable groups. We use uncertainty, computed by means of the
conformal prediction technique, as a proxy to analyze the bias of 11 models
against women and non-white annotators and observe to what extent it diverges
from metrics based on performance, such as the $F_1$ score. The results show
that some pre-trained models predict with high accuracy the labels coming from
minority groups, even if the confidence in their prediction is low. Therefore,
by measuring the confidence of models, we are able to see which groups of
annotators are better represented in pre-trained models and lead the debiasing
process of these models before their effective use.

</details>


### [15] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: AccessEval基准揭示了主流大语言模型在回答残障相关问题时表现出负面情感、刻板印象和事实错误，影响特定残障群体，强调在技术实际应用中需关注并缓解模型偏见。数据集已开放。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在实际应用中被广泛部署，但对现实生活中的查询表现出差异，尤其在残障群体的情境下缺乏系统性评估。因此，作者希望系统性分析LLM在不同残障类型和领域中的表现差异及潜在偏见。

Method: 作者提出了AccessEval这一基准，使用中性和残障意识查询对21种闭源和开源LLM进行评测，涵盖6个现实领域和9种残障类型。评估指标包括情感倾向、社会认知、事实准确性。

Result: 分析发现，针对残障相关查询，LLM回应普遍更为负面，存在更多刻板印象和事实错误，特别在听力、语言和行动残障用户群体影响更明显。这些负面效应因领域和残障类型有所不同，反映了模型行为中的系统性残障歧视。

Conclusion: 作者通过现实决策场景考察LLM，揭示模型偏见会对残障用户造成实际伤害，强调日常应用中消除偏见的重要性。公开了数据集以支持进一步研究。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [16] [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
*Kaishuai Xu,Wenjun Hou,Yi Cheng,Wenjie Li*

Main category: cs.CL

TL;DR: 作者提出了RAR$^2$框架，将推理与检索互相增强，有效解决复杂医学问答中的知识需求和推理瓶颈，通过Direct Preference Optimization进行训练，在多组医学问答数据集上均取得了比RAG基线更好的表现。


<details>
  <summary>Details</summary>
Motivation: RAG虽然能通过外部知识增强回答，减少幻觉和知识空白，但面对复杂医学问题时，单纯输入不足以满足德性化知识需求。现有方法多集中于精化查询，缺乏对推理过程的显式建模，限制了获取和整合临床相关知识的能力。

Method: 提出RAR$^2$，结合Reasoning-Augmented Retrieval和Retrieval-Augmented Reasoning的联合学习框架。全流程包括构建思维过程以揭示隐性知识需求，指导检索和答案生成，并通过混合偏好对和Direct Preference Optimization进行模型训练，还设计了两种测试时扩展策略。

Result: RAR$^2$能更好地发现和满足隐性知识需求，并有效提升医学问答的推理与检索能力，在多个数据集上效果优异，优于常规RAG方法。

Conclusion: RAR$^2$框架在多个生物医学问答数据集上表现优异，超越了RAG基线方法，无论是否进行微调。

Abstract: Large Language Models (LLMs) have shown promising performance on diverse
medical benchmarks, highlighting their potential in supporting real-world
clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key
approach for mitigating knowledge gaps and hallucinations by incorporating
external medical information. However, RAG still struggles with complex medical
questions that require intensive reasoning, as surface-level input often fails
to reflect the true knowledge needs of the task. Existing methods typically
focus on refining queries without explicitly modeling the reasoning process,
limiting their ability to retrieve and integrate clinically relevant knowledge.
In this work, we propose RAR$^2$, a joint learning framework that improves both
Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$
constructs a thought process to uncover implicit knowledge requirements and
uses it to guide retrieval and answer generation. We build a training dataset
of mixed preference pairs and apply Direct Preference Optimization (DPO) to
train the model. Moreover, we design two test-time scaling strategies to
explore the boundaries of our framework. Experiments demonstrate the
effectiveness of RAR$^2$ across several biomedical question answering datasets,
outperforming RAG baselines with or without fine-tuning.

</details>


### [17] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: 该论文提出并公开TRUEBench基准，全面评估大语言模型在多语言、多轮对话及隐含约束场景下的实际能力。实验证明主流模型在该基准上表现有限，TRUEBench揭示出实际应用场景中模型的真实水平和瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在真实场景下作为生产力助手愈发重要，但现有的评测基准在评估其真实指令遵循能力方面存在不足。具体表现为缺乏多语言覆盖、未能捕捉用户请求中的隐含约束，以及忽视多轮对话的复杂性。

Method: 提出TRUEBench基准，包含12种语言的输入提示、单实例多语种指令、严格的评估标准（涵盖显式与隐式约束）、复杂多轮对话（包含累积约束及上下文切换）。此外，使用LLM验证器优化约束，确保评估可靠性。

Result: 实验表明，TRUEBench比现有基准难度显著更高。例如，强大模型如OpenAI o1在该基准上的通过率仅为69.07%。

Conclusion: TRUEBench为生产力场景下的大语言模型提供了更具挑战性和现实性的评估，揭示了其能力和局限性，对真实应用具有重要参考意义。

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [18] [The Role of Logic and Automata in Understanding Transformers](https://arxiv.org/abs/2509.24024)
*Anthony W. Lin,Pablo Barcelo*

Main category: cs.FL

TL;DR: 本文综述了利用逻辑和自动机等理论工具分析大型语言模型transformers能力的最新进展，并提出了许多相关开放问题。


<details>
  <summary>Details</summary>
Motivation: 目前对transformers底层能力机制理解不足，亟需理论角度解释其强大表现。

Method: 回顾近年有关transformers能力问题的研究进展，强调逻辑、自动机与电路复杂性理论的结合。

Result: 展现了逻辑、自动机等理论工具在分析transformers能力中的有效性，并提出交叉领域的若干开放问题。

Conclusion: 逻辑和自动机理论在理解transformers能力方面发挥了核心作用，本文还指出当前相关领域还存在许多未解决的问题。

Abstract: The advent of transformers has in recent years led to powerful and
revolutionary Large Language Models (LLMs). Despite this, our understanding on
the capability of transformers is still meager. In this invited contribution,
we recount the rapid progress in the last few years to the question of what
transformers can do. In particular, we will see the integral role of logic and
automata (also with some help from circuit complexity) in answering this
question. We also mention several open problems at the intersection of logic,
automata, verification and transformers.

</details>


### [19] [One-sided Hom shifts](https://arxiv.org/abs/2509.24754)
*Marie-Pierre Béal,Alexi Block Gorman*

Main category: cs.FL

TL;DR: 本文解决了有限型一侧移位与树移位是否与相应Hom移位共轭的判定性问题，并给出相应决策方法。


<details>
  <summary>Details</summary>
Motivation: 判定某些动力系统结构上的共轭关系属于符号动力系统的基础问题，但一般情形下难以解析，因此特定类型（如有限型移位与Hom移位）之间判定其结构同构具有理论价值与实际意义。

Method: 利用Williams的一侧移位理论进行证明。

Result: 确定判定算法，意味着对于相关移位系统的结构等价性问题已可有效识别。

Conclusion: 我们证明了两个判定性问题可以被解决：一侧的有限型移位是否与一侧的Hom移位共轭，以及有限型树移位是否与Hom树移位共轭。

Abstract: We prove that it is decidable whether a one-sided shift of finite type is
conjugate to a one-sided Hom-shift, and whether a tree-shift of finite type is
conjugate to a Hom tree-shift. The proof uses Williams's theory for one-sided
shifts

</details>
