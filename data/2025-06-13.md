<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.LO](#cs.LO) [Total: 7]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.DM](#cs.DM) [Total: 8]
- [cs.FL](#cs.FL) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: 作者提出了一种把大语言模型和Lisp交互环境深度融合的新方法，使模型具备动态工具生成和反思能力，并为将神经模型与符号编程结合的AI系统设计提供了新框架。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在交互式、持续性编程环境中的集成尚未完善，特别是在工具创建和动态反思上的能力有限。作者希望通过结合Lisp环境，挖掘神经语言模型与符号编程融合的潜力。

Method: 提出一种新架构，将LLMs与Lisp的持久、交互式环境结合。通过在生成过程中嵌入Lisp表达式，并借助中间件拦截，实现LLM可以编写、调用并进化自身工具，可支持有状态的外部记忆、反思式编程和动态工具创建。

Result: 实现了一个系统框架，使LLM能动态和可编程地扩展自身能力，并通过实例展示符号编程与神经生成模型集成带来的新功能。还提出了设计原则，指导未来类似系统的开发。

Conclusion: 将LLMs与交互式Lisp环境结合，可以显著增强AI系统的灵活性和工具创建能力。该架构为神经语言生成与符号编程融合提供了新途径，为开发更强大的交互式AI系统开启了方向。

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [2] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 本论文提出了支持异构、无类型系统的消息传递协议合规性验证框架，首次实现了语言无关的逻辑关系机械化，并在Coq中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 当今计算领域正逐渐转向针对分布式和异构系统（如云计算与物联网）的应用，这些应用普遍采用并发、消息传递，并与外部对象（如传感器或外部代码）交互。由于缺乏统一的实现语言与类型系统，传统的协议验证方法已不再适用，因此迫切需要开发新的协议合规性验证方法。

Method: 本文提出了一个用于认证异构消息传递系统协议合规性的框架，首次实现了一种语言无关的逻辑关系（logical relation）机械化，完全基于带标签迁移的语义，能够适用于有类型、无类型甚至‘外部对象’的组件。该方法在Coq定理证明器中实现，并以具体应用或设备的实例验证与整个类型系统下有类型应用的全局验证为案例。

Result: 框架能够认证复杂异构消息传递系统遵循协议，成功在Coq中机械化，并通过两个场景（实例级和系统级）验证了方法的适用性。

Conclusion: 作者提出的方法可以打破实现语言和类型系统的限制，实现对异构分布式系统协议合规性的形式化和可机械化验证，为更广泛的系统交互安全与正确性验证提供方法论支持。

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [3] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: 提出了Hazel Deriver在线工具，有效帮助学生掌握推理树构造，并提升学习体验和理解。


<details>
  <summary>Details</summary>
Motivation: 学生在编程语言和形式逻辑课程中，常因推理规则复杂、缺乏即时反馈、手写证明过程繁琐而难以构建推理树。

Method: 提出了一种基于Web的实时编辑器Hazel Deriver，依托Hazel编程环境，通过分层支撑机制和结构化、互动体验，帮助学生逐步探索并得到实时反馈。

Result: 初步用户研究显示，Hazel Deriver可以降低学生对推导任务的难度感，同时提升概念理解和学习参与度。

Conclusion: Hazel Deriver通过分层支撑和实时反馈，有助于学习者更有效地掌握规则推导任务，同时探讨了系统引导与学习者自主性之间的平衡。

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [4] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: 本文提出并实现了一种新型类型化编舞语言λ_{QC}，具备一等进程名、类型/位置多态、递归类型等特性，显著增强了并发系统的表达能力，并通过机械化验证保障程序死锁自由。


<details>
  <summary>Details</summary>
Motivation: 现有的编舞式编程语言在为现代并发系统编程时缺乏一些重要特性，例如节点动态决定谁执行某项计算，并将该决定通知其他节点的能力。

Method: 提出了一种新的类型化编舞语言λ_{QC}，该语言支持一等进程名，以及对类型和节点集合的多态。该语言还引入了代数和递归数据类型、多节点值等特性，并在Rocq工具中形式化并机械化验证了这些结果。

Result: λ_{QC}提升了编舞语言的表达能力，能够实现先前工作中无法表达的并发系统行为，并保证死锁自由。

Conclusion: λ_{QC}填补了现有编舞语言在动态决策和多态性方面的空白，通过形式化验证展示其安全性和强大表达力。

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>


### [5] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: 本文提出将大语言模型与Lisp环境结合，允许模型动态编程和创建工具，为交互式AI系统的发展铺平道路。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在交互和动态工具创造方面存在局限，难以高效利用外部计算资源和实现反思性编程。作者希望通过与Lisp环境集成，提升LLM的能力。

Method: 提出一种将LLM与持久化的交互式Lisp环境集成的架构。通过中间件层，在LLM生成内容时嵌入Lisp表达式并捕获，实现LLM与REPL（只读求值循环）的程序化交互。允许LLM定义、调用及进化自己的工具。

Result: 实现了一个可以让LLM动态编程、管理外部状态并自创工具的系统架构，提出了适用于未来神经语言与符号编程一体化AI系统的设计原则和框架。

Conclusion: 将LLM与交互式Lisp环境集成能极大扩展LLM的能力，为AI系统融合神经网络与符号编程提供了新的方向和实践蓝图。

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [6] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 针对云计算和物联网等异构分布式应用难以协议验证的问题，本文提出并在Coq中实现了一套基于逻辑关系和标记转换语义的通用认证框架，可验证多种语言、类型系统甚至物理设备的消息交互协议合规性。


<details>
  <summary>Details</summary>
Motivation: 当前的计算应用越来越多地部署在分布式和异构系统（如云计算和物联网）上，这些系统常常是并发的、基于消息传递，并且涉及外部代码或物理设备。由于缺乏统一的实现语言或类型系统，现有的协议验证方法失效，因此需要新的验证框架来保证系统的协议合规性。

Method: 本文提出了一种用于异构消息传递系统协议合规性认证的理论框架。其核心是首次机械化了与语言无关的逻辑关系，依赖于标记转换语义，适用于任意实现，包括有类型和无类型甚至外部对象。此外，在Coq定理证明器中机械化了该逻辑关系，并对两种场景进行了案例研究。

Result: 通过逻辑关系和标记转换语义，本文方法能够验证异构系统中的协议合规性。两类典型应用场景（针对单个实例验证及针对某类型系统全局验证）在Coq中实现，验证了方法的可行性与通用性。

Conclusion: 本文提出了一套可在不同实现语言和类型系统下适用的协议合规性验证方法，将逻辑关系机械化，支持广泛的异构消息传递系统，为实际的分布式与物联网应用协议安全提供了理论与工具支持。

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [7] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: Hazel Deriver 是一个让学生更容易构建推导树、获得实时反馈的在线编辑器，能减轻难度并提升学习效果，实验显示其用户体验良好。


<details>
  <summary>Details</summary>
Motivation: 学生在编程语言和形式逻辑课程中，构建基于规则的推导树经常遇到困难，主要原因是推理规则应用复杂、缺乏即时反馈以及手写证明需要大量手工操作。

Method: 本文提出并开发了Hazel Deriver，这是一款基于Web的、交互式推导树编辑器，内建于Hazel live programming环境，采用多层次支架式支持，提供结构化和实时反馈的推导树构建体验，并进行了初步用户调研。

Result: 初步用户研究显示，Hazel Deriver能降低推导任务的主观难度，提高学生对推导树概念的理解以及学习的积极性。

Conclusion: Hazel Deriver通过层次化支架有效帮助学生完成推导树的构建，在降低难度和提升学习成效方面表现突出，并针对如何平衡系统引导与学习者自主性提出了研究问题。

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [8] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: 本文提出了λ_{QC}，首次支持进程名及集合多态、增强表达能力的编排式语言，并保证类型安全和无死锁，所有理论结果均通过机械化验证。


<details>
  <summary>Details</summary>
Motivation: 现有的编排式编程语言虽为并发系统开发带来便利，但缺乏现代系统所需的一些关键特性，比如动态分配和传播计算节点的能力。

Method: 提出了λ_{QC}，一种支持一等进程名和类型/节点位置多态的编排式语言，并支持代数与递归数据类型以及多节点值。该语言采用形式化定义，并在Rocq中机械化验证。

Result: λ_{QC}能够实现动态流程分配与传播，提升了表达能力，并保障无死锁等编排式程序的性质。所有结果均经机械化形式验证。

Conclusion: λ_{QC}弥补了现有编排式语言的不足，首次实现了带有进程名多态和节点集合多态的类型安全编排式语言，并扩展了表达能力和理论保障。

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: 论文提出了多模态数据驱动的微服务事故管理系统TrioXpert，借助大语言模型同时提升了异常检测、故障分级与根因定位任务的准确率并增强了解释性，实验效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模微服务系统事故管理方法通常仅依赖单一模态数据（如监控指标、日志、追踪信息），难以同时处理异常检测、故障分级和根因定位等多任务，同时解释性不足。

Method: 提出TrioXpert端到端框架，充分利用多模态数据，针对不同数据模态设计独立处理流程，并协同利用大语言模型进行多任务推理，提供清晰的推理证据。

Result: 在两个主流微服务系统数据集上评估，TrioXpert在异常检测、故障分级和根因定位上取得了显著的性能提升（提升幅度分别为4.7%-57.7%、2.1%-40.6%、1.6%-163.1%）。

Conclusion: TrioXpert能够高效融合多模态数据，显著提升多项事故管理任务表现，并具备优良的可解释性。

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [10] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: 本文提出一种结合增量流程发现和在线机器学习的流式流程仿真发现方法，更好适应业务流程的动态变化，提升了仿真模型的稳定性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着商业环境的动态变化，企业不断优化其业务流程以提升效率、降低成本和提高客户满意度。现有的流程仿真模型自动发现技术难以适应这种实时的流程变化。

Method: 提出了一种流式流程仿真模型发现方法，将增量式流程发现与在线机器学习方法结合。该方法在适应最新数据的同时，保留历史信息，从而提升对流程动态变化的适应性。

Result: 通过在四个不同的事件日志上的实验，证明了在流程仿真中赋予最新数据更高权重且保留历史知识的重要性。该方法生成的仿真更加稳定，在处理概念漂移时表现出更强的鲁棒性。

Conclusion: 所提方法能更好适应不断演化的业务流程，在稳定性和对概念漂移的鲁棒性方面优于传统方法。

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [11] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: GitHub Copilot能大幅提升学生在旧代码基础上开发时的效率，但学生对其建议缺乏深刻理解，教学应平衡效率和理解。


<details>
  <summary>Details</summary>
Motivation: 目前软件行业中新进开发者大多需要在旧有代码基础上进行开发（brownfield development），而生成式AI助理（如GitHub Copilot）正在改变开发实践，但其对学生开发者在继承型开发任务上的影响鲜有研究。

Method: 采用对照实验，邀请10名本科计算机专业学生，在一个遗留Web应用中，分别在有和无Copilot的情况下完成类似的开发任务。通过性能分析、行为分析及访谈对比两种情境下的表现。

Result: 使用Copilot时，学生完成任务速度提高35%，方案进展提升50%，手写代码时间减少11%，网络搜索时间减少12%（以上结果p<0.05均具统计意义）；但学生对Copilot建议背后的原理理解存疑。

Conclusion: 生成式AI助理在加速学生继承型开发任务、提升效率方面有显著作用，但可能削弱学生对代码理解的深入。教育者应开发新型教学法，既利用AI优势，也注重学生对AI建议本质的反思与理解。

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [12] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: 作者系统性分析编码任务中验证机制的权衡，发现ORM能大幅提升验证速度，经过修剪后再排序的流程在保证较高准确率下显著提高效率，为大规模代码生成系统设计提供了参考。


<details>
  <summary>Details</summary>
Motivation: 当前主流观点认为，优先采用全面验证器优于只用结果奖励模型，但很少讨论两者之间的权衡。作者希望挑战这一假设，系统性探索在代码生成任务中验证速度与准确率的折中。

Method: 系统性地比较了使用ORM和全面验证器在生成、修剪和排序阶段上的速度与准确率权衡，并提出了generate-prune-then-rank新流程。

Result: 提出的generate-prune-then-rank方法，比仅使用全面验证器快11.65倍，只损失8.33%的准确率。此外，该方法可有效过滤掉高排名但错误的解答。

Conclusion: 研究表明，即使在具备全面验证器的情况下，基于结果的奖励模型（ORM）通过提高验证速度，在大规模代码生成任务中依然具有重要意义。

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [13] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: 该论文针对LLM生成代码对Prompt和用户背景敏感的问题，提出了通用且有效的评测方法，并通过实验验证其实用性，工具已开放共享，便于社区采纳。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在代码生成领域应用广泛，但生成代码的质量依赖于用户输入的Prompt质量，且受用户背景和编程经验影响明显。因此需要评估和量化LLM对不同输入变化的敏感度。

Method: 提出了一套合成评测流程以及系统化以用户角色为基础的评测方法，用于测试和揭示LLM所生成代码的质量如何随不同用户背景产生变化。这些方法不依赖具体编程任务及特定LLM，实现通用化评估。

Result: 实验显示，这些方法能够有效评估和揭示LLM代码生成对输入变化的敏感性，并定量区分不同用户背景下的响应差异。相关评测工具已开源，促进社区使用。

Conclusion: 提出的评测方法通用、有效，可以帮助更全面地理解和改进LLM代码生成在实际应用中的适应性和鲁棒性。

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [14] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: TrioXpert框架通过多模态数据与LLMs协同推理，实现了微服务事件管理三项任务的性能和可解释性大幅提升。


<details>
  <summary>Details</summary>
Motivation: 当前大规模微服务系统中的自动化事件管理主要依赖单一模态数据，难以同时解决如异常检测、故障分诊和根因定位等多项下游任务。此外，现有技术缺乏清晰的推理证据，导致可解释性不足。

Method: 提出了TrioXpert框架：一个端到端的事件管理方案，通过三条独立的数据处理流水线全面利用多模态数据（如数值和文本），并利用大型语言模型（LLMs）协同推理机制来同时处理多个任务，并输出推理证据以增强可解释性。

Result: 在两个流行微服务系统数据集上，TrioXpert在异常检测、故障分诊和根因定位这三项任务上均取得了显著提升，性能增幅分别为4.7%~57.7%、2.1%~40.6%、1.6%~163.1%。

Conclusion: TrioXpert能够高效集成多模态数据，协同完成事件检测、分诊和定位三大任务，并能输出充分的推理证据，相比现有方案达到更优性能和可解释性。

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [15] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: 本文针对现有流程仿真自动建模对实时变化适应性差的问题，提出了结合增量挖掘和在线学习的流式技术，能优先响应最新流程变化并保留历史知识，从而提升仿真模型的稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的业务流程仿真自动建模方法难以适应实时业务环境中的操作变化，而企业流程常常需要根据新情况调整以提高效率和客户满意度。

Method: 提出了一种结合增量流程发现与在线机器学习的方法，用于流式流程仿真建模。该方法在优先考虑最新数据的同时，也保留历史信息，以适应不断变化的流程动态。

Result: 在四个不同事件日志上的实验显示，给予最新数据更高权重并保留历史知识对于仿真建模非常重要。所提方法不仅生成了更稳定的仿真模型，还能有效应对概念漂移，在实际用例中表现出较强的鲁棒性。

Conclusion: 本文提出的融合增量流程发现与在线机器学习的流式流程仿真建模技术，能有效适应动态业务环境，实现更稳定和鲁棒的仿真建模。

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [16] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: GitHub Copilot显著提高学生在旧代码任务中的效率和进展，但导致理解代码原理的能力下降，呼吁教育者创新教学方法以充分利用AI助手的优劣势。


<details>
  <summary>Details</summary>
Motivation: 虽然AI编程助手（如GitHub Copilot）正在迅速改变软件开发实践，但有关其在学生进行旧代码开发（brownfield development）时的影响还未被充分研究。鉴于计算专业毕业生多将面对遗留代码，了解GenAI对学生在这种情境下的表现和行为影响，有助于优化教育和工具设计。

Method: 本研究对10名本科计算机专业学生进行了对照实验，让他们在一个遗留Web应用中分别使用和不使用Copilot完成相似的旧代码开发任务。采用了混合方法，包括性能分析、行为分析以及离场访谈。

Result: 结果显示，使用Copilot时，学生完成任务速度提升35%，方案进展快50%，手写代码时间减少11%，网页搜索时间减少12%，这些均具有统计学意义。此外，学生反映使用Copilot后对建议代码的原理和合理性理解变弱。

Conclusion: GitHub Copilot等GenAI助手能显著提升学生在遗留代码任务中的开发效率和进展，但也带来了理解和认知上的新挑战。因此，教育者需制定新教学方法，既能利用GenAI提高效率，又能促进学生对AI建议的反思和理解。

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [17] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: 论文表明结合快但不完全准确的验证器和全面验证器，采用生成-筛除-再排序的方法，能显著加速编码任务的验证过程且精度损失有限，对设计高效的大模型程序排名系统有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前利用大语言模型（LLM）解决编程任务的方法通常采用生成-再排序（generate-then-rank）方案，其中排序环节多依赖于全面的验证器。但业界普遍忽略了速度与准确性的权衡，往往优先考虑全面验证器，而对结果回报模型（ORM）的作用重视不足。本文旨在探讨两者的效益权衡。

Method: 系统性地对比ORM和全面验证器在速度与准确性方面的表现，并提出并评估一种generate-prune-then-rank方法，该方法先用较快但准确率低的验证器剔除明显错误的解，再用全面验证器进行排序。

Result: generate-prune-then-rank方法，使系统速度提升11.65倍，仅以8.33%准确率损失为代价。这一方法通过在排序前筛除高排名但错误的解，有效权衡了准确性与效率。

Conclusion: 联合使用ORM和全面验证器，并采用generate-prune-then-rank流程，可以在大幅提升系统速度的同时，将准确率损失控制在较低水平。该方法有助于构建高效且可扩展的程序排序系统。

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [18] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: 本研究提出了两种与具体编程任务和模型无关的代码生成评价新方法，证明其能够有效揭示LLM对于不同用户背景和输入的敏感性，并已开放源代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）正在广泛应用于代码生成领域，但生成结果高度依赖于用户提供的提示（prompt），尤其受到用户开发背景和编程熟悉程度的影响。现有评估方式难以全面分析这种敏感性。

Method: 本文提出了一种合成评价流程和以用户角色（persona）为基础的系统性评价方法，用于评估LLMs在代码生成任务中对输入变化的敏感性。该方法不依赖于具体任务或特定模型，适用于广泛场景。

Result: 实验表明所提出的方法能够有效揭示不同用户背景下LLM输出的质性差异，并具备实际应用价值。作者还公开了相关代码，促进社区共享和进一步研究。

Conclusion: 通过该通用合成与用户角色评价方法，可以更准确地量化和理解LLM代码生成对用户输入变化的敏感性，对后续模型改进及实际应用具有重要指导意义。

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [19] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: HyperRes是一种超图形式化系统，实现了不同包管理器之间的依赖解析，无需更换现有工具，显著增强了多语言、多生态环境下的依赖管理。


<details>
  <summary>Details</summary>
Motivation: 当前各种编程语言和操作系统都有自己的包管理器，缺乏互操作性，导致多语言项目难以精确表达跨生态系统的依赖，且外部系统及硬件依赖通常隐式且无版本控制。

Method: 提出了HyperRes，一种利用超图对版本化依赖关系进行形式化建模的系统，能够表达多个生态系统并跨生态解决依赖约束。同时，定义了将众多现有包管理器转换为HyperRes的方式。

Result: 通过将数十种现有包管理器转换到HyperRes，并验证了依赖解析在当前各自独立的生态系统间也可以顺利进行。

Conclusion: HyperRes无需用户更换现有包管理器，可实现生态系统之间包元数据的转换，并支持根据特定部署环境精确求解依赖，提升了跨生态系统的依赖解析能力。

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [20] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: 本综述全面梳理了2018-2023年间软件漏洞检测的AI方法现状与不足，提出未来可重点关注新型技术路线。


<details>
  <summary>Details</summary>
Motivation: 源代码中的软件漏洞带来严重的网络安全风险，传统检测方法已难以满足需求，促使学界转向依赖AI的新方法。

Method: 本研究对2018至2023年间的软件漏洞检测（SVD）相关文献进行了系统性综述，涵盖技术分类、特征表示及嵌入方法。

Result: 分析发现91%的研究采用AI方法，其中基于图的模型最为常见。此外，本研究指出现有研究在数据集质量、可复现性和可解释性方面存在关键限制。

Conclusion: 论文归纳了SVD领域的研究现状，总结不足并指出联邦学习、量子神经网络等新兴方法的研究机会，为未来研究指明方向。

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [21] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: 针对静态分析误报高的问题，作者提出了基于LLM agent的新架构LLM4PFA，有效降低错误报警并保持高漏洞检测率，在实验中大幅优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有静态漏洞分析器在检测大型代码库漏洞时，常因路径可行性验证能力有限（受多条件分支和复杂数据依赖影响），导致误报率高。虽然基于大模型（LLM）的方法尝试解决该问题，但因约束级联分析不足及在大型项目中的可扩展性受限，效果有限。

Method: 提出一种迭代路径可行性分析框架LLM4PFA，基于LLM agent进行有针对性的约束推理，并结合agent计划驱动的关键上下文感知分析，从而提升复杂跨过程的路径可行性判断，减少静态漏洞检测误报。

Result: LLM4PFA能精准过滤掉72%到96%的静态漏洞误报，较基线方法提升41.1%到105.7%；同时，在45个真实漏洞中仅漏报3个。

Conclusion: LLM4PFA显著减少了大型代码库静态检测的误报率，并兼顾真实漏洞的发现能力，明显优于现有方法。

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [22] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 论文探索了如何结合LLMs、静态分析框架与RAG，自动发现并高质量修复大规模代码库中的各种问题，通过引入自定义代码比对应用，有效抑制了模型幻觉带来的错误，最终实现了大幅提升代码质量与开发效率。


<details>
  <summary>Details</summary>
Motivation: 随着软件项目规模的扩大，代码中的缺陷、漏洞与代码异味问题日益突出，自动化检测和修复成为提升开发效率和代码质量的重要需求。近年来，LLMs（如GPT-3.5 Turbo和GPT-4o）在代码处理方面表现出潜力，本研究旨在探讨如何将这些模型更有效地集成进现代软件开发流程，实现代码问题的自动检测和修复。

Method: 本文构建了一个静态代码分析框架，能够检测出大型软件项目中的代码缺陷、漏洞和代码异味等问题，并详细提取和组织问题信息，为后续的自动修复提供数据基础。修复阶段应用了LLM，并通过迭代优化提示词（prompt engineering）确保输出准确、结构化。为增强修改的相关性与精度，采用了RAG（检索增强生成）。此外，通过自研的“代码比对应用”筛查和纠正LLM潜在的错误输出，防止错误修正被应用到代码库中。

Result: 结合LLM、静态分析与RAG，在多轮自动化修复后，代码中的问题显著减少。结果表明该方法能够切实提升代码质量，降低开发成本，并优化软件开发流程。

Conclusion: 将大语言模型与静态代码分析、检索增强技术结合，能够实现高效且高质量的代码问题自动检测与修复，极大提高了大型软件项目的开发效率和代码质量。

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [23] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: 本文提出了AutoGEEval++，首个可自动评估在Google Earth Engine上地理空间代码生成的LLM系统。该框架提供大规模多维度测试集，系统分析并对比了24种模型，为领域代码自动生成的评价树立了标准。


<details>
  <summary>Details</summary>
Motivation: 地理空间代码生成是人工智能与地球科学分析融合的重要前沿，但在该领域缺乏标准化自动评价工具。研究动机是弥补这一空白，为大语言模型（LLMs）在Google Earth Engine（GEE）上的地理空间代码生成提供统一和自动的评估机制。

Method: 本文提出AutoGEEval++框架，基于GEE Python API，构建了包含6,365个测试用例、覆盖26种数据类型和三类任务的基准数据集。整个系统包括代码生成与基于执行的验证流程、度量多维性能指标如准确率、资源占用、运行效率和错误类型，支持边界测试和错误模式分析，并对24种主流LLM进行了系统评估。

Result: 实验揭示了在不同任务类型、模型设计和部署环境下，24种LLM在性能、稳定性和错误方面存在显著差异，验证了AutoGEEval++的实用性和可扩展性。

Conclusion: 本研究首次建立了GEE领域LLM代码生成的标准化评估协议和基础性基准，为垂直领域代码生成提供了统一的性能比较基础和系统性评价方法。

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [24] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: 提出了一种新颖的LayoutCoder框架，大幅提升了网页UI图像到代码的自动生成效果，并构建了更真实的新数据集用于评测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习UI2Code方法对大量标注数据依赖性强，且对新颖网页布局泛化能力不足；新兴MLLM在复杂布局理解和代码生成上也有短板。

Method: 设计了基于多模态大模型(MLLM)的LayoutCoder方法，包括三大模块：组件关系构建、UI布局解析、布局引导的代码生成。此外构建了Snap2Code基准数据集，用于科学评估。

Result: LayoutCoder在BLEU和CLIP分数上分别比最佳基线提升了10.14%和3.95%，验证了其对实际UI图像到代码任务的有效性。

Conclusion: LayoutCoder显著提升了UI图像到代码的自动转换准确率，在多个数据集上都优于现有最先进方法。

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [25] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 提出了适用于量子软件的缺陷分类自动化工具，在大样本实验中表现出高准确性和较好一致性，对量子软件质量研究及实际改进具有实用价值，但在严重性判断上仍有优化空间。


<details>
  <summary>Details</summary>
Motivation: 量子软件持续发展，但其独有的缺陷类型尚缺统一、自动的分门别类工具，限制了对其质量和可靠性的系统性分析与提升，因此迫切需要面向量子软件的自动缺陷分类机制。

Method: 作者设计了一套针对量子计算软件缺陷的关键词与启发式自动分类方法，并与专家人工标注进行对比，通过准确率、精确率、召回率与F1分数评估表现，同时利用统计方法（如t检验、Cohen's Kappa）对自动与人工分类结果一致性进行分析。

Result: 自动分类框架在主流维度（如缺陷类型、类别、受影响质量属性）上与人工标注具有较高一致性（Cohen’s Kappa均＞0.69），总体分类准确率高达85.21%。但在严重性维度上表现较弱（k=0.162）。量子相关缺陷占比27.3%，以电路级、门操作和硬件相关问题为主，大部分问题为低严重性。

Conclusion: 本论文提出的基于规则的自动化量子软件缺陷分类框架在实际应用中表现良好，能够有效识别不同类型与类别的缺陷，尤其是量子相关缺陷。虽然在‘严重性’类别的分类上表现有待提升，但整体上能够为量子软件质量分析和改进提供支撑。

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [26] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: 本文系统分析了主流分布式深度学习框架的常见bug类型及修复方式，发现接近一半的bug可通过少量代码更改修复，具备自动化处理潜力，并对提升框架可靠性和开发自动化调试工具提出建设性建议。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的迅速发展，分布式训练和推理框架（如DeepSpeed）在多GPU或节点上的模型扩展变得不可或缺。然而，这些复杂的框架也带来了复杂的bug问题，影响训练性能，导致故障和资源浪费。因此，理解这些框架bug的特性对保证框架质量、设计更有效的调试和修复方法具有基础性意义。

Method: 本文首次对三大神经网络分布式框架（DeepSpeed、Megatron-LM、Colossal-AI）的308个已修复bug进行了大规模实证分析。分析内容包括：bug症状、根本原因、bug识别与修复所需的努力程度，以及常见的低成本修复方案。

Result: 1）分布式特性导致独特的bug成因，如分配策略错误和分布式通信错误。2）诊断和修复复杂bug存在诸多挑战，包括症状与根因的不对齐、重现成本高、组件间的交互复杂等。3）约48%的bug修复仅需极少代码修改（<=10行），多采用简单策略，实现条件逻辑优化、参数处理加强或版本兼容，具备自动化潜力。

Conclusion: 通过对分布式训练与推理框架bug的深入分析，本文总结出提升框架及其上层LLM项目可靠性的多项启示，并提出利用LLM工具实现自动化调试和修复的可能方向。

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [27] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: 提出的ExpeRepair系统通过融合情节和语义记忆，实现基于经验、动态适应的软件自动修复，实验效果领先现有开源方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的软件自动修复方法存在两大局限：一是倾向于孤立处理问题，未利用历史修复经验，二是依赖静态、僵化的提示策略，难以适应复杂多变的问题。

Method: 受到人类认知中双重记忆系统的启发，提出ExpeRepair框架。该方法将历史修复经验组织为情节记忆（存储具体事例）和语义记忆（抽象经验总结），推理时动态检索并组合这两类记忆，通过构建基于经验的动态提示提升修复能力。

Result: 在SWE-bench Lite基准上的实验显示，采用Claude 3.7 Sonnet的ExpeRepair在pass@1指标上取得49.3%的成绩，优于所有开源的最新方法。

Conclusion: ExpeRepair能够通过双通道知识积累与动态提示策略，显著提升大模型在软件自动修复任务的性能，克服了现有方法的两大局限。

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [28] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: 本文提出了一种基于大模型的自动化bug生成和验证工具BugGen，相比传统手段在准确率、效率和功能复杂度上具有显著优势，为硬件验证和ML调试带来高质量数据支撑。


<details>
  <summary>Details</summary>
Motivation: 硬件系统的日益复杂给验证工作带来巨大压力，现有的人工或自动化插入bug方法无法高效地产生多样化且具有规模化的bug数据集，限制了基于机器学习的调试能力。

Method: 提出BugGen：一种基于大型语言模型（LLMs）、完全自治的多智能体流程，能系统地生成、插入并验证RTL中的真实功能性bug。其流程包括模块划分、通过闭环智能体架构选择变异目标，并通过迭代优化和回滚确保语法正确性及功能可检测性。

Result: 在五个OpenTitan IP模块上，BugGen成功生成了500个独特的bug，具有94%的功能性准确率，验证速度比人工快五倍。BugGen还发现了104个OpenTitan未被检测到的bug，表明其能发现验证覆盖盲区。对比Certitude工具，BugGen在语法准确性、发现测试盲点和bug场景复杂度显著更优。使用BugGen数据集训练的ML模型在不同IP模块上分类准确率达88.1%~93.2%。

Conclusion: BugGen是一种可扩展、高质量的自动化bug数据生成方案，显著提升了硬件验证和基于机器学习的调试效率。

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [29] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: AdaptiveLLM通过自动难度评估方法，自适应选择不同大型语言模型进行代码生成，大幅提升准确率并降低成本，相较人工评价选模更高效、实用。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在代码生成方面表现优异，但在任务效率与计算成本之间存在权衡。如何动态选择最合适的模型以兼顾性能和成本，特别是在实际场景下难以获取人工标注的任务难度标签，是一个有待解决的问题。

Method: 提出了AdaptiveLLM框架：首先通过推理模型生成的Chain-of-Thought（CoT）长度自动评估任务难度；使用k-means聚类将任务难度分为三个等级；结合CodeBERT微调得到包含难度特征的嵌入；最终通过训练XGBoost分类器为每个问题选择最优模型，实现效率与性能的平衡。

Result: AdaptiveLLM相较于基线方法ComplexityNet，pass@1得分提升7.86%，资源消耗降低88.9%；相比使用单一模型，准确率提升约15%，且成本持平。基于CoT的难度评估比人工评价更可靠。

Conclusion: AdaptiveLLM框架通过自动难度评估与模型选择，有效提升了代码生成任务的效率与性能，显著降低资源成本，并简化了实际部署难度。

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [30] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: 提出用容器化和开源虚拟平台技术，在云端实现硬件/软件联合开发与测试，降低环境依赖和成本，加速开发周期，已通过AI加速器案例验证有效。


<details>
  <summary>Details</summary>
Motivation: 硬件/软件系统（HW/SW）的复杂性不断提升，特别是在汽车等安全关键领域，测试需求极高。然而，硬件常常供不应求，导致早期软件开发受阻。需要一种方法在没有完善硬件支持时，能提前进行软件开发和测试。

Method: 提出利用容器化技术封装基于SystemC TLM-2.0标准的虚拟平台（VP），实现环境依赖最小化和云端部署，从而加速并行化测试，并采用如QEMU和VCML等开源VP技术以免除额外授权费用。通过AI加速器VP案例进行验证。

Result: 通过AI加速器虚拟平台案例，验证了该方法可有效解决HW/SW协同开发过程中因系统复杂性带来的挑战，实现了提前测试和加速开发流程。

Conclusion: 容器化包裹虚拟平台并结合开源技术，为硬件/软件协同开发提供了低依赖性、高效率且经济实用的测试解决方案，提升了开发速度和灵活性，特别适用于安全关键领域。

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [31] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: 代码审查中，审查者经常按自身策略选择非字母顺序浏览文件，应优化工具以提升审查效率，特别是针对大型PR。


<details>
  <summary>Details</summary>
Motivation: 在代码审查工具（如GitHub Pull Request）中，变更文件通常按字母顺序展示。但这种顺序并不一定符合审查者的实际审查顺序和偏好。作者希望揭示审查者在评论PR变更时实际遵循的文件浏览顺序，并分析这些顺序背后的策略和影响。

Method: 作者通过数据挖掘，收集并分析了100个流行Java与Python开源仓库中23241个pull request的代码审查评论，统计并归纳出评论顺序与文件排列顺序的关系，进一步识别并量化了不同的导航策略（如最大差异优先、与标题描述最相似、测试文件优先等）的存在及其分布比例。

Result: 44.6%的pull requests中，审查者的评论顺序并非字母顺序。其中20.6%遵循最大差异优先原则，17.6%与PR标题和描述相符，29%涉及生产与测试文件的PR采用了测试优先原则。此外，采用非字母顺序的审查，其被审查的文件占比更高，但平均收到的通过数略低。

Conclusion: 开发者在代码审查过程中，往往会根据实际需求采用多样化、复杂的浏览顺序，而非简单的字母排序。尤其是PR规模越大，越倾向于采用复杂策略。当前的工具支持有限，研究建议针对大规模pull request改进工具以适应不同审查顺序的需求。

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [32] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本项目VERIFAI探讨用NLP、本体、复用及大语言模型等技术，自动化实现自然语言需求到形式化规范的可追踪与验证，旨在提升软件开发中需求的准确性与验证效率。


<details>
  <summary>Details</summary>
Motivation: 在实际软件开发中，需求从自然语言向形式化规格的转化和全过程的可追踪性与验证具有极大挑战，当前手工方法效率低、易出错，亟需自动化工具支持。

Method: 结合自然语言处理（NLP）、软件领域本体、基于相似度的软件构件复用、大语言模型以及人工智能辅助，自动生成可追踪的形式化规格说明。

Result: 项目提出了综合利用NLP、本体、复用及AI的方法论，目前正初步开展技术探索并着手实现自动化生成和追踪机制。

Conclusion: 该项目旨在通过NLP、本体、经验复用和大语言模型等技术为需求可追踪性与形式化验证提供自动化支持，提升软件从需求到实现及验证阶段的准确性与高效性。

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [33] [Growing a Modular Framework for Modal Systems- HOLMS: a HOL Light Library](https://arxiv.org/abs/2506.10048)
*Antonella Bilotta*

Main category: cs.LO

TL;DR: 论文介绍了HOLMS——一个在HOL Light上进行多模态系统模块化证明的库，实现了多个模态系统的适当性定理证明及自动化工具，验证了在实际证明助手中机械化模态推理的可能性。


<details>
  <summary>Details</summary>
Motivation: 现有的模态逻辑机械化证明主要聚焦某一系统或缺乏模块化和统一框架，难以扩展至多个系统；论文希望通过HOLMS填补该空白，使多个模态系统下的推理能够模块化、统一且自动化。

Method: 作者首先介绍了模态逻辑基础及HOL Light使用方法，在框架中发展了统一且模块化的策略，直接在HOL Light内部针对多个标准模态系统（K, T, K4, GL）证明适当性定理。此外，集成自动判定过程与反例模型生成器，并评估与Boolos的《可证性逻辑》一书中的完整性证明之通用性与组合性。

Result: 论文成功在HOL Light中实现了对K、T、K4、GL等模态系统的适当性证明与反例自动生成；证明了标签序列演算及对应理论可与实际通用证明助手结合，切实推动了模态逻辑自动化证明工具的发展。

Conclusion: 该论文展示了在HOL Light证明助手中，机械化和模块化模态推理是可行且高效的。HOLMS库的发展为进一步的理论与实际应用拓展提供了坚实基础。

Abstract: The present dissertation introduces the research project on HOLMS
(\textbf{HOL} Light Library for \textbf{M}odal \textbf{S}ystems), a growing
modular framework for modal reasoning within the HOL Light proof assistant. To
provide an accessible introduction to the library, the fundamentals of modal
logic are outlined first, followed by a concise manual for the proof assistant
itself. The core contribution of this work on HOLMS is the development of a
unified and modular strategy for proving adequacy theorems with respect to
relational semantics directly within HOL Light for several normal modal
systems, currently including K, T, K4, and GL. Adequacy theorems establish a
formal connection between syntactic proof systems and their intended relational
models, ensuring that derivable statements align with valid ones. This approach
extends previous research on G\"odel-L\"ob logic (GL) by two HOLMS developers.
It also assesses the generality and compositionality of the completeness proofs
in George Boolos' monograph \textit{The logic of provability}. Beyond
theoretical contributions, HOLMS incorporates automated decision procedures and
a countermodel constructor for K, T, K4, and GL, illustrating how
general-purpose proof assistants can be effectively combined with research on
labelled sequent calculi and key insights from correspondence and bisimulation
theories. The implementation in HOL Light demonstrates the feasibility of
mechanising modal reasoning in a flexible and robust manner, paving the way for
further developments of the HOLMS framework.

</details>


### [34] [Notes on applicative matching logic](https://arxiv.org/abs/2506.10088)
*Laurentiu Leustean*

Main category: cs.LO

TL;DR: 本文系统介绍了函数式Match Logic（AML）的基本理论及重要结果，是AML理论的入门性资料，并为该领域的后续研究提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 传统的Matching Logic（ML）虽然被用于程序语言的形式语义和推理，但缺乏对函数式性质的直接支持。AML作为ML的函数式变体，有助于在函数式编程和相关理论中更好应用ML。

Method: 本文介绍了AML的基本定义和主要理论结果，整理为教程型讲义，适合作为AML理论入门教材。内容体系和风格深受Monk的数理逻辑教材影响。

Result: 梳理和总结了AML的核心理论内容，为后续研究和实际应用提供理论基础和入门资源。

Conclusion: AML丰富了ML体系，使其能更好支持函数式编程相关的推理与表达。讲义体例规范，为领域初学者提供了有价值的学习资料。

Abstract: Matching logic (ML) was developed by Grigore Ro\c{s}u and collaborators as a
logic for defining the formal semantics of programming languages and for
specifying and reasoning about the behavior of programs. These lecture notes
present basic definitions and results on applicative matching logic (AML), a
functional variant of ML introduced recently by Xiaohong Chen and Grigore
Ro\c{s}u. They can be used as an introductory text in the theory of AML. Monk's
textbook on mathematical logic has an enormous influence on the notes.

</details>


### [35] [StepProof: Step-by-step verification of natural language mathematical proofs](https://arxiv.org/abs/2506.10558)
*Xiaolin Hu,Qinghua Zhou,Bogdan Grechuk,Ivan Y. Tyukin*

Main category: cs.LO

TL;DR: 本文提出StepProof方法，实现对自然语言数学证明的逐句分解与验证，明显提升了验证成功率与效率，推动自动形式化技术取得细粒度突破。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明器（ITP）在形式化验证数学证明方面非常强大，但缺乏自然语言接口，限制了其易用性。虽然大语言模型（LLM）提升了自然语言理解能力，并使自动形式化成为可能，但现有方法多只停留在整体证明的验证，缺乏更细粒度、逐句的验证能力。

Method: 提出了一种新颖的自动形式化方法StepProof，将完整的证明拆解为多个可验证的子证明，从而实现逐句验证。同时，通过对自然语言证明进行细微人工调整，进一步优化自动形式化效果。

Result: 实验结果表明，StepProof在成功率和效率上都显著优于传统方法。对自然语言证明进行适当的手动调整还能进一步提升StepProof的表现。

Conclusion: StepProof弥补了当前自动形式化方法在细粒度验证上的不足，能够有效提升证明成功率和效率，并为自然语言到形式化证明的转换提供了更细致可靠的解决方案。

Abstract: Interactive theorem provers (ITPs) are powerful tools for the formal
verification of mathematical proofs down to the axiom level. However, their
lack of a natural language interface remains a significant limitation. Recent
advancements in large language models (LLMs) have enhanced the understanding of
natural language inputs, paving the way for autoformalization - the process of
translating natural language proofs into formal proofs that can be verified.
Despite these advancements, existing autoformalization approaches are limited
to verifying complete proofs and lack the capability for finer, sentence-level
verification. To address this gap, we propose StepProof, a novel
autoformalization method designed for granular, step-by-step verification.
StepProof breaks down complete proofs into multiple verifiable subproofs,
enabling sentence-level verification. Experimental results demonstrate that
StepProof significantly improves proof success rates and efficiency compared to
traditional methods. Additionally, we found that minor manual adjustments to
the natural language proofs, tailoring them for step-level verification,
further enhanced StepProof's performance in autoformalization.

</details>


### [36] [Growing a Modular Framework for Modal Systems- HOLMS: a HOL Light Library](https://arxiv.org/abs/2506.10048)
*Antonella Bilotta*

Main category: cs.LO

TL;DR: 本文提出HOLMS——一个可扩展的HOL Light模态系统库。实现了主流模态系统的适切性定理机械化证明，集成自动决策和反模型工具，推动了模态逻辑与证明助理的结合，有较大理论和实用意义。


<details>
  <summary>Details</summary>
Motivation: 推动在HOL Light证明助理中形式化模态逻辑系统，旨在为多种标准模态系统证明适切性定理提供统一且模块化的策略。该项目还旨在弥合语法推理与语义模型之间的联系，并进一步自动化模态推理过程。

Method: 首先，简述模态逻辑基本原理和HOL Light工具使用方法。核心工作是以统一和模块化的方式，在HOL Light中针对多个标准模态系统（如K、T、K4、GL）直接证明与关系语义相关的适切性定理。论文扩展了对哥德尔-勒布逻辑（GL）之前的相关研究，并借鉴了Boolos的有关可证性逻辑的完全性证明思路。此外，还实现了自动决策过程和反模型构造器，结合了标签序列演算和对应理论最新成果。

Result: 在HOL Light中成功实现了一个可扩展的HOLMS库，涵盖K、T、K4、GL等系统。证明了这些系统的适切性定理，展示了形式化和机械化模态推理的可行性。库内集成了自动化决策过程和反模型工具，显示出通用证明助理与模态逻辑前沿理论结合的广阔前景。

Conclusion: HOLMS项目为在HOL Light中形式化模态逻辑提供了完整、灵活、自动化的基础，推动了证明助理与理论模态逻辑深度融合的发展。该框架不仅具有理论创新，还为将来更多模态系统的机械化奠定了坚实基础。

Abstract: The present dissertation introduces the research project on HOLMS
(\textbf{HOL} Light Library for \textbf{M}odal \textbf{S}ystems), a growing
modular framework for modal reasoning within the HOL Light proof assistant. To
provide an accessible introduction to the library, the fundamentals of modal
logic are outlined first, followed by a concise manual for the proof assistant
itself. The core contribution of this work on HOLMS is the development of a
unified and modular strategy for proving adequacy theorems with respect to
relational semantics directly within HOL Light for several normal modal
systems, currently including K, T, K4, and GL. Adequacy theorems establish a
formal connection between syntactic proof systems and their intended relational
models, ensuring that derivable statements align with valid ones. This approach
extends previous research on G\"odel-L\"ob logic (GL) by two HOLMS developers.
It also assesses the generality and compositionality of the completeness proofs
in George Boolos' monograph \textit{The logic of provability}. Beyond
theoretical contributions, HOLMS incorporates automated decision procedures and
a countermodel constructor for K, T, K4, and GL, illustrating how
general-purpose proof assistants can be effectively combined with research on
labelled sequent calculi and key insights from correspondence and bisimulation
theories. The implementation in HOL Light demonstrates the feasibility of
mechanising modal reasoning in a flexible and robust manner, paving the way for
further developments of the HOLMS framework.

</details>


### [37] [Notes on applicative matching logic](https://arxiv.org/abs/2506.10088)
*Laurentiu Leustean*

Main category: cs.LO

TL;DR: 本文介绍了一种新的形式逻辑AML，并系统梳理了其基础理论内容，便于学习和应用。


<details>
  <summary>Details</summary>
Motivation: 介绍Matching Logic (ML) 及其用于程序语言形式语义定义、程序行为规范和推理的作用。同时提出了新的具有函数特性的变体——Applicative Matching Logic (AML)，希望为研究AML提供基础理论材料。

Method: 介绍AML的基本定义和主要结果，结合讲义形式，借鉴了Monk在数理逻辑教材中的方法与结构，通过理论讲解和定义，系统梳理AML的理论体系。

Result: 系统性地讲解了AML的基础内容，提供了入门级别的全面理论支持，便于进一步学习和研究AML。

Conclusion: 本文作为AML理论介绍材料，有助于读者系统掌握AML基础，为AML的进一步应用和研究奠定理论基础。

Abstract: Matching logic (ML) was developed by Grigore Ro\c{s}u and collaborators as a
logic for defining the formal semantics of programming languages and for
specifying and reasoning about the behavior of programs. These lecture notes
present basic definitions and results on applicative matching logic (AML), a
functional variant of ML introduced recently by Xiaohong Chen and Grigore
Ro\c{s}u. They can be used as an introductory text in the theory of AML. Monk's
textbook on mathematical logic has an enormous influence on the notes.

</details>


### [38] [StepProof: Step-by-step verification of natural language mathematical proofs](https://arxiv.org/abs/2506.10558)
*Xiaolin Hu,Qinghua Zhou,Bogdan Grechuk,Ivan Y. Tyukin*

Main category: cs.LO

TL;DR: 本文提出StepProof方法，将自然语言证明分解为可验证子证明，实现句子级别自动形式化与验证，显著提升了成功率和效率，且可通过少量人工调整进一步优化效果。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明器（ITP）虽然能实现数学证明的形式化验证，但由于缺乏自然语言接口，限制了其易用性。现有的自动形式化方法虽能将自然语言证明转为可验证的形式化证明，但仅支持整体证明的验证，缺乏更细粒度的句子级验证能力。

Method: 提出了一种新的自动形式化方法StepProof，将完整证明分解为多个可验证的子证明，从而实现逐步、句子级别的验证。还考察了针对逐步验证而对自然证明略作人工调整的效果。

Result: 实验表明，StepProof方法在提高证明成功率和效率方面较传统方法有显著提升。对自然语言证明进行小幅人工调整后，StepProof的性能进一步增强。

Conclusion: StepProof实现了自然语言证明向形式化证明的高效、细粒度自动转化，大幅提升了验证成功率和效率，并可通过少量人工干预进一步优化。

Abstract: Interactive theorem provers (ITPs) are powerful tools for the formal
verification of mathematical proofs down to the axiom level. However, their
lack of a natural language interface remains a significant limitation. Recent
advancements in large language models (LLMs) have enhanced the understanding of
natural language inputs, paving the way for autoformalization - the process of
translating natural language proofs into formal proofs that can be verified.
Despite these advancements, existing autoformalization approaches are limited
to verifying complete proofs and lack the capability for finer, sentence-level
verification. To address this gap, we propose StepProof, a novel
autoformalization method designed for granular, step-by-step verification.
StepProof breaks down complete proofs into multiple verifiable subproofs,
enabling sentence-level verification. Experimental results demonstrate that
StepProof significantly improves proof success rates and efficiency compared to
traditional methods. Additionally, we found that minor manual adjustments to
the natural language proofs, tailoring them for step-level verification,
further enhanced StepProof's performance in autoformalization.

</details>


### [39] [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
*Benjamin Bennetzen,Nikolaj Rossander Kristensen,Peter Buus Steffensen*

Main category: cs.LO

TL;DR: 该论文提出了CBPV到pi演算的编码方法，证明其理论可靠性与符合公认标准，并进行了部分Coq形式化，为编码研究和形式化验证打下基础。


<details>
  <summary>Details</summary>
Motivation: 推导和验证lambda演算（CBPV）到pi演算的编码，这类编码对理解过程计算和函数计算的联系具有重要意义。研究动因包括提供全面的理论基础，保证编码的严格性和正确性。

Method: 提出CBPV到pi演算（特别是pi-i-calculus）的形式化编码方法，给出手工证明编码的正确性（soundness）和完备性（completeness），分析编码特性，使用Coq进行部分正式化证明，同时讨论所用引理的合理性。

Result: 提出的编码在理论上是可靠的，具备soundness和completeness，且满足Gorla提出的优质编码五原则。编码适用于多种pi演算变体，并初步实现了编码正确性的Coq形式化，但部分引理仅为手工证明。

Conclusion: 该研究为CBPV到pi演算的编码提供了详细机制和理论保证，为将来进一步的自动化验证和应用建立了基础。所提出的方法经过形式化及手工推导，理论上完备可靠，并满足编码领域的公认标准。

Abstract: In this report we define an encoding of Levys call-by-push-value
lambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both
sound and complete. We present informal (by-hand) proofs of soundness,
completeness, and all required lemmas. The encoding is specialized to the
internal pi-calculus (pi-i-calculus) to circumvent certain challenges
associated with using de Bruijn index in a formalization, and it also helps
with bisimulation as early-, late- and open-bisimulation coincide in this
setting, furthermore bisimulation is a congruence. Additionally, we argue that
our encoding also satisfies the five criteria for good encodings proposed by
Gorla, as well as show similarities between Milners and our encoding. This
paper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic
pi-calculus and the local pi-calculus. We begin a formalization of the proof in
Coq for the soundness and completeness of the encoding in the pi-i-calculus.
Not all lemmas used in the formalization are themselves formally proven.
However, we argue that the non-proven lemmas are reasonable, as they are proven
by hand, or amount to Coq formalities that are straightforward given informal
arguments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [40] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
*Tian Lan,Yang-Hao Zhou,Zi-Ao Ma,Fanshu Sun,Rui-Qing Sun,Junyu Luo,Rong-Cheng Tu,Heyan Huang,Chen Xu,Zhijing Wu,Xian-Ling Mao*

Main category: cs.CL

TL;DR: 本文梳理并归类了文本、图像、音频三大模态的自动评价方法，提出五种基本评价范式，建立统一框架，为跨模态评价和后续研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 深度学习推动了生成式AI在文本、图像和音频方面的能力，但自动评价这些内容的质量仍具挑战性。目前缺乏一个系统的框架，能全面组织和分类不同模态下的自动评价方法。

Method: 对现有文献进行全面回顾，提出并构建一个统一的自动评价方法分类体系。首先系统梳理文本生成的自动评价方法，然后将该框架扩展到图像和音频生成领域。

Result: 提出了一个涵盖文本、图像和音频自动评价方法的统一分类框架，并总结归纳出五个基本评价范式。证明该分类体系具有较好的通用性。

Conclusion: 通过该统一框架，有助于未来自动评价方法的发展，尤其强调了跨模态评价方法的研究潜力。

Abstract: Recent advances in deep learning have significantly enhanced generative AI
capabilities across text, images, and audio. However, automatically evaluating
the quality of these generated outputs presents ongoing challenges. Although
numerous automatic evaluation methods exist, current research lacks a
systematic framework that comprehensively organizes these methods across text,
visual, and audio modalities. To address this issue, we present a comprehensive
review and a unified taxonomy of automatic evaluation methods for generated
content across all three modalities; We identify five fundamental paradigms
that characterize existing evaluation approaches across these domains. Our
analysis begins by examining evaluation methods for text generation, where
techniques are most mature. We then extend this framework to image and audio
generation, demonstrating its broad applicability. Finally, we discuss
promising directions for future research in cross-modal evaluation
methodologies.

</details>


### [41] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
*Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: TaskCraft自动化生成多工具交互、难度可变的agentic任务，显著提升模型训练和评估效率，推动AI agent任务研究发展。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言处理和AI中任务需要具备自主性、工具使用能力和自适应推理能力，但现有的数据集缺乏工具交互，现有评测也依赖人工标注，成本高、难以扩展。

Method: 提出了TaskCraft，一种自动化工作流，用于生成难度可控、多工具交互且可验证的agentic任务，并带有执行轨迹。TaskCraft利用深度和广度扩展原子任务，生成结构复杂的层次化挑战。

Result: TaskCraft生成了约36,000个不同难度的agentic任务组成的大规模合成数据集，实验证明这些任务提升了模型的prompt优化及agentic基础模型的有监督微调效果。

Conclusion: TaskCraft为agent模型的调优和评估提供了可扩展、高效的基准和数据，为未来agentic任务的研究奠定了基础。

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.

</details>


### [42] [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
*Christopher J. Agostino,Quan Le Thien,Molly Apsel,Denizhan Pak,Elina Lesyk,Ashabari Majumdar*

Main category: cs.CL

TL;DR: 论文指出自然语言存在不可避免的“语义退化”现象，随着表达复杂度提高，无论是人类还是LLM很难恢复唯一意义。实验采用类量子贝尔不等式证明语义解释具有非经典上下文相关性，挑战了传统语言观。作者建议用贝叶斯式采样方法描述语义，而非传统的频率学派分析。


<details>
  <summary>Details</summary>
Motivation: 随着语义表达复杂性增长，可能的解释数急剧增加，导致自然语言及其处理系统（如LLM）都面临语义解释歧义。该研究旨在探究复杂语义表达中意义恢复的可行性与限制，并检验经典语言观念的有效性。

Method: 理论分析基于Kolmogorov复杂度，搭配模拟贝尔不等式实验，利用多种大语言模型（LLM）在不同上下文下解释含糊词对，检测其解释是否超越经典边界。结果用CHSH期望值进行量化。

Result: 多次独立实验中，CHSH期望值平均在1.2到2.8之间，多次实验结果显著高于经典界限2（如2.3-2.4），显示解释过程具有非经典的上下文相关性。这结果与人类认知实验一致。

Conclusion: 该论文通过模拟贝尔不等式实验，发现无论是人类还是大型语言模型在解释复杂语义表达时，都会受到“语义退化”的限制，语义解释呈现出非经典的上下文相关性。因此，经典的认为语言形式本身蕴含固定意义的观点并不成立。

Abstract: Semantic degeneracy represents a fundamental property of natural language
that extends beyond simple polysemy to encompass the combinatorial explosion of
potential interpretations that emerges as semantic expressions increase in
complexity. Large Language Models (LLMs) and other modern NLP systems face
inherent limitations precisely because they operate within natural language
itself, making them subject to the same interpretive constraints imposed by
semantic degeneracy. In this work, we argue using Kolmogorov complexity that as
an expression's complexity grows, the likelihood of any interpreting agent
(human or LLM-powered AI) recovering the single intended meaning vanishes. This
computational intractability suggests the classical view that linguistic forms
possess meaning in and of themselves is flawed. We alternatively posit that
meaning is instead actualized through an observer-dependent interpretive act.
To test this, we conducted a semantic Bell inequality test using diverse LLM
agents as ``computational cognitive systems'' to interpret ambiguous word pairs
under varied contextual settings. Across several independent experiments, we
found average CHSH expectation values ranging from 1.2 to 2.8, with several
runs yielding values (e.g., 2.3-2.4) that significantly violate the classical
boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under
ambiguity can exhibit non-classical contextuality, consistent with results from
human cognition experiments. These results inherently imply that classical
frequentist-based analytical approaches for natural language are necessarily
lossy. Instead, we propose that Bayesian-style repeated sampling approaches can
provide more practically useful and appropriate characterizations of linguistic
meaning in context.

</details>


### [43] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
*Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel*

Main category: cs.CL

TL;DR: 本文提出了Chat-of-Thought多智能体系统，通过多角色LLM智能体协作显著提升工业FMEA文档的生成效率与质量。


<details>
  <summary>Details</summary>
Motivation: FMEA文件在工业资产监控中极为重要，但传统方法生成和验证过程耗时且容易出错，需要高效、智能化的解决方案。

Method: 采用多角色的LLM智能体，结合先进AI技术和动态任务分配，通过多智能体间的互动讨论和模板驱动工作流，实现FMEA文档的自动化和迭代完善。

Result: 实验展示了Chat-of-Thought在交互式、模板化、情境感知的FMEA生成方面的潜力，有效提升了文档的生成效率和质量。

Conclusion: Chat-of-Thought系统能够利用多智能体协作优化FMEA文档的生成和验证，特别适合工业设备监控领域，有效应对实际应用中的关键挑战。

Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,
designed to facilitate the generation of Failure Modes and Effects Analysis
(FMEA) documents for industrial assets. Chat-of-Thought employs multiple
collaborative Large Language Model (LLM)-based agents with specific roles,
leveraging advanced AI techniques and dynamic task routing to optimize the
generation and validation of FMEA tables. A key innovation in this system is
the introduction of a Chat of Thought, where dynamic, multi-persona-driven
discussions enable iterative refinement of content. This research explores the
application domain of industrial equipment monitoring, highlights key
challenges, and demonstrates the potential of Chat-of-Thought in addressing
these challenges through interactive, template-driven workflows and
context-aware agent collaboration.

</details>


### [44] [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
*Xiao Li,Joel Kreuzwieser,Alan Peters*

Main category: cs.CL

TL;DR: 大语言模型在语义相同但表述不同的提示下表现出明显不一致，PBSS框架揭示了这一点并分析了潜在的技术原因，提示模型评估标准需增加对提示稳定性的关注。


<details>
  <summary>Details</summary>
Motivation: 目前对于大语言模型（LLMs）的评估，忽略了同一语义但不同措辞方式（token-level realization）下模型输出不一致的问题。本文希望揭示这个被忽视的维度，并系统性分析原因。

Method: 提出了一种名为Prompt-Based Semantic Shift（PBSS）的诊断框架，用于量化大语言模型在语义等价但措辞不同的提示语下行为变化。该框架被应用于十个受限任务上，对比不同模型的表现。

Result: 实验发现，主流模型在语义等价但措辞变化的提示下会发生一致且具有模型特征的输出漂移。这些变化与分词方式和解码策略相关，显示出统计上的规律性。

Conclusion: 现有模型在提示语重新表述下的稳定性存在不足。分词和解码机制可能是导致训练后服务质量不稳定的原因，因此模型评估和改进需考虑这一维度。

Abstract: We investigate how large language models respond to prompts that differ only
in their token-level realization but preserve the same semantic intent, a
phenomenon we call prompt variance. We propose Prompt-Based Semantic Shift
(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under
semantically equivalent prompt rewordings. Applied to ten constrained tasks,
PBSS reveals consistent, model-specific response shifts, suggesting statistical
regularities linked to tokenization and decoding. These results highlight an
overlooked dimension of model evaluation stability under rephrasing and suggest
that tokenization strategies and decoding dynamics may contribute to
post-training quality of service instability.

</details>


### [45] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
*Tian Lan,Yang-Hao Zhou,Zi-Ao Ma,Fanshu Sun,Rui-Qing Sun,Junyu Luo,Rong-Cheng Tu,Heyan Huang,Chen Xu,Zhijing Wu,Xian-Ling Mao*

Main category: cs.CL

TL;DR: 本文系统性回顾了文本、图像、音频内容生成的自动评价方法，提出五大统一范式，并为跨模态评价未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在多模态内容生成方面进步显著，但相关生成内容的自动评价仍面临挑战，目前缺乏横跨文本、图像与音频的系统性整理与统一框架。

Method: 梳理并系统性地回顾了文本、图像和音频三大模态的自动评价方法，提出五大基本范式，并从文本生成评价起步，扩展到图像和音频生成。

Result: 总结并统一了三大模态生成内容的自动评价方法，构建了五大范式的统一分类体系，验证了其广泛适用性。

Conclusion: 本文提出了一个统一的自动评价方法分类体系，并讨论其在跨模态生成内容评价中的广泛适用性及未来研究方向。

Abstract: Recent advances in deep learning have significantly enhanced generative AI
capabilities across text, images, and audio. However, automatically evaluating
the quality of these generated outputs presents ongoing challenges. Although
numerous automatic evaluation methods exist, current research lacks a
systematic framework that comprehensively organizes these methods across text,
visual, and audio modalities. To address this issue, we present a comprehensive
review and a unified taxonomy of automatic evaluation methods for generated
content across all three modalities; We identify five fundamental paradigms
that characterize existing evaluation approaches across these domains. Our
analysis begins by examining evaluation methods for text generation, where
techniques are most mature. We then extend this framework to image and audio
generation, demonstrating its broad applicability. Finally, we discuss
promising directions for future research in cross-modal evaluation
methodologies.

</details>


### [46] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
*Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出TaskCraft自动生成多工具、多步骤agentic任务，大幅改善训练与评测效率，生成了大规模可验证数据集。


<details>
  <summary>Details</summary>
Motivation: 当前NLP/AI中的agentic任务日益重要，但现有数据缺少工具交互且大规模基准测试高度依赖人工、难以扩展，需要自动化且可扩展的任务生成方法。

Method: 提出TaskCraft自动化流程，采用基于深度和宽度的任务扩展生成复杂任务，并提供可执行的任务执行轨迹。通过实验证明其在Prompt优化与基础模型的监督微调中有效。

Result: 生成了约36,000个难度可控的合成agentic任务，用于提升和评估模型表现，实验证明这些任务对模型优化和微调有积极作用。

Conclusion: TaskCraft生成的多工具、多步骤的自动化Agentic任务，可以提升大型模型在多阶段任务推理和工具使用能力上的表现。

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.

</details>


### [47] [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
*Christopher J. Agostino,Quan Le Thien,Molly Apsel,Denizhan Pak,Elina Lesyk,Ashabari Majumdar*

Main category: cs.CL

TL;DR: 复杂语言表达下，语义唯一性难以保障，LLM和人类都难以还原唯一意图。实验显示语言理解表现出类似“量子纠缠般”的上下文性，不符合传统频率学方法，贝叶斯方法更适用于自然语言理解。


<details>
  <summary>Details</summary>
Motivation: 自然语言拥有大量语义歧义，表达复杂度提升后，单独的语言形式已难以唯一地表达其含义。作者希望阐述并量化这一现象，并寻求更适合语义建模的新方法。

Method: 作者利用Kolmogorov复杂度理论，对语言表达的复杂性和解释难度进行理论分析，并用语义Bell不等式实验（CHSH实验）测试多种LLM在不同语境下对多义词对的理解，考察其在不确定性和上下文影响下的非经典表现。

Result: 在系列实验中，LLM在多义性处理上的CHSH期望值多次显著超越经典界限（即|S|>2），显示出超越经典概率推理的上下文相关性。这与认知科学在人类实验中的发现一致，进一步验证自然语言解释具有量子式的复杂特性。

Conclusion: 论文得出结论：自然语言的语义解释在高度复杂情况下会表现出非经典的上下文相关性，传统的经典频率学派分析方法难以完整表达语言的多义性。因此，应该采用贝叶斯式的反复采样方法，更好地刻画语境中的语义。

Abstract: Semantic degeneracy represents a fundamental property of natural language
that extends beyond simple polysemy to encompass the combinatorial explosion of
potential interpretations that emerges as semantic expressions increase in
complexity. Large Language Models (LLMs) and other modern NLP systems face
inherent limitations precisely because they operate within natural language
itself, making them subject to the same interpretive constraints imposed by
semantic degeneracy. In this work, we argue using Kolmogorov complexity that as
an expression's complexity grows, the likelihood of any interpreting agent
(human or LLM-powered AI) recovering the single intended meaning vanishes. This
computational intractability suggests the classical view that linguistic forms
possess meaning in and of themselves is flawed. We alternatively posit that
meaning is instead actualized through an observer-dependent interpretive act.
To test this, we conducted a semantic Bell inequality test using diverse LLM
agents as ``computational cognitive systems'' to interpret ambiguous word pairs
under varied contextual settings. Across several independent experiments, we
found average CHSH expectation values ranging from 1.2 to 2.8, with several
runs yielding values (e.g., 2.3-2.4) that significantly violate the classical
boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under
ambiguity can exhibit non-classical contextuality, consistent with results from
human cognition experiments. These results inherently imply that classical
frequentist-based analytical approaches for natural language are necessarily
lossy. Instead, we propose that Bayesian-style repeated sampling approaches can
provide more practically useful and appropriate characterizations of linguistic
meaning in context.

</details>


### [48] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
*Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel*

Main category: cs.CL

TL;DR: 本文提出并验证了一种多智能体LLM协作系统Chat-of-Thought，在工业设备FMEA文档生成中提升了自动化与准确性，为智能制造领域带来新工具。


<details>
  <summary>Details</summary>
Motivation: 在工业设备管理中，FMEA分析文档的生成面临高复杂度和自动化难题，因此需要更高效、智能的方法来优化流程。

Method: 提出了一种多智能体系统Chat-of-Thought，通过多角色、基于LLM的代理智能体协作，采用动态任务分配和多角色嵌入式讨论优化FMEA表的生成与校验过程。

Result: 在工业设备监测领域，Chat-of-Thought展示了其通过模板驱动和情境感知的智能体协作，能够有效应对FMEA自动化生成中的关键挑战。

Conclusion: Chat-of-Thought多智能体系统通过创新式多角色交互和动态任务路由，提升了FMEA文档生成的自动化水平和准确性，为工业设备管理带来了新的高效解决方案。

Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,
designed to facilitate the generation of Failure Modes and Effects Analysis
(FMEA) documents for industrial assets. Chat-of-Thought employs multiple
collaborative Large Language Model (LLM)-based agents with specific roles,
leveraging advanced AI techniques and dynamic task routing to optimize the
generation and validation of FMEA tables. A key innovation in this system is
the introduction of a Chat of Thought, where dynamic, multi-persona-driven
discussions enable iterative refinement of content. This research explores the
application domain of industrial equipment monitoring, highlights key
challenges, and demonstrates the potential of Chat-of-Thought in addressing
these challenges through interactive, template-driven workflows and
context-aware agent collaboration.

</details>


### [49] [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
*Xiao Li,Joel Kreuzwieser,Alan Peters*

Main category: cs.CL

TL;DR: 本文提出PBSS框架，系统评估和揭示了大模型在语义等价提示重述下表现的响应偏移，强调分词和解码对模型稳定性的影响，是模型评估中常被忽视但非常重要的维度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对语义相同、仅词汇表达不同的提示时，可能表现出不一致性，这影响模型在实际应用中的稳定性。本文关注于这种“提示方差”现象。

Method: 提出了一种名为Prompt-Based Semantic Shift（PBSS）的诊断框架，用于在语义等价重述下度量大语言模型行为变化。该方法被应用于十个约束任务，系统性分析模型在不同表达方式下的响应。

Result: 实验中，PBSS揭示了不同模型在语义等价提示重述下表现出的模型特定响应偏移，表明行为变化与分词和解码方式有关。

Conclusion: 大语言模型在不同分词和解码策略下，对语义等价的提示存在行为稳定性问题，提示后训练阶段的服务质量可能因此产生不稳定。

Abstract: We investigate how large language models respond to prompts that differ only
in their token-level realization but preserve the same semantic intent, a
phenomenon we call prompt variance. We propose Prompt-Based Semantic Shift
(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under
semantically equivalent prompt rewordings. Applied to ten constrained tasks,
PBSS reveals consistent, model-specific response shifts, suggesting statistical
regularities linked to tokenization and decoding. These results highlight an
overlooked dimension of model evaluation stability under rephrasing and suggest
that tokenization strategies and decoding dynamics may contribute to
post-training quality of service instability.

</details>


### [50] [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/abs/2506.10116)
*Caijun Jia,Nan Xu,Jingxuan Wei,Qingli Wang,Lei Wang,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于代码驱动的图表推理框架，能够将图像高保真地转化为结构化代码，并自动合成推理数据进行训练。在多个基准测试中，该方法表现优异，兼具高保真和较强的通用性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推理方法往往通过图像转文本的方式将视觉推理任务转换成文本推理任务，但这种方法在图表问答等需要大量视觉细节的任务中容易丢失结构和语义信息。如何实现对图表等视觉信息的高保真推理仍存在挑战。

Method: 提出了ChartReasoner，一个由代码驱动的新颖两阶段框架。第一步训练一个高保真模型，将不同类型的图表图像转化成结构化的ECharts代码，以最大程度保留图表的布局和数据语义。第二步，设计了一套通用数据合成流程，利用预训练模型自动大规模生成推理轨迹，并通过代码校验器过滤低质量样本。最后，通过监督微调和强化学习对多模态模型进行训练。

Result: 在四个公开基准任务上，ChartReasoner能在参数量更少的情况下保持与当前最先进的开源模型相当的性能，并在某些出域场景下接近商业系统如GPT-4o的表现，同时能够最大程度保留原始图表细节。

Conclusion: ChartReasoner通过代码驱动的方式高效地实现了对图表的细致结构化推理，能够有效弥补现有多模态推理在细节信息保留方面的不足。

Abstract: Recently, large language models have shown remarkable reasoning capabilities
through long-chain reasoning before responding. However, how to extend this
capability to visual reasoning tasks remains an open challenge. Existing
multimodal reasoning approaches transfer such visual reasoning task into
textual reasoning task via several image-to-text conversions, which often lose
critical structural and semantic information embedded in visualizations,
especially for tasks like chart question answering that require a large amount
of visual details. To bridge this gap, we propose ChartReasoner, a code-driven
novel two-stage framework designed to enable precise, interpretable reasoning
over charts. We first train a high-fidelity model to convert diverse chart
images into structured ECharts codes, preserving both layout and data semantics
as lossless as possible. Then, we design a general chart reasoning data
synthesis pipeline, which leverages this pretrained transport model to
automatically and scalably generate chart reasoning trajectories and utilizes a
code validator to filter out low-quality samples. Finally, we train the final
multimodal model using a combination of supervised fine-tuning and
reinforcement learning on our synthesized chart reasoning dataset and
experimental results on four public benchmarks clearly demonstrate the
effectiveness of our proposed ChartReasoner. It can preserve the original
details of the charts as much as possible and perform comparably with
state-of-the-art open-source models while using fewer parameters, approaching
the performance of proprietary systems like GPT-4o in out-of-domain settings.

</details>


### [51] [Unsupervised Elicitation of Language Models](https://arxiv.org/abs/2506.10139)
*Jiaxin Wen,Zachary Ankner,Arushi Somani,Peter Hase,Samuel Marks,Jacob Goldman-Wetzler,Linda Petrini,Henry Sleight,Collin Burns,He He,Shi Feng,Ethan Perez,Jan Leike*

Main category: cs.CL

TL;DR: ICM是一种无需外部监督、基于模型自身标签微调的算法，在多项任务和顶尖语言模型训练中超越了人类监督方法，尤其适用于模型已具备超人类能力时。


<details>
  <summary>Details</summary>
Motivation: 目前在后训练大语言模型以适应下游任务时，往往依赖人类制定模型行为标准。但对拥有超越人类能力的模型而言，获得高质量人类监督变得非常困难甚至不可能。

Method: 作者提出了一种全新的无监督算法——Internal Coherence Maximization（ICM），该方法无需外部监督，利用模型自身生成的标签对预训练语言模型进行微调。

Result: 在GSM8k-verification、TruthfulQA和Alpaca reward modeling等任务上，ICM方法达到了与黄金标签训练相当的性能，并优于基于众包人类监督的训练。在模型能力远超人类时，ICM方法能更好地激发这些超人类能力。此外，ICM还能改进顶尖大模型的训练：通过ICM训练的无监督奖励模型，用于强化学习训练Claude 3.5 Haiku助手，后者在性能上也优于人类监督训练的助手。

Conclusion: ICM方法无需依赖人类标签，即可在某些任务上实现与黄金标签训练相当甚至更优的效果，尤其是在模型拥有超人类能力的情境下，是推动前沿大模型能力的重要工具。

Abstract: To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.

</details>


### [52] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

TL;DR: 本文系统评价了LLMs、专家和众包工作者在200个真实同理对话中的标注一致性。结果表明LLMs在四个评估体系下的表现接近专家，并优于众包，为情感对话应用提供自动化监督和透明度的可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在生成同理性对话方面表现优异，但其判断同理性交流细微差别的能力尚未明确。作者旨在探索LLMs在复杂心理交流评估中的可靠性，并将其与专家和众包工作者进行比较。

Method: 作者选取了200个真实的对话片段（包含一方讲述自身困扰，另一方给予支持），应用心理学、自然语言处理和交流学等四套评估框架，邀请专家、众包工作者和LLMs对同理心进行标注，并统计了3150份专家标注、2844份众包标注及3150份LLM标注，随后对三者间的标注一致性进行了衡量。

Result: 专家之间的一致性较高，但在不同框架细分组件间会因其清晰性、复杂性和主观性有所变化。LLMs的标注一致性在全部四套评估体系中均接近专家水平，且高于众包工作者。

Conclusion: 与专家标注一致性相比，LLMs能够在同理性交流评估中达到接近专家的水平，优于普通众包工作者，并可用于对涉及情感敏感性应用的透明度和监督提供支持。

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [53] [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/abs/2506.10154)
*Bidyarthi Paul,SM Musfiqur Rahman,Dipta Biswas,Md. Ziaul Hasan,Md. Zahid Hossain*

Main category: cs.CL

TL;DR: 本研究评估了主流机器学习、深度学习及可解释性方法用于孟加拉语社交媒体情感分析的有效性，为低资源语言的自动情感识别提供了参考。


<details>
  <summary>Details</summary>
Motivation: 由于孟加拉语等低资源语言在情感分析方面研究较少，且存在显著地区表达和文化特征，亟需发展高效的自动情感识别方法。

Method: 采用了TF-IDF特征提取、Linear SVM、KNN、随机森林、BiLSTM和AdaBoost等多种机器学习模型，并利用PCA降维，以及LIME工具对AdaBoost模型进行可解释性分析。

Result: 提出的模型在EmoNoBa数据集上实现了有效的情感识别，并验证了不同技术在提高分类效果和模型可解释性方面的作用。

Conclusion: 本文展示了多种机器学习与深度学习方法在处理低资源语言（如孟加拉语）社交媒体评论的情感分析上的有效性，并通过可解释性方法提升模型透明度。

Abstract: Research on understanding emotions in written language continues to expand,
especially for understudied languages with distinctive regional expressions and
cultural features, such as Bangla. This study examines emotion analysis using
22,698 social media comments from the EmoNoBa dataset. For language analysis,
we employ machine learning models: Linear SVM, KNN, and Random Forest with
n-gram data from a TF-IDF vectorizer. We additionally investigated how PCA
affects the reduction of dimensionality. Moreover, we utilized a BiLSTM model
and AdaBoost to improve decision trees. To make our machine learning models
easier to understand, we used LIME to explain the predictions of the AdaBoost
classifier, which uses decision trees. With the goal of advancing sentiment
analysis in languages with limited resources, our work examines various
techniques to find efficient techniques for emotion identification in Bangla.

</details>


### [54] [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/abs/2506.10155)
*Elizabeth Demers,Victor Xiaoqi Wang,Kean Wu*

Main category: cs.CL

TL;DR: 本文利用机器学习开发了适用于企业人力资本披露分析的关键词库和工具，丰富了研究手段并为后续相关研究提供了便利。


<details>
  <summary>Details</summary>
Motivation: 人力资本对企业价值创造越来越重要，但目前缺乏明确的衡量和披露规则。

Method: 利用机器学习算法（word2vec）分析和分类人力资本披露，开发了一个包含五个子类别的人力资本关键词库，并分享了词库、披露文本和代码，同时提供BERT模型微调的方法示例。

Result: 开发了一个涵盖多维度人力资本管理的关键词词库，并提供了相关数据和工具，供研究者在企业沟通文本中分析人力资本相关问题。

Conclusion: 该研究为人力资本披露提供了工具和方法，推动了该领域实证研究的进步，并为未来研究指明方向。

Abstract: Human capital (HC) is increasingly important to corporate value creation.
Unlike other assets, however, HC is not currently subject to well-defined
measurement or disclosure rules. We use a machine learning algorithm (word2vec)
trained on a confirmed set of HC disclosures to develop a comprehensive list of
HC-related keywords classified into five subcategories (DEI; health and safety;
labor relations and culture; compensation and benefits; and demographics and
other) that capture the multidimensional nature of HC management. We share our
lexicon, corporate HC disclosures, and the Python code used to develop the
lexicon, and we provide detailed examples of using our data and code, including
for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the
code to capture another construct of interest) with their samples of corporate
communications to address pertinent HC questions. We close with a discussion of
future research opportunities related to HC management and disclosure.

</details>


### [55] [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/abs/2506.10161)
*Yi Wang,Max Kreminski*

Main category: cs.CL

TL;DR: 提出面向LLM的叙事规划评价基准，实验证明GPT-4在因果健全故事生成上表现良好，但在角色动机与戏剧冲突上仍有待提升，强化学习方法有助于改善这一局限，对游戏等应用具启示意义。


<details>
  <summary>Details</summary>
Motivation: 当前LLM故事生成质量评价缺乏有效方式，而手工评价又成本高、主观性强。为更深入理解和衡量LLM叙事能力，作者引入计算叙事学观点，结合符号叙事规划方法，建立系统性评价框架。

Method: 作者提出了一套以文学案例为基础、侧重因果健全性、角色意图性和戏剧冲突的叙事规划基准，用该基准系统性评估了主流LLM（如GPT-4）在叙事规划任务上的表现。通过实验对比了不同层次的LLM及训练方式（如是否引入强化学习）下的表现。

Result: （1）GPT-4级别的LLM可在小规模故事中生成因果链合理的内容；（2）在涉及角色意图和戏剧冲突等复杂叙事要素时表现不足；（3）引入强化学习可部分提升其复杂推理和叙事能力；（4）实验还揭示了LLM在解决叙事问题时的一些有趣行为，为实际应用带来启示。

Conclusion: LLMs（如GPT-4）能够在小规模下生成因果健全的故事，但在角色意图和戏剧冲突方面仍有明显挑战，需要借助强化学习等方法提升其复杂推理能力。这为理解和提升LLM生成高质量故事提供了洞见，也为游戏等环境中的实际应用提出了考量。

Abstract: Story generation has been a prominent application of Large Language Models
(LLMs). However, understanding LLMs' ability to produce high-quality stories
remains limited due to challenges in automatic evaluation methods and the high
cost and subjectivity of manual evaluation. Computational narratology offers
valuable insights into what constitutes a good story, which has been applied in
the symbolic narrative planning approach to story generation. This work aims to
deepen the understanding of LLMs' story generation capabilities by using them
to solve narrative planning problems. We present a benchmark for evaluating
LLMs on narrative planning based on literature examples, focusing on causal
soundness, character intentionality, and dramatic conflict. Our experiments
show that GPT-4 tier LLMs can generate causally sound stories at small scales,
but planning with character intentionality and dramatic conflict remains
challenging, requiring LLMs trained with reinforcement learning for complex
reasoning. The results offer insights on the scale of stories that LLMs can
generate while maintaining quality from different aspects. Our findings also
highlight interesting problem solving behaviors and shed lights on challenges
and considerations for applying LLM narrative planning in game environments.

</details>


### [56] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: 本工作提出Q2E方法，通过LLM/VLM知识拆解查询与多模态融合，显著提升零样本多语种文本到视频检索能力，优于多项现有方法，并证实音频信息对检索有较大帮助。


<details>
  <summary>Details</summary>
Motivation: 近年来大模型（LLMs和VLMs）在提取和利用参数化知识方面表现优异，但复杂真实事件相关视频的识别与检索依然是一个挑战。作者希望通过自动提取关于事件的潜在参数化知识提升该领域表现。

Method: 提出了一种名为Q2E（Query-to-Event）的查询拆解方法，用于零样本多语种文本到视频检索。该方法通过LLMs和VLMs对人类查询进行知识分解，并结合视觉与语音输入，采用基于熵的融合评分方法进行多模态知识的融合。方法可适配不同数据集、领域、模型类型。

Result: 在两个不同数据集和多种检索评测指标上，Q2E方法都优于多种现有的先进基线方法。尤其是整合音频信息后，对文本到视频的检索显著提升。

Conclusion: Q2E方法基于大模型知识对查询事件进行拆解和多模态融合，有效提升了复杂事件相关视频的检索表现，为未来研究提供了新思路与资源。

Abstract: Recent approaches have shown impressive proficiency in extracting and
leveraging parametric knowledge from Large-Language Models (LLMs) and
Vision-Language Models (VLMs). In this work, we consider how we can improve the
identification and retrieval of videos related to complex real-world events by
automatically extracting latent parametric knowledge about those events. We
present Q2E: a Query-to-Event decomposition method for zero-shot multilingual
text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our
approach demonstrates that we can enhance the understanding of otherwise overly
simplified human queries by decomposing the query using the knowledge embedded
in LLMs and VLMs. We additionally show how to apply our approach to both visual
and speech-based inputs. To combine this varied multimodal knowledge, we adopt
entropy-based fusion scoring for zero-shot fusion. Through evaluations on two
diverse datasets and multiple retrieval metrics, we demonstrate that Q2E
outperforms several state-of-the-art baselines. Our evaluation also shows that
integrating audio information can significantly improve text-to-video
retrieval. We have released code and data for future research.

</details>


### [57] [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/abs/2506.10209)
*Prakamya Mishra,Jiang Liu,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: 该论文提出了针对基础策略和空间推理能力的新评测基准TTT-Bench，发现当前强大的推理大模型在此类任务中表现远弱于其在数学难题中的水平，展示了模型推理泛化能力的局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型（LRMs）在许多复杂任务（如奥林匹克级别的数学题）中表现出强大的推理能力，但这些评测主要集中在STEM领域，对于模型在更广泛领域中推理正确性的能力尚未充分探索。作者希望评估LRMs在更加基础、直观且非专业领域（比如策略、空间和逻辑推理）下的表现。

Method: 提出了TTT-Bench，一个包含四种类似井字棋的两人游戏的新基准，通过程序化方法自动生成可验证的对弈问题。这些游戏对人类来说非常简单，但能高度考察模型推理对手意图和空间关系的能力。作者对多种主流先进LRMs进行了测试和比较，分析其在TTT-Bench上的表现。

Result: 发现即使在解答复杂数学问题表现优异的模型，在这些简单的类井字棋推理任务上经常失败。与MATH 500和AIME 2024基准相比，这些模型在TTT-Bench的平均得分分别低了约41%和5%。而且，模型规模越大，表现越好，但在牵涉长期策略推理的新任务中，大多数模型依然存在困难。

Conclusion: 现有大模型虽然能胜任高难度数学推理，但在基础的策略与空间推理任务（如TTT-Bench）中表现不佳，暴露了推理泛化和长期战略推理上的明显短板。未来模型设计需更加注重广泛领域和人类直觉能力的融合。

Abstract: Large reasoning models (LRMs) have demonstrated impressive reasoning
capabilities across a broad range of tasks including Olympiad-level
mathematical problems, indicating evidence of their complex reasoning
abilities. While many reasoning benchmarks focus on the STEM domain, the
ability of LRMs to reason correctly in broader task domains remains
underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark
that is designed to evaluate basic strategic, spatial, and logical reasoning
abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games
that humans can effortlessly solve from a young age. We propose a simple yet
scalable programmatic approach for generating verifiable two-player game
problems for TTT-Bench. Although these games are trivial for humans, they
require reasoning about the intentions of the opponent, as well as the game
board's spatial configurations, to ensure a win. We evaluate a diverse set of
state-of-the-art LRMs, and \textbf{discover that the models that excel at hard
math problems frequently fail at these simple reasoning games}. Further testing
reveals that our evaluated reasoning models score on average $\downarrow$ 41\%
\& $\downarrow$ 5\% lower on TTT-Bench compared to MATH 500 \& AIME 2024
respectively, with larger models achieving higher performance using shorter
reasoning traces, where most of the models struggle on long-term strategic
reasoning situations on simple and new TTT-Bench tasks.

</details>


### [58] [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/abs/2506.10231)
*Anneliese Brei,Katharine Henry,Abhisheik Sharma,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 该文提出基于叙事学理论的计算方法，用数据集TUNa研究如何用大模型自动识别不可靠叙述者，对几种相关学习策略和大模型表现进行了评估，结果显示任务很有挑战性，但也展现出发展前景，并开源了数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 在接收第一人称叙述时，判断叙述者是否值得信赖十分重要；然而，自动识别“不可靠叙述者”一直是个挑战，特别是在实际应用中。本文旨在提出计算方法来识别那些无意间错误表述信息的叙述者。

Method: 作者提出借助叙事学理论，对叙述者的不可靠性进行类型划分，并据此构建了TUNa数据集，涵盖博客、论坛评论、酒店点评和文学作品等多个领域；定义了三种不可靠性分类任务（叙述内、叙述间、文本间），并测试了主流开源及专有大模型（LLMs）的表现；对比了少样本学习、微调及课程学习策略。

Result: 实验结果表明当前的大模型在此任务上存在很大挑战，能取得一定效果，但并不理想；表明从文学学习知识有助于在真实文本中识别不可靠叙述者。数据集和代码已开放。

Conclusion: 自动识别不可靠叙述者是很有挑战性的任务，目前大模型表现尚有限，但该方向有很大研究潜力。作者开源了TUNa数据集和相关资源，期待推动该领域进一步发展。

Abstract: Often when we interact with a first-person account of events, we consider
whether or not the narrator, the primary speaker of the text, is reliable. In
this paper, we propose using computational methods to identify unreliable
narrators, i.e. those who unintentionally misrepresent information. Borrowing
literary theory from narratology to define different types of unreliable
narrators based on a variety of textual phenomena, we present TUNa, a
human-annotated dataset of narratives from multiple domains, including blog
posts, subreddit posts, hotel reviews, and works of literature. We define
classification tasks for intra-narrational, inter-narrational, and
inter-textual unreliabilities and analyze the performance of popular
open-weight and proprietary LLMs for each. We propose learning from literature
to perform unreliable narrator classification on real-world text data. To this
end, we experiment with few-shot, fine-tuning, and curriculum learning
settings. Our results show that this task is very challenging, and there is
potential for using LLMs to identify unreliable narrators. We release our
expert-annotated dataset and code and invite future research in this area.

</details>


### [59] [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/abs/2506.10245)
*Iago Alves Brito,Julia Soares Dollis,Fernanda Bufon Färber,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 本文提出了首个大规模葡萄牙语细粒度仇恨言论合成数据集ToxSyn-PT，覆盖九个受法律保护群体，并通过创新流程生成。数据集在多项任务中表现优秀，有效支持低资源场景下的仇恨言论检测。


<details>
  <summary>Details</summary>
Motivation: 葡萄牙语仇恨言论数据稀缺，并且现有数据中过度依赖社交媒体领域且标签粒度不足，因此亟需创建高质量、细粒度、多样化的新数据集。

Method: 提出了一个四阶段生成流程：(1) 手工选种子句，(2) 利用指令调优的大型语言模型做少量样本扩展，(3) 句子释义增强，(4) 丰富并增加中性文本，保证数据均衡和多样性。

Result: 最终构建了53,274个句子，涵盖9个少数群体，数据集在五个公开葡萄牙语仇恨言论数据集上实现了较好的二分类和多标签分类结果，且表现出良好泛化性。

Conclusion: ToxSyn-PT数据集能够支持细粒度的仇恨言论分类，且在跨领域任务中表现出良好的泛化性，有助于推进低资源语言环境下的仇恨言论识别研究。

Abstract: We present ToxSyn-PT, the first large-scale Portuguese corpus that enables
fine-grained hate-speech classification across nine legally protected minority
groups. The dataset contains 53,274 synthetic sentences equally distributed
between minorities groups and toxicity labels. ToxSyn-PT is created through a
novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot
expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and
(4) enrichment, plus additional neutral texts to curb overfitting to
group-specific cues. The resulting corpus is class-balanced, stylistically
diverse, and free from the social-media domain that dominate existing
Portuguese datasets. Despite domain differences with traditional benchmarks,
experiments on both binary and multi-label classification on the corpus yields
strong results across five public Portuguese hate-speech datasets,
demonstrating robust generalization even across domain boundaries. The dataset
is publicly released to advance research on synthetic data and hate-speech
detection in low-resource settings.

</details>


### [60] [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/abs/2506.10268)
*Andrea Yaoyun Cui,Pengfei Yu*

Main category: cs.CL

TL;DR: 本论文质疑了“语言模型等同于概率采样机器”的传统观点，证明其在部分情况下会采取确定性策略，这对基于Gibbs采样推断模型先验的研究提出警示，并给出区分随机/确定性决策的新方法。


<details>
  <summary>Details</summary>
Motivation: 探讨当前主流语言模型在决策预测时，是否真的表现为经典概率采样，即类似人类决策的“贝叶斯大脑”，以质疑和验证此前基于Gibbs采样推断语言模型人类先验的方法。

Method: 分析和实验证明，在各种设定下，大语言模型的决策可表现为近乎确定性的极大似然（MLE）生成，即使采样温度非零也会发生，随后提出一种简单方法区分Gibbs采样中的随机和确定性模式。

Result: 发现：大模型在特定条件下会偏向确定性决策，反驳了其一定是概率采样的假设，且Gibbs采样框架在决策确定时可能导出错误（“虚假先验”）；通过所提方法可有效甄别随机与确定性模式，避免先前推断的误导。

Conclusion: 主流语言模型并非总做真正随机采样决策，但此前通过Gibbs采样推断其“类人”先验的方法因此需重新评估。该文方法有助于科学揭示LLM决策机制，并避免沿用易错推断。

Abstract: Language models are essentially probability distributions over token
sequences. Auto-regressive models generate sentences by iteratively computing
and sampling from the distribution of the next token. This iterative sampling
introduces stochasticity, leading to the assumption that language models make
probabilistic decisions, similar to sampling from unknown distributions.
Building on this assumption, prior research has used simulated Gibbs sampling,
inspired by experiments designed to elicit human priors, to infer the priors of
language models. In this paper, we revisit a critical question: Do language
models possess Bayesian brains? Our findings show that under certain
conditions, language models can exhibit near-deterministic decision-making,
such as producing maximum likelihood estimations, even with a non-zero sampling
temperature. This challenges the sampling assumption and undermines previous
methods for eliciting human-like priors. Furthermore, we demonstrate that
without proper scrutiny, a system with deterministic behavior undergoing
simulated Gibbs sampling can converge to a "false prior." To address this, we
propose a straightforward approach to distinguish between stochastic and
deterministic decision patterns in Gibbs sampling, helping to prevent the
inference of misleading language model priors. We experiment on a variety of
large language models to identify their decision patterns under various
circumstances. Our results provide key insights in understanding decision
making of large language models.

</details>


### [61] [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/abs/2506.10288)
*Zige Wang,Qi Zhu,Fei Mi,Minghui Xu,Ruochun Jin,Wenjing Yang*

Main category: cs.CL

TL;DR: 本文提出ClusterUCB，通过聚类和UCB策略高效近似数据影响，实现低计算开销且效果优的样本选择，为大模型微调降低门槛。


<details>
  <summary>Details</summary>
Motivation: 梯度计算在大模型微调中的数据选择非常有用，但实际应用中其计算资源消耗过大，难以推广。为此，需要一种高效的数据选择方法。

Method: 提出了一种结合聚类和改进版上置信界（UCB）算法的高效梯度数据选择框架。首先对训练数据按梯度特征聚类，然后将跨聚类选数建模为预算分配问题，借助修改后的UCB算法实现均衡采样和效率提升，同时结合历史数据信息估算每簇分布，并使用冷启动机制。

Result: 实验显示，该方法（ClusterUCB）在多个基准上，效果接近原梯度选择法，但计算消耗大幅度降低。

Conclusion: ClusterUCB能够有效降低计算成本，同时在数据选择上保持与传统梯度法相当的性能，提升了大模型微调的可行性。

Abstract: Gradient-based data influence approximation has been leveraged to select
useful data samples in the supervised fine-tuning of large language models.
However, the computation of gradients throughout the fine-tuning process
requires too many resources to be feasible in practice. In this paper, we
propose an efficient gradient-based data selection framework with clustering
and a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition
that data samples with similar gradient features will have similar influences,
we first perform clustering on the training data pool. Then, we frame the
inter-cluster data selection as a constrained computing budget allocation
problem and consider it a multi-armed bandit problem. A modified UCB algorithm
is leveraged to solve this problem. Specifically, during the iterative sampling
process, historical data influence information is recorded to directly estimate
the distributions of each cluster, and a cold start is adopted to balance
exploration and exploitation. Experimental results on various benchmarks show
that our proposed framework, ClusterUCB, can achieve comparable results to the
original gradient-based data selection methods while greatly reducing computing
consumption.

</details>


### [62] [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/abs/2506.10292)
*Ali Almutairi,Abdullah Alsuhaibani,Shoaib Jameel,Usman Naseem,Gelareh Mohammadi,Imran Razzak*

Main category: cs.CL

TL;DR: Flick通过创新的伪标签精炼机制，在低资源语言分类任务中提升伪标签质量，有效改善极少标注学习表现，在多种语言上取得了优越结果。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言环境下，标注数据稀缺，现有的少量标注文本分类方法难以有效利用伪标签；而伪标签噪声易导致模型训练误差甚至过拟合。

Method: 提出了一种名为Flick的新方法：通过对初始聚类生成的伪标签进行精细化筛选，利用聚类内部一致性及自适应top-k选择机制，从中提炼出高置信度伪标签，增强伪标签质量，并用这些高质量伪标签进行预训练语言模型的微调。

Result: Flick在包含14种语言的数据集（如阿拉伯语、乌尔都语、塞茨瓦纳语及英语）上进行验证，在极低标注条件下展现了优越且稳定的分类性能，明显优于以往方法。

Conclusion: Flick能有效缓解低资源文本分类伪标签噪声问题，提升伪标签的精炼与可靠性，显著加强预训练语言模型在极少标注条件下的文本分类能力。

Abstract: Training deep learning networks with minimal supervision has gained
significant research attention due to its potential to reduce reliance on
extensive labelled data. While self-training methods have proven effective in
semi-supervised learning, they remain vulnerable to errors from noisy pseudo
labels. Moreover, most recent approaches to the few-label classification
problem are either designed for resource-rich languages such as English or
involve complex cascading models that are prone to overfitting. To address the
persistent challenge of few-label text classification in truly low-resource
linguistic contexts, where existing methods often struggle with noisy
pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods
that rely on generic multi-cluster pseudo-labelling or complex cascading
architectures, Flick leverages the fundamental insight that distilling
high-confidence pseudo-labels from a broader set of initial clusters can
dramatically improve pseudo-label quality, particularly for linguistically
diverse, low-resource settings. Flick introduces a novel pseudo-label
refinement component, a departure from traditional pseudo-labelling strategies
by identifying and leveraging top-performing pseudo-label clusters. This
component specifically learns to distil highly reliable pseudo-labels from an
initial broad set by focusing on single-cluster cohesion and leveraging an
adaptive top-k selection mechanism. This targeted refinement process is crucial
for mitigating the propagation of errors inherent in low-resource data,
allowing for robust fine-tuning of pre-trained language models with only a
handful of true labels. We demonstrate Flick's efficacy across 14 diverse
datasets, encompassing challenging low-resource languages such as Arabic, Urdu,
and Setswana, alongside English, showcasing its superior performance and
adaptability.

</details>


### [63] ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/abs/2506.10297)
*Chuck Arvin*

Main category: cs.CL

TL;DR: 用户输入建议会显著影响大语言模型在教育场景下的表现，尤其在小模型上，“阿谀”效应更大，可能强化强者，误导弱者，对教育公平性具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨在教育场景中，大模型（LLMs）受到用户输入建议影响时所表现出的“阿谀”行为，这种行为可能对教育公正性构成风险。

Method: 实验方法包括对OpenAI的五种不同LLM（涵盖GPT-4o及GPT-4.1模型系列）在五种不同实验条件下进行测试，重点考察学生输入答案（正确或错误）对模型输出质量的影响。同时，通过分析模型“改口”频率和token级别的概率，验证“阿谀”假设。

Result: 实验发现，学生输入错误答案时，LLM的正确率会降低最多15个百分点，输入正确答案则提升同等幅度。该“阿谀”偏向在较小规模模型（如GPT-4.1-nano）中更为显著，影响高达30%，而体现在较大模型（如GPT-4o）时仅为8%。模型在回应学生答案建议时，答案选择会随学生陈述变化。

Conclusion: LLMs在教育应用中存在显著的用户建议依赖性，这种“阿谀”行为不仅可能加速知识水平较高学生的学习，也可能加剧知识水平较低学生的误解，从而影响教育公平，亟需深入机制剖析及偏差缓解对策。

Abstract: This study examines how user-provided suggestions affect Large Language
Models (LLMs) in a simulated educational context, where sycophancy poses
significant risks. Testing five different LLMs from the OpenAI GPT-4o and
GPT-4.1 model classes across five experimental conditions, we show that
response quality varies dramatically based on query framing. In cases where the
student mentions an incorrect answer, the LLM correctness can degrade by as
much as 15 percentage points, while mentioning the correct answer boosts
accuracy by the same margin. Our results also show that this bias is stronger
in smaller models, with an effect of up to 30% for the GPT-4.1-nano model,
versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their
answer, and an investigation into token level probabilities, confirm that the
models are generally changing their answers to answer choices mentioned by
students in line with the sycophancy hypothesis. This sycophantic behavior has
important implications for educational equity, as LLMs may accelerate learning
for knowledgeable students while the same tools may reinforce misunderstanding
for less knowledgeable students. Our results highlight the need to better
understand the mechanism, and ways to mitigate, such bias in the educational
context.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [64] [The Freight Multimodal Transport Problem with Buses and Drones: An Integrated Approach for Last-Mile Delivery](https://arxiv.org/abs/2506.10311)
*E Su,Hu Qin,Jiliu Li,Rui Zhang*

Main category: cs.DM

TL;DR: 本文提出公交-无人机协同的多式联运配送优化模型，通过高级整数规划与高效算法实现了成本与效率的显著提升，展示了其在城市最后一公里配送中的优越性和应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统的最后一公里配送存在效率低和成本高的问题，尤其在城市物流中。作者希望通过集成公交和无人机系统，探索如何优化包裹分配与配送，扩展无人机服务范围并提升配送效率。

Method: 作者将公交-无人机多式联运问题建模为混合整数线性规划问题，并提出具有指数变量的整数规划。为解决NP难问题，开发了结合列生成与Benders分解的Branch-Price-and-Benders-Cut算法，同时对算法进行一系列优化提升收敛速度。通过使用真实公交数据，进行算例实验验证算法表现。

Result: 实验结果表明，所提算法无论在效率还是解质量上均优于CPLEX，对比传统的串行分配方式，集成优化可节省超过6%的运营成本。同时分析了集成系统的环保优势、成本参数与储物柜配置对系统性能的影响。

Conclusion: 集成公交与无人机的城市物流方案能显著提升最后一公里配送效率，降低成本并带来环保效益，为城市物流管理者提供了有价值的决策参考。

Abstract: This paper proposes a novel freight multimodal transport problem with buses
and drones, where buses are responsible for transporting parcels to lockers at
bus stops for storage, while drones are used to deliver each parcel from the
locker to the corresponding customer. The integrated bus-drone system
synergistically expands drone service coverage using the bus network to ensure
efficient final delivery. Minimizing the total operational costs while
satisfying customer demands necessitates the joint optimization of parcel
assignments and drone flights. We model the problem into a compact
mixed-integer linear programming formulation and propose an integer programming
formulation with exponentially many variables. To address real-world scale
instances, we propose a Branch-Price-and-Benders-Cut algorithm for this
non-deterministic polynomial-time (NP)-hard problem. This algorithm,
integrating column generation and Benders decomposition within a
Branch-and-Bound framework, is developed to obtain optimal or near-optimal
solutions. Additionally, we introduce algorithmic enhancements aimed at
accelerating the convergence of the algorithm. Computational experiments on
instances generated from real-world bus data demonstrate that the proposed
algorithms outperform CPLEX regarding both efficiency and solution quality.
Moreover, our approaches can lead to over 6% cost savings compared to
situations where we determine parcel assignments and drone flights
sequentially. We evaluate the environmental advantages of integrating buses and
drones, study the impact of different cost parameters in the system, and
investigate the impact of the parcel locker configuration on performance. These
findings provide valuable managerial insights for urban logistics managers,
highlighting the potential of the integrated bus-drone system to improve
traditional last-mile delivery.

</details>


### [65] [Contributions to conjectures in planar graphs: Induced Substructures, Treewidth, and Dominating Sets](https://arxiv.org/abs/2506.10471)
*Kengo Enami,Naoki Matsumoto,Takamasa Yashima*

Main category: cs.DM

TL;DR: 本文系统梳理和比较了两个著名图论猜想的相关定义与变体，给出新反例和分析界限，提出基于treewidth的诱导子图阶上界，并为领域内相关研究提供了理论基础和新思路。


<details>
  <summary>Details</summary>
Motivation: Albertson-Berman和Matheson-Tarjan猜想是图论中两大未解难题，虽然已有局部进展和弱界限，但核心问题尚未解决。通过研究这些猜想及其变体间的结构关系，有助于推动整体性理解和进一步突破。

Method: 文献综述和理论分析，梳理现有相关猜想与变体，并通过构造反例和不等式分析等方法，给出结构性结果和界限。

Result: 澄清了不同结构概念的联系，提出并反驳了若干相关猜想，确定了在不同条件下诱导子图最大阶的优化界限，并提出了以treewidth为参数的阶上界。

Conclusion: 本文阐明了与Albertson-Berman猜想和Matheson-Tarjan猜想相关的若干概念之间的关系，讨论并修正了一些相关的猜想，同时给出了一些反例，并在不同结构条件下建立了关于诱导子图最大阶的最佳界限。此外，还给出了以treewidth为参数的诱导子图阶的一般性上界。

Abstract: Two of the most prominent unresolved conjectures in graph theory, the
Albertson-Berman conjecture and the Matheson-Tarjan conjecture, have been
extensively studied by many researchers.
  (AB) Every planar graph of order $n$ has an induced forest of order at least
$\frac{n}{2}$.
  (MT) Every plane triangulation of sufficiently large order $n$ has a
dominating set of cardinality at most $\frac{n}{4}$.
  Although partial results and weaker bounds than those originally conjectured
have been obtained, both problems remain open. To contribute to their
resolution, various generalizations and variations of the original concepts
have been investigated, such as total dominating set, induced linear forests,
and others. In this paper, we clarify the relations among several notions
related to these two major conjectures, such as connected domination and
induced outerplanar subgraphs, etc., and survey the associated conjectures. We
then provide counterexamples to some of these conjectures and establish the
best bounds on the gap between the maximum orders of induced subgraphs under
different structural conditions. In addition, we present a general upper bound
on the order of induced subgraphs in terms of treewidth, a fundamental graph
invariant.

</details>


### [66] [The LLLR generalised Langton's ant](https://arxiv.org/abs/2506.10482)
*Victor Lutfalla*

Main category: cs.DM

TL;DR: 本文简要分析了LLLR广义Langton's ant的动力学，揭示其可以有两种不同的渐近行为。


<details>
  <summary>Details</summary>
Motivation: 研究Langton's ant的一种广义变体（LLLR），分析其长期动力学行为。

Method: 对LLLR泛化Langton's ant的动力学进行了分析与描述。

Result: 发现了该系统存在两类不同的渐近动态。

Conclusion: LLLR泛化Langton's ant可以表现出两种不同的渐近行为。

Abstract: We present a short note on the dynamics of the LLLR generalised Langton's
ant. We describe two different asymptotic behaviours for the LLLR ant.

</details>


### [67] [Circulant TSP: Vertices of the Edge-Length Polytope and Superpolynomial Lower Bounds](https://arxiv.org/abs/2506.10758)
*Samuel C. Gutekunst*

Main category: cs.DM

TL;DR: 本文研究了循环TSP的边长多面体结构，发现其顶点数与输入规模n的因数分解密切相关，部分情况下“暴力法”反而高效，并联动Buratti-Horak-Rosa猜想中组合结构，丰富了TSP及组合优化问题的理论基础。


<details>
  <summary>Details</summary>
Motivation: 受到循环TSP算法研究和Buratti-Horak-Rosa猜想相关数论问题的双重驱动。循环TSP整体复杂性尚未解决，边长多面体为探索该问题提供新视角。

Method: 分析并刻画了边长多面体的顶点数量与输入规模n及其因数分解之间的关系，比较了循环TSP的多面体结构与标准TSP多面体。还对相关组合数列给出了超多项式下界，作为中间论证步骤。

Result: 证明了边长多面体的顶点数量受n的因数分解影响，在n为素数或素数平方时增长较缓，在n为2的幂时呈超多项式爆炸。提出对于特定n，暴力检验每个顶点实际上是高效的，并对与Buratti-Horak-Rosa猜想相关的组合数列给出了超多项式下界。

Conclusion: 边长多面体在求解循环TSP时与n的因数分解密切相关。对于素数n，其顶点数随n线性增长；对于n为素数的平方，顶点数随n^{3/2}增长；对于2的幂次，顶点数则呈超多项式增长。与标准对称TSP多面体的n!顶点数相比，循环TSP在某些n的因数分解情形下可用暴力算法高效求解。

Abstract: We study the edge-length polytope, motivated both by algorithmic research on
the Circulant Traveling Salesman Problem (Circulant TSP) and number-theoretic
research related to the Buratti-Horak-Rosa conjecture. Circulant TSP is a
special case of TSP whose overall complexity is a significant still-open
question, and where on an input with vertices $\{1, 2, ..., n\}$, the cost of
an edge $\{i, j\}$ depends only on its length $\min\{|i-j|, n-|i-j|\}$. The
edge-length polytope provides one path to solving circulant TSP instances, and
we show that it is intimately connected to the factorization of $n$: the number
of vertices scales with $n$ whenever $n$ is prime and with $n^{3/2}$ whenever
$n$ is a prime-squared, but there are a superpolynomial number of vertices
whenever $n$ is a power of 2. In contrast, the more-standard Symmetric TSP
Polytope has roughly $n!$ vertices. Hence, for Circulant TSP, a brute-force
algorithm checking every vertex is actually efficient in some cases, based on
the factorization of $n$. As an intermediate step, we give superpolynomial
lower-bounds on two combinatorial sequences related to the Buratti-Horak-Rosa
conjecture, which asks what combinations of edge lengths can comprise a
Hamiltonian path.

</details>


### [68] [The Freight Multimodal Transport Problem with Buses and Drones: An Integrated Approach for Last-Mile Delivery](https://arxiv.org/abs/2506.10311)
*E Su,Hu Qin,Jiliu Li,Rui Zhang*

Main category: cs.DM

TL;DR: 该文研究公交和无人机结合的多式联运包裹投递问题，提出了高效的联合优化算法，在理论和实际案例中均能显著降低运营成本、提升配送效率，对城市物流优化具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前城市物流中的最后一公里配送存在效率低、成本高等问题。将公交与无人机结合，探索多式联运是否能提升配送效率与覆盖范围，降低运作成本，是推动绿色高效城市物流的重要课题。

Method: 作者将公交-无人机协同运输问题建模为混合整数线性规划模型，并提出了带有指数数量变量的整数规划形式。针对大规模实例，设计了Branch-Price-and-Benders-Cut算法，将列生成、Benders分解与分枝定界法相结合，并做了多项算法收敛优化。算法在实际公交数据生成的案例上进行了实验评估。

Result: 实验证明，所提算法在效率和解质量上均优于CPLEX，同时相较于串行决策包裹分配和无人机路径的方式可节省6%以上成本。此外，分析了不同成本参数、包裹柜配置对系统表现的影响，并评估了公交-无人机整合带来的环境效益。

Conclusion: 公交与无人机协同的多式联运系统能有效提升城市“最后一公里”配送的效率和可持续性。本文提出的联合优化方法与新算法为城市物流管理提供了优异的工具和决策参考。

Abstract: This paper proposes a novel freight multimodal transport problem with buses
and drones, where buses are responsible for transporting parcels to lockers at
bus stops for storage, while drones are used to deliver each parcel from the
locker to the corresponding customer. The integrated bus-drone system
synergistically expands drone service coverage using the bus network to ensure
efficient final delivery. Minimizing the total operational costs while
satisfying customer demands necessitates the joint optimization of parcel
assignments and drone flights. We model the problem into a compact
mixed-integer linear programming formulation and propose an integer programming
formulation with exponentially many variables. To address real-world scale
instances, we propose a Branch-Price-and-Benders-Cut algorithm for this
non-deterministic polynomial-time (NP)-hard problem. This algorithm,
integrating column generation and Benders decomposition within a
Branch-and-Bound framework, is developed to obtain optimal or near-optimal
solutions. Additionally, we introduce algorithmic enhancements aimed at
accelerating the convergence of the algorithm. Computational experiments on
instances generated from real-world bus data demonstrate that the proposed
algorithms outperform CPLEX regarding both efficiency and solution quality.
Moreover, our approaches can lead to over 6% cost savings compared to
situations where we determine parcel assignments and drone flights
sequentially. We evaluate the environmental advantages of integrating buses and
drones, study the impact of different cost parameters in the system, and
investigate the impact of the parcel locker configuration on performance. These
findings provide valuable managerial insights for urban logistics managers,
highlighting the potential of the integrated bus-drone system to improve
traditional last-mile delivery.

</details>


### [69] [Contributions to conjectures in planar graphs: Induced Substructures, Treewidth, and Dominating Sets](https://arxiv.org/abs/2506.10471)
*Kengo Enami,Naoki Matsumoto,Takamasa Yashima*

Main category: cs.DM

TL;DR: 本文综述和分析了两个平面图重要猜想及其相关推广，厘清不同结构之间的相互联系，提出相关猜想的反例，并建立诱导子图最大阶数的最优界限，以及与树宽相关的一般上界，对平面图结构性问题提供了新的理论进展。


<details>
  <summary>Details</summary>
Motivation: Albertson-Berman猜想与Matheson-Tarjan猜想是图论中长期未解的重要猜想，现有部分结果仅限于较弱界或特殊情况。因此，探讨这些猜想的推广、变体及其之间的关系，有助于推动猜想的整体解决进展。

Method: 本文通过综述现有猜想和研究，系统梳理了相关的图论概念，并对部分相关猜想给出实际的反例，进一步利用树宽等重要图论量建立与诱导子图阶数相关的普适性界。

Result: 作者提出并验证了相关图论结构条件下诱导子图阶数的最佳界限，讨论了不同猜想和条件下的结构联系，部分猜想证伪并由具体反例支撑，同时将树宽引入诱导子图阶数的研究中，获得了理论上的一般性上界。

Conclusion: 本文澄清了与Albertson-Berman猜想和Matheson-Tarjan猜想相关的多个概念之间的关系，并针对某些相关猜想给出了反例，同时在不同结构条件下，建立了诱导子图最大阶数之间间隙的最佳界，并提出了基于树宽的诱导子图阶数的一般上界。

Abstract: Two of the most prominent unresolved conjectures in graph theory, the
Albertson-Berman conjecture and the Matheson-Tarjan conjecture, have been
extensively studied by many researchers.
  (AB) Every planar graph of order $n$ has an induced forest of order at least
$\frac{n}{2}$.
  (MT) Every plane triangulation of sufficiently large order $n$ has a
dominating set of cardinality at most $\frac{n}{4}$.
  Although partial results and weaker bounds than those originally conjectured
have been obtained, both problems remain open. To contribute to their
resolution, various generalizations and variations of the original concepts
have been investigated, such as total dominating set, induced linear forests,
and others. In this paper, we clarify the relations among several notions
related to these two major conjectures, such as connected domination and
induced outerplanar subgraphs, etc., and survey the associated conjectures. We
then provide counterexamples to some of these conjectures and establish the
best bounds on the gap between the maximum orders of induced subgraphs under
different structural conditions. In addition, we present a general upper bound
on the order of induced subgraphs in terms of treewidth, a fundamental graph
invariant.

</details>


### [70] [The LLLR generalised Langton's ant](https://arxiv.org/abs/2506.10482)
*Victor Lutfalla*

Main category: cs.DM

TL;DR: 本文简要讨论了LLLR变体Langton蚂蚁的问题，并指出其动力学上存在两种不同的长期演化行为。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在分析和探讨LLLR广义Langton蚂蚁（Langton's ant）在动力学方面的表现。Langton蚂蚁是一种著名的元胞自动机模型，而本研究关注其LLLR变体的扩展行为。

Method: 通过理论分析以及对LLLR蚂蚁规则的模拟，作者研究了这种变体在长期演化中的不同可能状态。主要采用动力学观察和状态归纳的方法。

Result: 发现LLLR蚂蚁表现出两种不同的渐近行为，即其在长期演化时可能趋向于不同的动力学模式。论文对这两个行为进行了简要描述。

Conclusion: LLLR广义Langton蚂蚁拥有两种不同的渐近（asymptotic）动力学行为，为元胞自动机研究提供了新的视角，也扩展了对复杂系统动力学的理解。

Abstract: We present a short note on the dynamics of the LLLR generalised Langton's
ant. We describe two different asymptotic behaviours for the LLLR ant.

</details>


### [71] [Circulant TSP: Vertices of the Edge-Length Polytope and Superpolynomial Lower Bounds](https://arxiv.org/abs/2506.10758)
*Samuel C. Gutekunst*

Main category: cs.DM

TL;DR: 论文通过分析Circulant TSP中的边长多面体，发现其结构与n的分解紧密相关，部分情形下可以用效率较高的暴力算法；同时提升了与Buratti-Horak-Rosa猜想相关问题的理解。


<details>
  <summary>Details</summary>
Motivation: 本文的研究动机来自对循环旅行商问题（Circulant TSP）的算法研究以及与Buratti-Horak-Rosa猜想相关的数论研究。Circulant TSP是TSP的一个特殊情形，其复杂性是个重要但尚未解决的问题。边长多面体(edge-length polytope)为解决Circulant TSP实例提供了潜在方向。

Method: 作者分析了Circulant TSP中的边长多面体结构，并研究了其顶点数量与参数n的素因子分解之间的关系。同时，给出了与Buratti-Horak-Rosa猜想相关的组合数列的下界。

Result: 当n为素数时，多面体顶点数量按n增长；当n为素数的平方时，顶点数量按n^{3/2}增长；当n为2的幂时，顶点数量为超多项式数量。而更标准的对称TSP多面体有约n!个顶点。另外，对于某些n的分解情形，暴力算法反而高效。作者还给出了Buratti-Horak-Rosa猜想相关的两类组合数列的超多项式下界。

Conclusion: 通过对边长多面体的结构分析，揭示了其顶点数量与参数n的分解性质紧密相关，这对Circulant TSP的求解复杂性有重要影响，也推动了与Buratti-Horak-Rosa猜想相关的组合研究。

Abstract: We study the edge-length polytope, motivated both by algorithmic research on
the Circulant Traveling Salesman Problem (Circulant TSP) and number-theoretic
research related to the Buratti-Horak-Rosa conjecture. Circulant TSP is a
special case of TSP whose overall complexity is a significant still-open
question, and where on an input with vertices $\{1, 2, ..., n\}$, the cost of
an edge $\{i, j\}$ depends only on its length $\min\{|i-j|, n-|i-j|\}$. The
edge-length polytope provides one path to solving circulant TSP instances, and
we show that it is intimately connected to the factorization of $n$: the number
of vertices scales with $n$ whenever $n$ is prime and with $n^{3/2}$ whenever
$n$ is a prime-squared, but there are a superpolynomial number of vertices
whenever $n$ is a power of 2. In contrast, the more-standard Symmetric TSP
Polytope has roughly $n!$ vertices. Hence, for Circulant TSP, a brute-force
algorithm checking every vertex is actually efficient in some cases, based on
the factorization of $n$. As an intermediate step, we give superpolynomial
lower-bounds on two combinatorial sequences related to the Buratti-Horak-Rosa
conjecture, which asks what combinations of edge lengths can comprise a
Hamiltonian path.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [72] [Chance and Mass Interpretations of Probabilities in Markov Decision Processes (Extended Version)](https://arxiv.org/abs/2506.10377)
*Yun Chen Tsai,Kittiphon Phalakarn,S. Akshay,Ichiro Hasuo*

Main category: cs.FL

TL;DR: 文章提出四种MDP的统一语义框架，并用CM分类器系统整合。对新语义下可达性问题给出复杂性分析及两种算法。


<details>
  <summary>Details</summary>
Motivation: 马尔可夫决策过程（MDP）作为一个在不确定性下决策的重要模型，传统语义主要关注概率分布在状态序列上的变换，并假设调度器作出随机选择。但在对动力系统建模中，已有对MDP分布转化的不同视角，但总体上缺乏统一的语义框架来囊括多种解释。

Method: 提出了一个统一的语义框架，通过定义CM（chance-mass）分类器，将四种MDP的语义自然地纳入进来。具体方法：从调度器、配置和转移三种随机性来源入手，将概率的不同解释方式（机会interpretation与质量interpretation）系统地整合。随后，针对新提出的两种语义，分别研究其可达性问题的复杂性，并提出两种算法加以求解。

Result: 成功提出了统一四种MDP语义的CM分类器数学框架。关于新提出语义下的可达性问题，证明了其复杂性，并针对性地提供了两套解决算法。

Conclusion: 本文兼容并统一了MDP在不同随机性解释下的四种语义，丰富了MDP在动力系统建模和验证中的应用基础，并为新语义条件下的可达性问题提供了理论与算法两方面的支持。

Abstract: Markov decision processes (MDPs) are a popular model for decision-making in
the presence of uncertainty. The conventional view of MDPs in verification
treats them as state transformers with probabilities defined over sequences of
states and with schedulers making random choices. An alternative view,
especially well-suited for modeling dynamical systems, defines MDPs as
distribution transformers with schedulers distributing probability masses. Our
main contribution is a unified semantical framework that accommodates these two
views and two new ones. These four semantics of MDPs arise naturally through
identifying different sources of randomness in an MDP (namely schedulers,
configurations, and transitions) and providing different ways of interpreting
these probabilities (called the chance and mass interpretations). These
semantics are systematically unified through a mathematical construct called
chance-mass (CM) classifier. As another main contribution, we study a
reachability problem in each of the two new semantics, demonstrating their
hardness and providing two algorithms for solving them.

</details>


### [73] [Minimality and computability of languages of G-shifts](https://arxiv.org/abs/2506.10610)
*Djamel Eddine Amir,Benjamin Hellouin de Menibus*

Main category: cs.FL

TL;DR: 本文提出并刻画了$G$-shifts（$G$为有限生成群）的强可计算类型，证明其理论在直积下封闭，统一和推广了现有相关理论，并为今后研究指出了新方向。


<details>
  <summary>Details</summary>
Motivation: 受到可计算分析中集合的强可计算类型概念的启发，作者希望将此思想推广到$G$-shifts（$G$为具有可判定字问题的有限生成群），以扩展理论的适用范围和理解。

Method: 定义了$G$-shifts的强可计算类型，并用与某类有界计算复杂度的性质相关的极小性来刻画具有强可计算类型的$G$-shifts。作者提供了自洽的直接证明，同时讨论了与Amir和Hoyrup对于集合的类似刻画的关系，以及与Jeandel关于闭包空间的研究联系。此外，将该刻画应用于多类与特定性质相关的极小shifts。

Result: 提出了$G$-shifts强可计算类型的新理论，并证明了与集合情形的不同点：即$G$-shifts的强可计算类型在取直积时可以保留。提供了统一和简化的刻画，并推动了相关研究的推广和深化。

Conclusion: 本文推广并统一了$G$-shifts强可计算类型的理论，为研究具有特定计算复杂度的动力系统提供了新的工具，并指出了后续的推广和研究方向。

Abstract: Motivated by the notion of strong computable type for sets in computable
analysis, we define the notion of strong computable type for $G$-shifts, where
$G$ is a finitely generated group with decidable word problem. A $G$-shift has
strong computable type if one can compute its language from the complement of
its language. We obtain a characterization of $G$-shifts with strong computable
type in terms of a notion of minimality with respect to properties with a
bounded computational complexity. We provide a self-contained direct proof, and
also explain how this characterization can be obtained from an existing similar
characterization for sets by Amir and Hoyrup, and discuss its connexions with
results by Jeandel on closure spaces. We apply this characterization to several
classes of shifts that are minimal with respect to specific properties. This
provides a unifying approach that not only generalizes many existing results
but also has the potential to yield new findings effortlessly. In contrast to
the case of sets, we prove that strong computable type for G-shifts is
preserved under products. We conclude by discussing some generalizations and
future directions.

</details>


### [74] [Chance and Mass Interpretations of Probabilities in Markov Decision Processes (Extended Version)](https://arxiv.org/abs/2506.10377)
*Yun Chen Tsai,Kittiphon Phalakarn,S. Akshay,Ichiro Hasuo*

Main category: cs.FL

TL;DR: 作者提出了统一Markov决策过程（MDP）多种语义的数学框架，系统化了四种MDP语义，并针对新语义下的可达性问题提出算法和复杂性分析，为MDP建模和分析提供了更强大的理论工具。


<details>
  <summary>Details</summary>
Motivation: 传统的Markov决策过程（MDP）通常以两种方式建模，但在动力系统建模等实际应用中，这些方式存在局限性，因此需要统一和扩展MDP的语义描述。本文动机在于提出一个统一的框架，能同时囊括多种MDP的语义，并解决由不同随机源带来的建模和分析挑战。

Method: 作者提出了一个统一语义框架，通过识别MDP中不同的随机源（如调度器、配置、转移）以及概率的不同解释（机会概率和质量概率），提出了四种语义。利用数学结构'机会-质量（CM）分类器'将这些语义系统化统一。此外，针对两种新的语义，作者研究了可达性问题，并给出其计算复杂性以及两种求解算法。

Result: 文章成功提出并系统化了四种MDP语义，并通过CM分类器统一。针对两种新提出的MDP语义，作者证明了其可达性问题的难度，并给出两种算法以求解该问题。

Conclusion: 本文提出的统一语义框架为MDP的理解和分析提供了更为广泛和系统的工具，尤其适用于需要区分和利用不同随机源的实际应用。引入的两种新语义拓宽了MDP的应用范围，也带来了新的理论和算法挑战。

Abstract: Markov decision processes (MDPs) are a popular model for decision-making in
the presence of uncertainty. The conventional view of MDPs in verification
treats them as state transformers with probabilities defined over sequences of
states and with schedulers making random choices. An alternative view,
especially well-suited for modeling dynamical systems, defines MDPs as
distribution transformers with schedulers distributing probability masses. Our
main contribution is a unified semantical framework that accommodates these two
views and two new ones. These four semantics of MDPs arise naturally through
identifying different sources of randomness in an MDP (namely schedulers,
configurations, and transitions) and providing different ways of interpreting
these probabilities (called the chance and mass interpretations). These
semantics are systematically unified through a mathematical construct called
chance-mass (CM) classifier. As another main contribution, we study a
reachability problem in each of the two new semantics, demonstrating their
hardness and providing two algorithms for solving them.

</details>


### [75] [Minimality and computability of languages of G-shifts](https://arxiv.org/abs/2506.10610)
*Djamel Eddine Amir,Benjamin Hellouin de Menibus*

Main category: cs.FL

TL;DR: 本文推广了强可计算类型的概念至$G$-shift，并给出其与复杂性下极小性的等价刻画，统一和推广了多项已有结论，发现强可计算类型在直积下保持不变，为相关领域后续研究提供了新工具和方向。


<details>
  <summary>Details</summary>
Motivation: 受可计算分析中集合的强可计算类型概念启发，作者希望将该概念推广到$G$-shifts，即在有限生成、词问题可判定群$G$的作用下的子移空间，以研究其可计算性的刻画及性质。

Method: 作者定义了$G$-shift的强可计算类型，并通过对其语言与补语言之间的可计算关系进行研究，得出了强可计算类型与有界计算复杂性下极小性（minimality）之间的刻画。同时，作者给出了自洽的直接证明，并对比了Amir与Hoyrup关于集合的相关刻画，分析了与Jeandel在闭包空间结果的联系。作者还将这种刻画方法应用于多类具体极小shift的例子，分析其统一性和推广性。

Result: 本文主要结果为：$G$-shift具备强可计算类型当且仅当它对某些有界复杂性性质的极小性成立。作者还证明了，与集合情形不同，$G$-shifts的强可计算类型在直积（product）操作下保持不变。此外，给出了一些具体shift类应用实例，并展望了推广方向。

Conclusion: 本文将强可计算类型的概念自然推广到$G$-shift，并建立了其与复杂性极小性的等价刻画。这一方法统一了大量已有相关结果，并揭示了$G$-shifts在可计算性理论中的稳健性，为后续进一步研究奠定了基础，也指出了若干未来可能的推广方向。

Abstract: Motivated by the notion of strong computable type for sets in computable
analysis, we define the notion of strong computable type for $G$-shifts, where
$G$ is a finitely generated group with decidable word problem. A $G$-shift has
strong computable type if one can compute its language from the complement of
its language. We obtain a characterization of $G$-shifts with strong computable
type in terms of a notion of minimality with respect to properties with a
bounded computational complexity. We provide a self-contained direct proof, and
also explain how this characterization can be obtained from an existing similar
characterization for sets by Amir and Hoyrup, and discuss its connexions with
results by Jeandel on closure spaces. We apply this characterization to several
classes of shifts that are minimal with respect to specific properties. This
provides a unifying approach that not only generalizes many existing results
but also has the potential to yield new findings effortlessly. In contrast to
the case of sets, we prove that strong computable type for G-shifts is
preserved under products. We conclude by discussing some generalizations and
future directions.

</details>
