<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [RightTyper: Effective and Efficient Type Annotation for Python](https://arxiv.org/abs/2507.16051)
*Juan Altmayer Pizzorno,Emery D. Berger*

Main category: cs.PL

TL;DR: RightTyper 用动态采样+统计方法，为未标注 Python 代码高效生成精准类型注解，把类型检查变成异常智能检测，性能开销小，效果更好。


<details>
  <summary>Details</summary>
Motivation: Python 静态类型标注有诸多好处，但手工注释繁琐，现有自动类型推断方法要么不精确，要么性能开销大，要么无法发现用户自定义和稀有类型，且大多假定代码本身已正确，这对于大规模未注释代码库普遍不成立。

Method: RightTyper 基于采样的动态分析，利用自我分析、自适应采样策略、统计过滤和细粒度类型信息的聚合，既避免了传统动态方法的高开销，又保障了推断的精确性和高召回率。

Result: RightTyper 相比以往方法能够更全面地生成精确类型信息，并将类型异常检测融入代码分析中，平均仅带来 30% 性能开销，大幅提升了类型推断的可靠性和实用性。

Conclusion: RightTyper 能有效生成精确的 Python 类型注解，同时还能将类型检查转变为异常检测，从而帮助开发者发现潜在的程序行为异常。该方法高效、性能开销低。

Abstract: Python type annotations bring the benefits of static type checking to the
language. However, manually writing annotations can be time-consuming and
tedious. The result is that most real-world Python code remains largely
untyped. Past approaches to annotating types in Python code fall short in a
number of ways. Static approaches struggle with dynamic features and infer
overly broad types. AI-based methods are inherently unsound and can miss rare
or user-defined types. Dynamic methods can impose extreme runtime overheads,
degrading performance by up to 270x, abort execution as they exhaust resources,
and even infer incorrect types that lead to runtime errors. Crucially, all
prior work assumes implicitly that the code to be annotated is already correct.
This assumption is generally unwarranted, especially for large codebases that
have been untyped.
  This paper presents RightTyper, a novel approach for Python that overcomes
these disadvantages. RightTyper not only generates precise type annotations
based on actual program behavior, improving recall in type checking relative to
prior approaches. It also turns type checking into anomaly detection, allowing
the type checker to identify corner cases that the programmer can audit for
unintended behavior. RightTyper is also fast and space-efficient, imposing just
30% performance overhead on average. RightTyper achieves these characteristics
by a principled yet pervasive use of sampling--guided by self-profiling--along
with statistical filtering and careful resolution and aggregation of type
information.

</details>


### [2] [Understanding Haskell-style Overloading via Open Data and Open Functions](https://arxiv.org/abs/2507.16086)
*Andrew Marmaduke,Apoorv Ingle,J. Garrett Morris*

Main category: cs.PL

TL;DR: 作者以System F$_\mathrm{D}$为基础，提出了Haskell类型类重载的新统一语义，具备更高表达能力，并实现了形式化验证，无需额外类型等价假设。


<details>
  <summary>Details</summary>
Motivation: 当前Haskell风格的重载语义在表达能力和形式化方面存在局限，尤其是在处理类型类高级特性时，现有语义需要额外的类型等价公理。本文旨在提出一种更统一且具备更强表达能力的重载语义。

Method: 作者提出了一种新的核心语言System F$_\mathrm{D}$，该语言通过集合性的实例（而非单一定义）来表达开放数据类型和开放函数，并在Lean4定理证明器中形式化了其元理论。

Result: System F$_\mathrm{D}$能够更为灵活和表达性强地编码Haskell类型类系统中的高级特性，并且无需假设额外的类型等价公理。

Conclusion: 提出的System F$_\mathrm{D}$为Haskell风格重载和类型类系统提供了更统一且表达能力更强的语义基础，对相关功能的形式化机制有积极推动。

Abstract: We present a new, uniform semantics for Haskell-style overloading. We realize
our approach in a new core language, System F$_\mathrm{D}$, whose metatheory we
mechanize in the Lean4 interactive theorem prover. System F$_\mathrm{D}$ is
distinguished by its open data types and open functions, each given by a
collection of instances rather than by a single definition. We show that System
F$_\mathrm{D}$ can encode advanced features of Haskell's of type class systems,
more expressively than current semantics of these features, and without
assuming additional type equality axioms.

</details>


### [3] [Querying Graph-Relational Data](https://arxiv.org/abs/2507.16089)
*Michael J. Sullivan,Zhibo Chen,Elvis Pranskevichus,Robert J. Simmons,Victor Petrovykh,Aljaž Mur Eržen,Yury Selivanov*

Main category: cs.PL

TL;DR: 该论文提出graph-relational数据库模型，解决了传统关系数据库与对象模型间的数据结构不匹配问题，并通过EdgeQL与Gel系统实现，高效支持复杂数据访问，兼顾易用性与性能。


<details>
  <summary>Details</summary>
Motivation: 关系型数据库虽然易于存储结构化数据，但其“扁平化”模式与应用程序预期获取的深度嵌套信息之间存在不匹配（即对象-关系阻抗不匹配问题）。

Method: 提出了graph-relational数据库模型，并对其查询的静态和动态语义进行了形式化定义。此外，将该模型落地到EdgeQL查询语言和Gel系统，后者可将EdgeQL编译为PostgreSQL查询。

Result: Gel系统能够高效支持对象形态的数据操作，大大提升了现有ORM方法的效率，并减少了手动编写复杂SQL的需求。

Conclusion: graph-relational数据库模型能高效、灵活并强类型地解决对象-关系阻抗不匹配问题，并通过EdgeQL和Gel系统实现落地，结合了ORM的易用性和手写SQL的高效性。

Abstract: For applications that store structured data in relational databases, there is
an impedance mismatch between the flat representations encouraged by relational
data models and the deeply nested information that applications expect to
receive. In this work, we present the graph-relational database model, which
provides a flexible, compositional, and strongly-typed solution to this
"object-relational mismatch." We formally define the graph-relational database
model and present a static and dynamic semantics for queries. In addition, we
discuss the realization of the graph-relational database model in EdgeQL, a
general-purpose SQL-style query language, and the Gel system, which compiles
EdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of
object-shaped data manipulation that is frequently provided inefficiently by
object-relational mapping (ORM) technologies, while achieving most of the
efficiency that comes from require writing complex PostgreSQL queries directly.

</details>


### [4] [Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs](https://arxiv.org/abs/2507.16660)
*Xuran Cai*

Main category: cs.PL

TL;DR: 本文提出了SPL分解框架，有效提升了寄存器分配、程序冗余消除和数据检索效率，相比传统方法具有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的编译器优化（如寄存器分配和生命周期最优的投机部分冗余消除）常用树分解算法，但这些方法常常忽略控制流图的稀疏性，导致计算代价高昂。

Method: 提出了SPL（串-并-循环）分解的新框架，并建立了图结构中部分约束满足问题（PCSP）的通用解决方案。该方法分别应用于三个优化问题。

Result: SPL分解通过更准确地建模变量干扰图，提高了寄存器分配效率和程序性能。同时，优化了LOSPRE，使程序执行中的冗余更有效地被消除。另外优化了银行选择指令的放置，提高了数据检索效率并减少延迟。

Conclusion: 实验表明，SPL分解在多个基准测试中优于现有方法，是复杂编译器优化（包括寄存器分配、LOSPRE和银行选择）的有力工具。

Abstract: This thesis addresses the complexities of compiler optimizations, such as
register allocation and Lifetime-optimal Speculative Partial Redundancy
Elimination (LOSPRE), which are often handled using tree decomposition
algorithms. However, these methods frequently overlook important sparsity
aspects of Control Flow Graphs (CFGs) and result in high computational costs.
We introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework
that offers optimal solutions to these challenges. A key contribution is the
formulation of a general solution for Partial Constraint Satisfaction Problems
(PCSPs) within graph structures, applied to three optimization problems. First,
SPL decomposition enhances register allocation by accurately modeling variable
interference graphs, leading to efficient register assignments and improved
performance across benchmarks. Second, it optimizes LOSPRE by effectively
identifying and eliminating redundancies in program execution. Finally, the
thesis focuses on optimizing the placement of bank selection instructions to
enhance data retrieval efficiency and reduce latency. Extensive experimentation
demonstrates significant performance improvements over existing methods,
establishing SPL decomposition as a powerful tool for complex compiler
optimizations, including register allocation, LOSPRE, and bank selection.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
*Ori Press,Brandon Amos,Haoyu Zhao,Yikai Wu,Samuel K. Ainsworth,Dominik Krupke,Patrick Kidger,Touqir Sajed,Bartolomeo Stellato,Jisun Park,Nathanael Bosch,Eli Meril,Albert Steppi,Arman Zharmagambetov,Fangzhao Zhang,David Perez-Pineiro,Alberto Mercurio,Ni Zhan,Talor Abramovich,Kilian Lieret,Hanlin Zhang,Shirley Huang,Matthias Bethge,Ofir Press*

Main category: cs.SE

TL;DR: 提出了针对算法创新能力的新基准AlgoTune，并展示了现有大模型缺乏创新但能实现一定速度提升，旨在激发更具创新性的语言模型研究。


<details>
  <summary>Details</summary>
Motivation: 当前大模型语言模型（LM）的评估主要集中在人类已知如何解决的任务上，而未能衡量其在开放性、创新性算法设计与实现方面的能力。为推动该领域的发展，需要一种能够激发模型创新性和复杂度理解的全新评测方法。

Method: 本文提出了AlgoTune基准，包括155个由领域专家收集的编码任务，跨计算机科学、物理与数学等学科。基准框架提供了解决方案代码的验证和计时机制，并将模型合成代码与主流开源库的参考实现进行比较。此外，作者开发了基线语言模型代理AlgoTuner，并在多种前沿模型上进行了评测。

Result: AlgoTuner在基准测试中平均比参考解算器（如SciPy、sk-learn、CVXPY等库）快1.72倍。但现有模型更倾向于表层优化，尚未展现出算法创新能力。

Conclusion: 本文的AlgoTune基准及评测结果表明当前大模型在算法创新与深层次问题求解方面仍有较大提升空间。希望该基准能推动更具创造力和超越人类表现的模型代理的发展。

Abstract: Despite progress in language model (LM) capabilities, evaluations have thus
far focused on models' performance on tasks that humans have previously solved,
including in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,
2024). We therefore propose testing models' ability to design and implement
algorithms in an open-ended benchmark: We task LMs with writing code that
efficiently solves computationally challenging problems in computer science,
physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks
collected from domain experts and a framework for validating and timing
LM-synthesized solution code, which is compared to reference implementations
from popular open-source packages. In addition, we develop a baseline LM agent,
AlgoTuner, and evaluate its performance across a suite of frontier models.
AlgoTuner achieves an average 1.72x speedup against our reference solvers,
which use libraries such as SciPy, sk-learn and CVXPY. However, we find that
current models fail to discover algorithmic innovations, instead preferring
surface-level optimizations. We hope that AlgoTune catalyzes the development of
LM agents exhibiting creative problem solving beyond state-of-the-art human
performance.

</details>


### [6] [Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing](https://arxiv.org/abs/2507.15889)
*Noah van der Vleuten*

Main category: cs.SE

TL;DR: 本文提出通过自举算法训练程序合成语言模型以获得修复能力，相较于常规微调显著提升模型性能，堪比更大规模的微调模型，同时指出APPS数据集在训练测试用例中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成语言模型通常依赖编程竞赛数据集（如MBPP、APPS）进行训练和评估，这些数据集体量小且质量有限，而大语言模型需要大量数据。此外，当前模型一次性生成代码的方式与人类通过编译器迭代开发代码的过程不符。

Method: 本文提出了一种用于程序合成的自举（bootstrapping）算法，使模型能够学习如何修复代码错误，即教会模型修复的能力。该方法与常规微调（fine-tuning）方法进行比较，并在模型推理阶段考察了修复行为的作用。

Result: 自举算法在实验中效果普遍优于常规微调，与比自身大68%的微调模型性能相当。在推理阶段，带有修复能力的自举还能提升不修复时的性能。然而，在推理时直接修复不如简单多采样解答的性能。此外，APPS数据集训练部分的测试用例存在问题，这对依赖这些数据的相关方法有影响。

Conclusion: 带有修复能力的自举算法显著提升了程序合成模型的性能，体现出修复能力的迁移增益，并暴露了当前数据集存在的缺陷。

Abstract: Language models for program synthesis are usually trained and evaluated on
programming competition datasets (MBPP, APPS). However, these datasets are
limited in size and quality, while these language models are extremely data
hungry. Additionally, the language models have a misaligned program synthesis
process compared to humans. While humans iteratively develop code with the help
of a compiler, most program synthesis models currently produce code in one go.
To solve these issues, we introduce a bootstrapping algorithm for program
synthesis, that supports teaching models how to repair. We show that
bootstrapping consistently outperforms regular fine-tuning. Compared to other
work, our bootstrapped model performs on par with fine-tuned models that are
68\% larger. Notably, bootstrapping with repairing also improves non-repairing
performance compared to regular bootstrapping during inference. However, on our
models, repairing during inference is likely inferior to simply sampling the
same number of solutions. Furthermore, we find that there are issues with the
example test cases in the training portion of the APPS dataset that are
valuable to the community, as many repairing and reinforcement learning methods
rely on them.

</details>


### [7] [StaAgent: An Agentic Framework for Testing Static Analyzers](https://arxiv.org/abs/2507.15892)
*Elijah Nnorom,Md Basim Uddin Ahmed,Jiho Shin,Hung Viet Pham,Song Wang*

Main category: cs.SE

TL;DR: 本文提出StaAgent，一个利用LLM驱动、面向智能体的静态分析器规则缺陷检测框架。通过自动生成测试代码与变异体并进行行为比对，有效发现主流分析器中的隐蔽问题，实现了比传统方法更高的覆盖和缺陷发现能力。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具对检测软件开发早期的bug至关重要，但其规则实现常存在测试不足和不一致性问题，影响了工具的可靠性。

Method: 提出StaAgent框架，结合大型语言模型（LLMs）的生成能力，通过四个专用智能体自动化地评估静态分析器的规则：1. 种子生成Agent生成具体现bug的测试代码；2. 代码验证Agent验证种子的正确性；3. 变异生成Agent产生等价变异体；4. 分析器评估Agent通过比较分析器对种子与变异体的处理，进行变形测试，从而发现规则实现中的缺陷。

Result: 在五款主流LLM与五个常用静态分析器上的实验显示，该方法揭示了64个问题规则，其中53个无法被现有最优方法检测。部分问题已被开源社区修复或确认。

Conclusion: StaAgent展现了基于LLM的多Agent框架在暴露静态分析器实现缺陷上的有效性，并为软件工程中的数据合成与自动测试提供了可扩展的解决方案。

Abstract: Static analyzers play a critical role in identifying bugs early in the
software development lifecycle, but their rule implementations are often
under-tested and prone to inconsistencies. To address this, we propose
StaAgent, an agentic framework that harnesses the generative capabilities of
Large Language Models (LLMs) to systematically evaluate static analyzer rules.
StaAgent comprises four specialized agents: a Seed Generation Agent that
translates bug detection rules into concrete, bug-inducing seed programs; a
Code Validation Agent that ensures the correctness of these seeds; a Mutation
Generation Agent that produces semantically equivalent mutants; and an Analyzer
Evaluation Agent that performs metamorphic testing by comparing the static
analyzer's behavior on seeds and their corresponding mutants. By revealing
inconsistent behaviors, StaAgent helps uncover flaws in rule implementations.
This LLM-driven, multi-agent framework offers a scalable and adaptable solution
to improve the reliability of static analyzers. We evaluated StaAgent with five
state-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)
across five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,
Infer, and PMD). The experimental results show that our approach can help
reveal 64 problematic rules in the latest versions of these five static
analyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,
and 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the
SOTA baseline. We have reported all the bugs to developers, with two of them
already fixed. Three more have been confirmed by developers, while the rest are
awaiting response. These results demonstrate the effectiveness of our approach
and underscore the promise of agentic, LLM-driven data synthesis to advance
software engineering.

</details>


### [8] [A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights](https://arxiv.org/abs/2507.16037)
*Zhili Zeng,Kimya Khakzad Shahandashti,Alvine Boaye Belle,Song Wang,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 本文评估了LLM驱动的智能代理链在Android到iOS应用迁移中的效果，通过人工评测和失败分析，提出了具体改进建议，为自动化跨平台翻译提供了理论与实践支持。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用的快速发展，对跨平台兼容性的需求激增，尤其是Android和iOS之间。然而，当前的应用迁移方法多为手动或规则驱动，效率低且易出错。虽然机器学习方法有一定进展，但对于上下文理解和自适应性仍有不足亟需改进。近年来，大语言模型（LLMs）引入了自动化代码翻译，但其在跨平台应用（如Android转iOS）翻译领域研究仍有限。本文旨在弥补该领域的研究空白。

Method: 本文提出了一种基于LLM的智能代理链方法，在Android到iOS应用迁移中充分考虑依赖关系、规范、程序结构和控制流。通过人工检查译码的语法正确性、语义准确性和功能完整性进行评估，并对翻译失败案例做了细致的根因分析。

Result: 实验揭示了LLM智能代理方法在移动应用跨平台迁移中的表现、优势及不足。尤其通过人工评测与失败案例分析，明确了方法的关键失效点，并提出了优化建议。

Conclusion: LLM智能代理方法在Android到iOS应用迁移中表现出一定潜力，但仍存在失败点和局限，需要针对依赖、结构等方面继续优化。研究为未来跨平台自动化翻译提供了改进方向和参考。

Abstract: The rapid advancement of mobile applications has led to a significant demand
for cross-platform compatibility, particularly between the Android and iOS
platforms. Traditional approaches to mobile application translation often rely
on manual intervention or rule-based systems, which are labor-intensive and
time-consuming. While recent advancements in machine learning have introduced
automated methods, they often lack contextual understanding and adaptability,
resulting in suboptimal translations. Large Language Models (LLMs) were
recently leveraged to enhance code translation at different granularities,
including the method, class, and repository levels. Researchers have
investigated common errors, limitations, and potential strategies to improve
these tasks. However, LLM-based application translation across different
platforms, such as migrating mobile applications between Android and iOS or
adapting software across diverse frameworks, remains underexplored.
Understanding the performance, strengths, and limitations of LLMs in
cross-platform application translation is critical for advancing software
engineering automation. This study aims to fill this gap by evaluating
LLM-based agentic approaches for mobile application translation, identifying
key failure points, and proposing guidelines to improve translation
performance. We developed a chain of agents that account for dependencies,
specifications, program structure, and program control flow when translating
applications from Android to iOS. To evaluate the performance, we manually
examined the translated code for syntactic correctness, semantic accuracy, and
functional completeness. For translation failures, we further conducted a
detailed root cause analysis to understand the underlying limitations of the
agentic translation process and identify opportunities for improvement.

</details>


### [9] [Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs](https://arxiv.org/abs/2507.16044)
*Meriem Mastouri,Emna Ksontini,Wael Kessentini*

Main category: cs.SE

TL;DR: 提出AutoMCP工具，通过OpenAPI规范自动生成MCP服务器，大幅简化开发流程。实验覆盖50个真实API、5,066端点，初次自动化成功率76.5%，经小修正达99.9%。分析总结了MCP应用现状、自动化瓶颈及OpenAPI规范常见问题。


<details>
  <summary>Details</summary>
Motivation: LLMs正从被动的文本生成转向主动代理，需要动态调用外部工具。Anthropic提出的MCP（Model Context Protocol）虽然为工具集成提供了标准，但实际构建MCP服务器过程繁琐、重复性高，阻碍了协议的广泛应用。研究动因在于探索如何简化和自动化MCP服务器的开发过程。

Method: 本文首先通过对22,000余个MCP-tagged GitHub项目的分析，研究手写MCP服务器带来的重复性劳动及低采纳率。接着提出AutoMCP——一个能够将OpenAPI 2.0/3.0规范编译为完整MCP服务器的工具，并在跨10个领域、50个真实API、5,066个接口上进行实验验证。通过抽样1,023个API调用，统计成功率、归因失败类型，并评估小规模规范修复对自动化效果的提升。

Result: AutoMCP自动生成的服务器实现覆盖了API定义、架构注册、鉴权等功能，开箱即用成功率为76.5%。主要失败原因是OpenAPI规范中的不一致或遗漏。进行小幅度规范修正（平均19行/规范）后，成功率提升至99.9%。归纳整理了常见OpenAPI缺陷类型，为未来标准化提供参考。

Conclusion: （i）全面分析了MCP的实际应用及人工开发的高成本；（ii）证明OpenAPI几乎可以完全自动化MCP服务端开发（经少量修正）；（iii）贡献了5,066个可调用工具集与API修复经验，为工具生态发展与标准改进提供数据基础与实践建议。

Abstract: Large Language Models (LLMs) are evolving from passive text generators into
active agents that invoke external tools. To support this shift, scalable
protocols for tool integration are essential. The Model Context Protocol (MCP),
introduced by Anthropic in 2024, offers a schema-driven standard for dynamic
tool discovery and invocation. Yet, building MCP servers remains manual and
repetitive, requiring developers to write glue code, handle authentication, and
configure schemas by hand-replicating much of the integration effort MCP aims
to eliminate.
  This paper investigates whether MCP server construction can be meaningfully
automated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged
GitHub repositories created within six months of release, fewer than 5% include
servers, typically small, single-maintainer projects dominated by repetitive
scaffolding. To address this gap, we present AutoMCP, a compiler that generates
MCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API
definitions and produces complete server implementations, including schema
registration and authentication handling.
  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across
over 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded
out of the box. Manual failure analysis revealed five recurring issues, all
attributable to inconsistencies or omissions in the OpenAPI contracts. After
minor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%
success.
  Our findings (i) analyze MCP adoption and quantify the cost of manual server
development, (ii) demonstrate that OpenAPI specifications, despite quality
issues, enable near-complete MCP server automation, and (iii) contribute a
corpus of 5,066 callable tools along with insights on repairing common
specification flaws.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [10] [An Adequate While-Language for Stochastic Hybrid Computation](https://arxiv.org/abs/2507.15913)
*Renato Neves,José Proença,Juliana Souza*

Main category: cs.LO

TL;DR: 本文提出了一门支持微分与概率成分的编程语言，配套操作和指称语义，既有理论证明（充分性定理），又有实际解释器实现，并适用于如布朗运动等复杂系统的形式化建模。


<details>
  <summary>Details</summary>
Motivation: 当前存在对同时具有微分和概率性质的程序形式化推理的需求，比如自适应巡航控制、连续随机游走及物理过程（如布朗运动）。但是尚缺乏专门支持这类系统的形式化语言和相关的理论工具。

Method: 提出了一种新的编程语言，能够形式化描述和推理同时具有微分和概率成分的系统；为该语言定义了操作语义，并实现了匹配的解释器；同时给出了对应的指称语义，并证明了两者之间的充分性定理。

Result: 成功开发出一门能够统一处理微分与概率构造的程序语言，具有明确的操作和指称语义基础，两者之间的关系已通过充分性定理确立。此外，实现了该语言的解释器。

Conclusion: 文章为涉及微分方程和概率过程的程序分析与建模提供了一种统一语言及理论基础，解决了原本缺乏系统性工具的问题，对分析复杂物理和工程系统具有重要价值。

Abstract: We introduce a language for formally reasoning about programs that combine
differential constructs with probabilistic ones. The language harbours, for
example, such systems as adaptive cruise controllers, continuous-time random
walks, and physical processes involving multiple collisions, like in Einstein's
Brownian motion.
  We furnish the language with an operational semantics and use it to implement
a corresponding interpreter. We also present a complementary, denotational
semantics and establish an adequacy theorem between both cases.

</details>


### [11] [On Expansions of Monadic Second-Order Logic with Dynamical Predicates](https://arxiv.org/abs/2507.16581)
*Joris Nieuwveld,Joël Ouaknine*

Main category: cs.LO

TL;DR: 该论文证明结构⟨ℕ; <⟩添加特定动态一元谓词后，MSO理论仍可判定，并提出了可推广的新技术概念。


<details>
  <summary>Details</summary>
Motivation: 自1960年代Büchi与Elgot & Rabin开创性论文以来，关于结构⟨ℕ; <⟩的MSO理论扩展一直备受关注。本论文旨在探索向该结构添加“动态”一元谓词后的可判定性问题。

Method: 分析结构⟨ℕ; <,P⟩，其中P是由某类整数线性递归序列取值得到的集合。引入并应用了新的概念——(有效的) prodisjunctivity，作为主要技术工具。

Result: 证明了对于一大类动态一元谓词P，⟨ℕ; <,P⟩的MSO理论是可判定的。提出的prodisjunctivity的概念还具有潜在的独立应用价值。

Conclusion: 在MSO理论中，向⟨ℕ; <⟩结构添加由整数线性递归序列定义的动态谓词后，理论可判定，并引入了新的技术工具prodisjunctivity。

Abstract: Expansions of the monadic second-order (MSO) theory of the structure $\langle
\mathbb{N} ; < \rangle$ have been a fertile and active area of research ever
since the publication of the seminal papers of B\"uchi and Elgot & Rabin on the
subject in the 1960s. In the present paper, we establish decidability of the
MSO theory of $\langle \mathbb{N} ; <,P \rangle$, where $P$ ranges over a large
class of unary ''dynamical'' predicates, i.e., sets of non-negative values
assumed by certain integer linear recurrence sequences. One of our key
technical tools is the novel concept of (effective) prodisjunctivity, which we
expect may also find independent applications further afield.

</details>


### [12] [Transordinal Fixed-Point Operators and Self-Referential Games: A Categorical Framework for Reflective Semantic Convergence](https://arxiv.org/abs/2507.16620)
*Faruk Alpay,Hamdi Al Alakkad*

Main category: cs.LO

TL;DR: 本文提出统一的理论架构，结合范畴论不动点、超限递归与博弈语义，数学严密地解释了在无限自指与修正过程中语义如何稳定收敛。无需数据驱动，理论保证唯一自洽解释，对于形式语言学及具自我反思能力的语言系统设计具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有语义收敛与自反性问题往往依赖统计或实证方法，缺乏形式化且通用的理论描述。该文试图构建一个统一的理论框架，严密解释通过无限自指、修正如何获得稳定解释（语义收敛），并为形式语言学及自我推理型语言系统提供理论基础。

Method: 提出了一个融合范畴论、不动点构造、超限递归与博弈语义的新理论框架。通过在所有序数阶段迭代意义修正算子，最终获得唯一的“超序列”不动点，并借助反思性博弈层级证明该对象是文本与解释者之间无限对话的唯一均衡点。给出超序列机制的形式描述，存在性与唯一性定理的严格证明，并对照形式系统中的反思、真理与稳定均衡等问题进行理论联系。

Result: 理论上证明了意义修正过程必然收敛到唯一且自洽的解释，无需依赖经验数据或统计方法，对形式语言学和自我反省的语言系统设计算出了可验证的数学保障，连接了反映性、真理与语义均衡等深层问题。

Conclusion: 通过完全符号化的方法，首次严密描述了语义收敛机制，为语言解释的形式化研究提供统一、一般性的理论支撑，同时为构建能自动推理自身输出的语言智能系统奠定了基础。

Abstract: We present a new theoretical framework that unifies category-theoretic
fixed-point constructions, transfinite recursion, and game-based semantics to
model how interpretations of language can stabilize through unlimited
self-reference. By iterating a meaning-refinement operator across all ordinal
stages, we isolate a unique "transordinal" fixed point and show, via a
hierarchy of reflective games, that this same object is the sole equilibrium of
an infinite dialogue between a text and its interpreter. The result delivers a
mathematically rigorous account of semantic convergence without resorting to
statistical training or empirical benchmarks, yet remains simple to explain:
start with a rough meaning, let speaker and listener correct each other
forever, and the process provably settles on a single, self-consistent
interpretation. Because the construction is entirely symbolic, it offers both
precise guarantees for formal linguistics and a blueprint for designing
language-aware systems that can reason about their own outputs. The paper
details the requisite transordinal machinery, proves existence and uniqueness
theorems, and connects them to long-standing questions about reflection, truth,
and equilibrium in formal systems and semantics.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs](https://arxiv.org/abs/2507.15863)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.CL

TL;DR: DEREK模块实现了面向企业的高安全、可追溯的文档问答系统，结合混合检索、重排序和自动验证，在法律基准集上表现卓越，适合高风险领域生产部署。


<details>
  <summary>Details</summary>
Motivation: 企业在法律和金融等高风险领域对于文档问答系统的安全性、可追溯性和准确性有迫切需求，现有方法在处理异构内容和提升检索问答的可追溯性方面存在不足。

Method: 提出了DEREK模块，集成混合向量（HNSW+BM25）索引、Cohere重排序、LangGraph验证器，并利用LLM和CO-STAR提示词进行生成，所有环节均支持TLS 1.3与AES-256加密保障安全。系统能处理多类型文件，通过1000token重叠分块，索引与检索结合向量和关键词搜索，输出经验证和可追溯的答案。

Result: 系统在LegalBench四个子集上，1000token分块提升Recall@50约1个百分点，混合检索+重排序提升Precision@10约7个百分点，验证模块将TRACe Utilization提高到0.50以上，且不支持的陈述低于3%。模块稳定安全，运维成本低。

Conclusion: DEREK模块可为企业提供高安全、易追溯、高精度的文档问答流程，特别适用于法律与金融等高要求环境，具有实际生产可用性和推广价值。

Abstract: We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)
Module, a secure and scalable Retrieval-Augmented Generation pipeline designed
specifically for enterprise document question answering. Designed and
implemented by eSapiens, the system ingests heterogeneous content (PDF, Office,
web), splits it into 1,000-token overlapping chunks, and indexes them in a
hybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via
combined vector+BM25 search, reranked with Cohere, and answered by an LLM using
CO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,
regenerating answers until every claim is grounded. On four LegalBench subsets,
1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank
boosts Precision@10 by approximately 7 pp; the verifier raises TRACe
Utilization above 0.50 and limits unsupported statements to less than 3%. All
components run in containers, enforce end-to-end TLS 1.3 and AES-256. These
results demonstrate that the DEREK module delivers accurate, traceable, and
production-ready document QA with minimal operational overhead. The module is
designed to meet enterprise demands for secure, auditable, and context-faithful
retrieval, providing a reliable baseline for high-stakes domains such as legal
and finance.

</details>


### [14] [Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity](https://arxiv.org/abs/2507.15864)
*Guowen Yuan,Tien-Hsuan Wu,Lianghao Xia,Ben Kao*

Main category: cs.CL

TL;DR: 本文针对低资源下的NER任务，提出通过“双重相似度”选择示例及“对抗示例训练”方法，有效提升模型效果，实验优于多种已有方法。


<details>
  <summary>Details</summary>
Motivation: 在低资源场景下，利用示例学习提升命名实体识别（NER）效果存在挑战。作者发现目前的示例选择主要依赖语义相似度，并且NER模型对示例的参考能力不足。

Method: 提出“双重相似度”示例选择方法（结合语义和特征相似度），以及对抗示例训练策略，强制模型在NER任务中有效利用示例。

Result: 在多个低资源NER任务上的实验表明，所提方法优于现有多种方法。

Conclusion: 通过改进示例选择与引入对抗性训练，使NER模型在低资源场景下更好地利用演示示例，显著提升识别效果。

Abstract: We study the problem of named entity recognition (NER) based on demonstration
learning in low-resource scenarios. We identify two issues in demonstration
construction and model training. Firstly, existing methods for selecting
demonstration examples primarily rely on semantic similarity; We show that
feature similarity can provide significant performance improvement. Secondly,
we show that the NER tagger's ability to reference demonstration examples is
generally inadequate. We propose a demonstration and training approach that
effectively addresses these issues. For the first issue, we propose to select
examples by dual similarity, which comprises both semantic similarity and
feature similarity. For the second issue, we propose to train an NER model with
adversarial demonstration such that the model is forced to refer to the
demonstrations when performing the tagging task. We conduct comprehensive
experiments in low-resource NER tasks, and the results demonstrate that our
method outperforms a range of methods.

</details>


### [15] [Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models](https://arxiv.org/abs/2507.15868)
*Altynbek Ismailov,Salia Asanova*

Main category: cs.CL

TL;DR: 本文系统分析LLM对提示词微小变动的鲁棒性与敏感程度，发现模型对部分语义关键变动缺乏响应，强调亟需优化模型对语义变化的分辨能力和差异化响应。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在代码生成等关键场景下使用，但面对提示中的小变动（如拼写错误或术语替换），模型表现如何尚不明确。研究动机是探究模型在无害噪声与根本性语义变化之间的敏感度边界。

Method: 作者收集了50道LeetCode题目，对每道题目设计三种最小化扰动：（i）逐步删减每步10%词汇的提示，测试欠规范鲁棒性；（ii）关键数量词替换（如'max'变'min'），测试语义逆转敏感度；（iii）通俗词换为技术术语，测试术语鲁棒性。六个主流模型（含三种'推理微调'版本）对每种变体生成Python代码，并检验输出是否随提示变更调整。

Result: 模型在删除90%提示内容后依然有85%的正确率，表现出对不足规范的『过度鲁棒性』。但对单一关键数量词反转，仅54%模型有反应，说明对语义逆转不敏感，推理微调模型敏感度更低。术语扰动介于两者之间，仅56%有反应。屏蔽关键锚点如函数名会提升模型敏感度。

Conclusion: 当前LLM难以区分无害噪声和语义变化，经常对二者一视同仁，导致在实际应用中存在潜在安全/准确性风险。作者建议评测和训练方法需奖励“差异敏感性”：在遭遇无害噪声时鲁棒、但遇到语义大变化时能适应或拒绝作答。

Abstract: Large language models (LLMs) now write code in settings where misreading a
single word can break safety or cost money, yet we still expect them to
overlook stray typos. To probe where useful robustness ends and harmful
insensitivity begins, we compile 50 LeetCode problems and craft three minimal
prompt perturbations that should vary in importance: (i) progressive
underspecification deleting 10 % of words per step; (ii) lexical flip swapping
a pivotal quantifier ("max" to "min"); and (iii) jargon inflation replacing a
common noun with an obscure technical synonym. Six frontier models, including
three "reasoning-tuned" versions, solve each mutated prompt, and their Python
outputs are checked against the original test suites to reveal whether they
reused the baseline solution or adapted. Among 11 853 generations we observe a
sharp double asymmetry. Models remain correct in 85 % of cases even after 90 %
of the prompt is missing, showing over-robustness to underspecification, yet
only 54 % react to a single quantifier flip that reverses the task, with
reasoning-tuned variants even less sensitive than their bases. Jargon edits lie
in between, passing through 56 %. Current LLMs thus blur the line between
harmless noise and meaning - changing edits, often treating both as ignorable.
Masking salient anchors such as function names can force re - evaluation. We
advocate evaluation and training protocols that reward differential
sensitivity: stay steady under benign noise but adapt - or refuse - when
semantics truly change.

</details>


### [16] [Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation](https://arxiv.org/abs/2507.16002)
*Sumit Singh,Rohit Mishra,Uma Shanker Tiwary*

Main category: cs.CL

TL;DR: 本研究通过结合外部检索增强的数据与预训练模型，有效提升了印地语NER表现，特别是在低资源或低上下文场景下，表明RA方法在NER任务上极具价值。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别（NER）是自然语言处理中的一项主要挑战，尤其是在资源有限的印地语等语言中。本研究旨在通过数据增强和预训练模型改善印地语NER的效果。

Method: 采用了两类方法：1）对MuRIL、XLM-R和Llama2-7B这三个印地语相关的预训练编码器进行有无检索增强（RA）的微调；2）在少样本设置下，使用Llama2-70B、Llama3-70B和GPT3.5-turbo等生成式大模型做NER任务，并评估RA的作用。RA通过从外部上下文（主要是维基百科）检索相关数据并进行增强。

Result: 在绝大多数情况下，结合检索增强的数据会明显优于未采用RA的基线方法。具体而言，MuRIL和XLM-R的宏F1分数分别从0.69、0.495（无RA）提高到0.70和0.71（有RA）。微调后的Llama2-7B也取得了显著提升。未微调的生成式模型，在增强数据的帮助下也表现更佳。GPT3.5-turbo成功适应了RA，而Llama2-70B和Llama3-70B在该检索上下文下表现一般。

Conclusion: 检索增强（RA）对于NER任务的提升作用显著，尤其在低上下文情况下效果更好。合理利用数据增强和大规模预训练模型是提升低资源语言NER性能的有效方法，对后续相关研究提供了重要参考。

Abstract: One major challenge in natural language processing is named entity
recognition (NER), which identifies and categorises named entities in textual
input. In order to improve NER, this study investigates a Hindi NER technique
that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and
Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf
(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments
the data with retrieved data from external relevant contexts, notably from
Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.
However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER
generation. Our investigation shows that the mentioned language models (LMs)
with Retrieval Augmentation (RA) outperform baseline methods that don't
incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69
and 0.495, respectively, without RA and increase to 0.70 and 0.71,
respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B
by a significant margin. On the other hand the generative models which are not
fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA
well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval
context. The findings show that RA significantly improves performance,
especially for low-context data. This study adds significant knowledge about
how best to use data augmentation methods and pretrained models to enhance NER
performance, particularly in languages with limited resources.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [17] [An unconditional lower bound for the active-set method in convex quadratic maximization](https://arxiv.org/abs/2507.16648)
*Eleon Bach,Yann Disser,Sophie Huiberts,Nils Mosis*

Main category: cs.DM

TL;DR: 该论文证明：即使目标函数为简单凸二次型，主动集法（无论采用哪种pivot规则）在最坏情况下需要指数迭代步，解决了线性或低次数目标是否足以构造指数下界这一公开难题，对相关优化算法复杂性理论有深远意义。


<details>
  <summary>Details</summary>
Motivation: 此前关于主动集方法（active-set method）在优化凸二次函数时，其最坏情况下迭代次数的下界仍存在未解问题，且已知最好下界需要高次数多项式目标，并未解决是否常数次数目标即足够提出指数下界。解决该公开悬而未决的问题有重要理论意义，特别是对理解与单纯形法相关的极限。

Method: 作者基于一个新颖的扩展形式递归构造（使用形变乘积），该多面体映射到抛物线的分段多边形近似，并保留了指数级多面体顶点。又设计了一个二次目标函数，迫使主动集方法跟随此抛物线边界，避免沿高维多面体边走捷径。全体分析对所有pivot规则都成立。

Result: 主动集方法在凸二次目标且线性约束下，无论采用何种pivot规则，在最坏情况下都需指数迭代。此工作大幅提升了此前的结果：从需要高次数多项式目标提升到常数（2）次多项式目标，并明确回答了常数次数目标是否足够这个公开问题。

Conclusion: 主动集方法即使在最简单的凸二次目标下也有本质极限，为所有pivot规则统一构成了指数级最坏迭代下界，这一发现为理解相关多面体法的复杂度瓶颈提供了坚实理论基础。

Abstract: We prove that the active-set method needs an exponential number of iterations
in the worst-case to maximize a convex quadratic function subject to linear
constraints, regardless of the pivot rule used. This substantially improves
over the best previously known lower bound [IPCO 2025], which needs objective
functions of polynomial degrees $\omega(\log d)$ in dimension $d$, to a bound
using a convex polynomial of degree 2. In particular, our result firmly
resolves the open question [IPCO 2025] of whether a constant degree suffices,
and it represents significant progress towards linear objectives, where the
active-set method coincides with the simplex method and a lower bound for all
pivot rules would constitute a major breakthrough.
  Our result is based on a novel extended formulation, recursively constructed
using deformed products. Its key feature is that it projects onto a polygonal
approximation of a parabola while preserving all of its exponentially many
vertices. We define a quadratic objective that forces the active-set method to
follow the parabolic boundary of this projection, without allowing any
shortcuts along chords corresponding to edges of its full-dimensional preimage.

</details>
