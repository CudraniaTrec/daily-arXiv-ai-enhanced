<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: 本文将抽象语法理论扩展到二级类语言，用更高阶范畴工具推导并证明了CBV等体系的替换性质。


<details>
  <summary>Details</summary>
Motivation: 现有的抽象语法理论对二级类（如CBV、CBPV等）语言的处理不充分，需要新的理论框架来准确描述其语法结构及替换性质。

Method: 采用了Fiore、Plotkin和Turi提出的抽象语法方法，通过范畴、动作范畴和双范畴的理论论证，适配带有二级类的编程语言语法，并用以推导替换相关的引理。

Result: 将基于范畴的抽象语法理论拓展为行动在动作范畴（actegory）上的形式，并实现了在CBV等相关计算体系下的替换引理证明。

Conclusion: 该文将抽象语法的相关理论扩展到了含有二级类的语言，并利用双范畴推导了CBV等体系下的替换引理，理论结果得到成功应用。

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: 论文围绕软件工程成果转化问题，阐述了在无法进行随机实验时，统计因果推断（SCI）对于评价工具、流程和指南实际效益的重要性，并提出SCI是在实际约束下实现因果分析的可行方法。


<details>
  <summary>Details</summary>
Motivation: 软件工程（SE）领域希望通过研究推动软件开发者和用户的福祉，关键在于将研究成果（如工具、流程和指南）有效转化到开发实践中。

Method: 传统上通过随机对照实验（RCT）来证明这些成果对相关性能指标的因果效应，但由于法律、伦理或实际操作限制，RCT并非总是可行。因此需要采用统计因果推断（SCI）的方法，从观测数据中推断因果关系。

Result: 提出了在无法进行随机实验的情况下，通过SCI对工具、流程和指南的效益进行因果分析的需求和方法。

Conclusion: SCI为软件工程研究的成果转化和价值评估提供了可靠的替代路径，使得因果关系的证据能够以观测数据为基础获得。

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [3] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 该论文通过系统性实验发现，提升自然语言提示的英语熟练度可显著提高各类大型语言模型生成代码的正确性和可靠性，说明英语表达能力是优化代码生成质量的关键工具。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注提示结构，但较少研究提示语的英语能力对生成代码质量的影响。本研究旨在验证英语熟练度独立于提示技巧时对代码生成的作用。

Method: 控制提示语的英语熟练度（从基础到高级），系统性地在164项编程任务中实验，并评估LLM生成代码的熟练度和正确性。

Result: LLM默认使用中级（B2）英语水平生成代码。尽管不同模型的代码熟练度效果有所不同，但全体模型在高熟练度英语提示下生成的代码更为正确。

Conclusion: 英语语言能力是影响大型语言模型（LLM）生成代码质量的重要因素。高水平的英语输入能显著提升代码的正确性。

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [4] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: 提出轻量化多智能体Ruby程序修复框架RAMP，无需大数据库和微调，通过测试驱动反馈和自我反思，实现高效鲁棒的修复能力。实验表明RAMP性能优于现有方法，且易于推广至其他小众语言。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）极大推动了自动化程序修复（APR）领域的发展，但目前相关研究大多集中于计算成本高、语言范围有限的方法，尤其很少关注广泛用于Web开发的Ruby语言。Ruby开发者长期面临着持续的修复挑战，这促使作者希望为该语言提供更高效的程序修复方案。

Method: 作者提出了名为RAMP的新型轻量级修复框架，将程序修复过程设定为基于反馈的迭代过程。RAMP采用多个协同智能体，负责生成有针对性的测试、反思错误并持续完善修复候选方案，直到找到正确解。与以往方法不同，RAMP无需大型多语种数据库或高昂的微调，仅依赖轻量化提示和测试反馈，直接作用于Ruby代码。

Result: RAMP在XCodeEval基准测试上的Ruby任务表现优异，pass@1达到了67%，明显优于现有方法；一般在五次迭代内就能成功修复。消融实验表明，测试生成与自我反思对RAMP性能提升至关重要。进一步分析发现，RAMP对修复错误答案、编译错误及运行时错误尤为有效。

Conclusion: RAMP框架不仅为Ruby自动化程序修复带来新的高效方法，也展示了多智能体修复策略的有效性，为将LLM调试工具扩展至小众语言提供了坚实基础和新思路。

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [5] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: 提出了结合多智能体和LLM的自动RTL生成流程，通过自我纠错机制实现最优性能，在公开数据集上达到最高通过率，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统RTL代码生成依赖人工，有自动化需求，但现有方法尚有性能瓶颈，因此提出新流程以提升生成质量、效率和通用性。

Method: 采用多智能体系统，将专用的LLM与硬件仿真工具结合，通过渐进式错误反馈系统（PEFA）实现自我纠错和复杂性提升，并在开放源代码代理框架上实现。

Result: 在两个公开自然语言到RTL数据集上进行基准测试，新方法设立了新的性能标杆，实现了最高通过率，并且token使用更高效；兼容开放与闭源LLM，性能差距被有效弥合。

Conclusion: 所提出的多智能体流程能够在无需人类干预的情况下自动完成复杂的RTL代码生成任务，实现了编译、功能正确性和可综合性检查，其性能达到当前最优。

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [6] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: PSD2Code提出基于PSD理解与约束对齐的多模态设计到代码方法，大幅提升自动生成前端代码的结构一致性和生产可用性，对现有方法构成突破。


<details>
  <summary>Details</summary>
Motivation: 现有的设计到代码（design-to-code）生成方法在结构一致性、素材排列对齐和生产可用性方面存在不足，难以将设计原型高效、准确转换为可用的前端代码。

Method: 提出了一种新颖的多模态方法——PSD2Code，利用PSD文件解析和素材对齐策略，通过ParseAlignGenerate流程提取PSD文件的层级结构、属性与元数据，并结合约束对齐方式和结构化提示来提升大模型生成前端代码（如React+SCSS）的质量与可控性。

Result: 该方法在代码相似度、视觉保真度以及生产就绪度等多项指标上相较于现有方法有显著提升，且对不同大语言模型展现良好的适应性。

Conclusion: 将结构化设计信息与多模态大语言模型结合，可以推动工业级代码生成和自动化前端开发向前迈进。

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [7] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: VulInstruct提出将安全规范知识引入LLM，以显著提升安全漏洞检测能力，在实际生产环境中发现了新高危漏洞，远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在漏洞检测任务中表现不佳，原因在于缺乏对安全规范的理解，而安全规范知识在训练数据中很少显式出现。为弥补此漏洞，作者提出了结合安全规范的分析方法。

Method: VulInstruct通过系统性地从历史漏洞提取安全规范，构建通用规范和领域特定规范知识库，并利用这些规范引导LLM进行安全行为推理，从而检测未知漏洞。

Result: 在严格评测标准下，VulInstruct在PrimeVul数据集上取得了45.0%的F1分数（相较基线提升32.7%），召回率为37.7%（提升50.8%），并独特检测到24.3%的新漏洞，是基线的2.4倍。同时，成功发现了一个重大高危生产漏洞，表明了其实际应用潜力。

Conclusion: VulInstruct能够显著提升大语言模型在安全漏洞检测中的性能，提升准确率和发现新漏洞的能力，并展示了实际应用价值。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [8] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: 本文提出了AdaTaint，通过神经-符号方法结合LLM和静态分析，有效减少误报并提升漏洞检测能力，对比主流工具有明显优势。


<details>
  <summary>Details</summary>
Motivation: 静态分析在发现软件漏洞方面有效，但存在源-汇规范不完整和误报率高的问题。

Method: 提出了AdaTaint，一个由大语言模型（LLM）驱动的污点分析框架，可自适应推断源/汇规范，并通过神经-符号推理过滤虚假警报。与仅使用LLM的检测器不同，AdaTaint结合程序事实和约束验证，保证适应性和确定性。

Result: 在Juliet 1.3、SV-COMP风格C基准以及三个大型实际项目上的评估表明，AdaTaint平均降低了43.7%的误报率，召回率提升了11.2%，其运行时开销与主流工具（CodeQL、Joern、LLM-only流程）持平。

Conclusion: 将LLM推理与符号验证结合，可以显著提升静态漏洞分析的准确性与可靠性，具有实际应用价值。

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [9] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 本文提出了一套新颖的基准和评估方法，用于更有效科学地评判基于大型语言模型的自主软件开发代理。研究表明，目前代理完成需求的能力刚过半，且最大不足在于理解与验证需求。此成果为推动该领域研究提供了实用工具和关键参考。


<details>
  <summary>Details</summary>
Motivation: 在软件工程领域，基于LLM的自主代理推动了端到端软件开发的新范式。然而，现有科学评估面临着过于简单的基准和难以公平对比不同代理体系结构的挑战，因此亟需更高质量的评估体系。

Method: 1. 构建了一个动态、具有挑战性的基准 E2EDevBench ，用于模拟真实开发场景。
2. 提出混合评估框架，结合测试案例功能评估与细粒度基于LLM的需求验证。
3. 在统一基础上实现三种代表性代理架构，进行受控实验，主要剖析工作流设计的影响。

Result: 先进的代理架构在该基准上能完成约50%的需求。其成功率直接取决于任务分解和协作的架构策略，且主要瓶颈源于对需求的遗漏和自我验证能力不足。

Conclusion: 本文为社区提供了更真实的基准和综合评估框架，并揭示了当前软件开发代理的能力与核心挑战，为未来提升需求理解和规划指明了方向。

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [10] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: 本文评估了23个大语言模型在软件工程场景下与人类（普通人和AI从业者）在关键AI价值观方面的一致性。虽然模型与AI从业者价值观接近，但在实际需求排序与自称价值观上存在差异，提示在开发中需加强对LLM价值观对齐的系统监管。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在软件工程任务中被广泛应用，如何确保模型在关键负责任AI价值观方面与人类一致成为迫切问题。本文关注于模型与不同人群价值观的对齐情况及其对软件开发过程的影响。

Method: 作者选取了23个不同的LLMs，通过四项任务进行评估：1）选择重要AI责任价值观，2）在具体情境下对其重要性进行评分，3）处理价值观之间的权衡冲突，4）优先排序体现这些价值观的软件需求。模型表现分别与美国代表性样本和AI从业者群体的选择做比对。

Result: 结果表明，LLMs在价值观偏好上通常更接近AI从业者，而非普通美国民众，尤其在公平、隐私、透明度、安全、问责方面。但模型在认知（任务1-3）与实际需求排序（任务4）间表现出不一致，说明模型自称和实际行为间存在忠实性偏差。

Conclusion: LLMs在负责任AI价值观的应用中还存在对齐和忠实性风险。在没有人工监督的情况下过度依赖LLMs从事需求工程工作，可能引发实际价值观偏差的风险。因此，需要更系统的基准测试、解释和监测方法来促进AI辅助软件开发中的价值观对齐。

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [11] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: SAFE插件通过GPT-4o对安全漏洞进行深入解释，帮助开发者更好地理解和解决问题，提高了静态安全测试工具的易用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 静态应用安全测试（SAST）工具常常因为通用性的警告信息而导致开发者难以理解安全漏洞，从而影响工具的实用性。

Method: 本研究提出了SAFE，一个集成开发环境（IDE）插件，利用大型语言模型（如GPT-4o）来解释SAST工具发现的安全漏洞，包括成因、影响以及缓解策略。

Result: 专家用户研究表明，SAFE生成的解释显著帮助初学者及中级开发者理解和解决安全漏洞，从而提升了SAST工具的整体易用性。

Conclusion: 结合LLM生成的解释信息能够提升SAST工具在开发者中的使用体验和有效性，特别对于不太熟悉安全领域的开发者帮助明显。

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [12] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: 本文提出通过Git仓库实现分布式实体间的异步通信，替代传统API及消息中间件，增强了系统的透明性和可审计性，推动了GitOps在多领域协作中的应用，但在性能上需权衡。


<details>
  <summary>Details</summary>
Motivation: 当前分布式实体之间的信息交换通常依赖传统API或消息中间件，存在集成复杂、可审计性差等问题。本文旨在探索一种更轻量、可审计且自治性强的通信方式，以应对跨域、组织间及隔离环境下的协作需求。

Method: 提出一种基于Git的异步通信模式，借鉴Kubernetes Operators及自定义资源（CRs）理念。系统中的实体作为发布者或消费者，通过共享Git仓库交换信息，使用spec字段描述期望状态，status字段反映实际结果，并利用Git的版本管理、签名与权限控制等特性保证系统的透明性和可追溯性。

Result: 该方法扩展了GitOps的应用领域，不仅用于基础设施管理，还支持跨领域、跨组织及隔离环境下的协作。相比于RESTful和消息中间件集成方式，Git为通信提供了更好的可审计性、透明性和松耦合性，但也存在一定的权衡，例如在性能和实时性上可能不及传统方法。

Conclusion: 使用Git作为分布式实体之间的通信媒介，能够实现透明、可追溯及高度自治的信息交换，适合跨域、跨组织及需求隔离的场景，为分布式系统的协作模式提供了新思路。

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [13] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: 本文提出DriveRLR，通过生成变异驾驶场景，用来评测和比较大语言模型在判别场景真实性方面的能力和鲁棒性。实验表明DriveRLR能有效展示不同模型表现，将促进自动驾驶系统的仿真测试和安全性提升。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在安全性方面面临挑战。基于情景的测试（尤其是仿真法）成为主流，但仿真场景的真实度评估较难，急需有效的评估方法。近年来大语言模型在推理和泛化方面表现优异，有望用于辅助评估场景真实度。

Method: 该文提出了DriveRLR基准工具。DriveRLR通过生成变异场景、构建提示词（prompts），用于评测不同LLM在判别驾驶场景真实度时的能力与鲁棒性。实验在DeepScenario数据集上，对GPT-5、Llama 4 Maverick和Mistral Small 3.2三大主流LLM进行了评测。

Result: DriveRLR能够有效区分不同LLM在场景评估上的鲁棒性差异，并证明了其作为评估工具的有效性与实用价值。

Conclusion: DriveRLR是一个有效评估LLM在驾驶场景真实度判别任务上鲁棒性的基准工具，可作为仿真测试工作流中的实用组件。适用于自动驾驶系统的安全性验证。

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [14] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: 论文系统分析了当前主流LLM在代码生成中的失败案例及其原因，总结出四大弱点和常见复杂点，为后续模型优化指明方向。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成领域中，大型语言模型（LLM）取得了显著进展，但现有的排行榜和基准测试仅提供了定量排名，无法深入揭示LLM反复失败的任务类型。这些失败任务和原因对于理解模型局限性、指导后续模型研发至关重要。

Method: 作者对四个主流代码生成基准任务中各大LLM的表现进行统计，筛选出模型最容易失败的任务。随后研究这些失败是否与解决方案代码的静态复杂度相关，并对114个LLM持续失败的任务进行了系统性分析。

Result: 分析发现，LLM存在四种反复出现的弱点，同时基准任务中一些常见复杂点也是导致模型失败的重要原因。

Conclusion: 通过揭示LLM在代码生成中稳定失败的任务和背后的原因，论文为理解模型现有局限性以及未来改进方向提供了重要参考。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [15] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 本文通过对GitHub项目的实证研究发现，使用大语言模型助手Cursor可短期显著提高开发速度，但会增加代码复杂性和警告，最终影响长期生产率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在软件工程领域显示出革命性的潜力，但关于LLM工具（如Cursor）能否显著提升生产力，目前缺乏实证数据，故本研究欲评估其真实影响。

Method: 采用前沿的差异中之差（difference-in-differences, DID）设计，比较在GitHub上采用Cursor的项目与匹配的未采用Cursor项目，并进一步使用面板广义矩估计（GMM）分析影响机制。

Result: 采用Cursor后，项目开发速度显著提升但短暂，同时静态分析警告和代码复杂性显著且持续增加，长期来看，这些不良代码质量因素又导致开发速度下降。

Conclusion: Cursor等LLM助手可短期提升开发效率，但会导致代码质量下降，进而妨碍长期开发进度，对业界、LLM设计者和相关研究具有重要启示。

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [16] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: 本文提出了基于真实用户场景的代码编辑能力评测基准EDIT-Bench，评估表明该任务具有挑战性，不同模型与上下文信息对结果影响显著。


<details>
  <summary>Details</summary>
Motivation: 当前AI编程助手越来越多采用由大语言模型（LLM）进行代码编辑的方式，但用于直接评估LLM代码编辑能力的基准很少，且现有数据集很多依赖人工合成的数据，因此需要一个更加贴近真实应用场景的评测基准。

Method: 提出了名为EDIT-Bench的基准数据集，用于评测LLM对真实世界中的代码编辑任务的能力。该基准收集了545个实际用户指令与代码环境的编辑任务，涵盖多种自然和编程语言，包含多种真实用例，例如修复错误和添加功能，并引入了需要理解代码环境、光标位置、用户指令等上下文信息的问题。对40种不同LLM进行了系统评测。

Result: EDIT-Bench是一个难度较高的代码编辑评测集，仅有5个模型得分超过60%。不同类型的用户指令之间模型表现存在差异；此外，不同级别的上下文信息对模型完成任务的成功率影响较大，性能差距最高达11%。

Conclusion: 评估与真实上下文相结合的代码编辑能力对于测试LLM现实世界应用效果至关重要，EDIT-Bench为该方向研究提供了更具代表性的评测平台。

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [17] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: 本文提出了超越微服务和单体架构的新型系统设计方法，通过严格的模块独立性和通用接口，实现了模块化、灵活的系统平台EIGHT，可以动态热插拔模块，极大提升了系统的可维护性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 虽然微服务将模块物理隔离，但依赖关系的传播与扩散依旧没有被阻止。迫切需要一种方法以彻底消除模块间依赖并实现更高的独立性。

Method: 提出了模块独立性的度量方法，并据此推导出模块独立的必要条件。进而提出新的系统设计理念和软件工程方法，通过设计通用接口来作为模块间的边界。基于该方法实现了一个名为EIGHT的平台架构。

Result: EIGHT架构证明，只要模块独立性得到保障，即使是单进程的单体应用，也能在运行时动态加载、卸载或修改任意部分。

Conclusion: 通过模块独立性理论及EIGHT平台架构，探索并验证了一条超越传统微服务与单体架构、适应日益复杂系统需求的新路径。

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [An Automated Theorem Generator with Theoretical Foundation Based on Rectangular Standard Contradiction](https://arxiv.org/abs/2511.04092)
*Yang Xu,Peiyao Liu,Shuwei Chen,Jun Liu*

Main category: cs.LO

TL;DR: 该文提出了自动定理生成新理论和工具，围绕“矩形标准矛盾”结构，实现定理自动发现，具有理论、算法和实际工具创新意义。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统化生成非平凡且逻辑有效定理的严密理论体系，限制了自动化推理和人工智能领域的基础研究进展。为填补这一关键理论空白，需要提出自动生成定理的新理论和工具。

Method: 以标准矛盾为核心，首次定义并证明了矩形标准矛盾这一新型逻辑结构，并围绕其核心理论性质提出了完整的自动定理生成（ATG）理论。同时设计了高效的模板化ATG算法，并开发了矩形自动定理生成器。

Result: 证明了矩形标准矛盾的两大核心性质：一是必不可满足的标准矛盾，二是具备非冗余性（去除任一子句，其余子句集可满足）。基于此，将矩形标准矛盾划分为前提子集A及补集否定H后，可以形成有效定理A ⊢ ¬H，且所有此类定理都逻辑等价。算法实现提升定理发现自动化。

Conclusion: 研究在理论和工具层面实现了自动化定理生成，并推动了机器从“验证者”向“发现者”转变，为逻辑与人工智能基础研究探索新路径。

Abstract: Currently, there is a lack of rigorous theoretical system for systematically
generating non-trivial and logically valid theorems. Addressing this critical
gap, this paper conducts research to propose a novel automated theorem
generation theory and tool. Based on the concept of standard contradiction
which possesses unique deductive advantages, this paper defines and proves, for
the first time, a new logical structure known as rectangular standard
contradiction. Centered on this structure, a complete Automated Theorem
Generation (ATG) theory is put forward. Theoretical proofs clarify two core
properties of rectangular standard contradiction: first, it is a standard
contradiction (necessarily unsatisfiable); second, it exhibits non-redundancy
(the remaining clause set becomes satisfiable after removing any clause).
Leveraging these properties, this paper proves that partitioning a rectangular
standard contradiction into a premise subset $A$ and negation of its complement
$H$, a valid theorem $A \vdash \neg H$ can be formed, and all such theorems are
logically equivalent. To implement this theory, an efficient template-based ATG
algorithm is designed, and a Rectangular Automated Theorem Generator is
developed. This research enables machines to transition from "verifiers" to
"discoverers", opening up new avenues for fundamental research in the fields of
logic and artificial intelligence.

</details>


### [19] [Compact Quantitative Theories of Convex Algebras](https://arxiv.org/abs/2511.04201)
*Matteo Mio*

Main category: cs.LO

TL;DR: 本文提出并定义了紧致定量等式理论，证明插值重心（凸）定量代数理论为紧致理论，并据此构造出可以刻画概率分布距离的其它紧致定量等式理论。


<details>
  <summary>Details</summary>
Motivation: 提出并研究紧致定量等式理论，解决定量等式理论中结论能否由有限证明得出的数学基础问题，尤其关注应用于概率分布距离刻画的代数结构。

Method: 定义并分析紧致定量等式理论，通过证明有限证明推导所有结论，具体应用于插值重心定量代数理论，再推广到其它凸代数。

Result: 证明了插值重心定量代数理论的紧致性，进一步得到了一系列关于凸代数的、用于刻画有限支持概率分布距离的紧致定量等式理论。

Conclusion: 证明了Mardare等人提出的插值重心（凸）定量代数理论是紧致的，并以此为范例获得了其它关于凸代数的紧致定量等式理论。

Abstract: We introduce the concept of compact quantitative equational theory. A
quantitative equational theory is defined to be compact if all its consequences
are derivable by means of finite proofs. We prove that the theory of
interpolative barycentric (also known as convex) quantitative algebras of
Mardare et. al. is compact. This serves as a paradigmatic example, used to
obtain other compact quantitative equational theories of convex algebras, each
axiomatizing some distance on finitely supported probability distributions.

</details>


### [20] [The Size of Interpolants in Modal Logics](https://arxiv.org/abs/2511.04577)
*Balder ten Cate,Louwe Kuijer,Frank Wolter*

Main category: cs.LO

TL;DR: 本文系统分析了（准）正规模态逻辑中的Craig中介式、均匀中介式和最强蕴涵式的大小。对于可列表型逻辑，这些对象具备多项式规模（在合理复杂性假设下），而非可列表型逻辑则存在无条件的指数级别下界。论文揭示了不同模态逻辑在相关逻辑计算任务上的本质复杂性分界。


<details>
  <summary>Details</summary>
Motivation: 研究Craig中介式、均匀中介式和最强蕴涵式在（准）正规模态逻辑中的规模，为其计算复杂性和资源消耗提供理论基础。

Method: 分析了不同模态逻辑体系下这些逻辑对象（中介式和蕴涵式）的大小，并将其归约到经典命题逻辑的相关问题上，通过上界与下界的技术证明，区分不同类型逻辑的规模性质。

Result: 1. 对于可列表型（tabular）模态逻辑，最强蕴涵式的计算可在多项式时间内归约为经典命题逻辑的均匀中介式计算，因此当且仅当NP ⊆ P/poly时，其规模为多项式级别。 2. 如果该模态逻辑具有Craig中介性质，则这一结论同样适用于Craig中介式和均匀中介式。 3. 对于几乎所有非可列表型标准正规模态逻辑，无条件地给出了中介式和最强蕴涵式的指数下界。4. 针对包含或被包含于S4或GL的正规模态逻辑，存在如下二分法：可列表型逻辑的相关对象有“命题规模”，而非可列表型逻辑则有无条件的指数下界。

Conclusion: 可列表型与非可列表型正规模态逻辑在中介式和最强蕴涵式大小上存在严格的二分，前者可多项式大小，后者有指数下界。此结果为相关逻辑推理任务的复杂性提供了清晰界限。

Abstract: We start a systematic investigation of the size of Craig interpolants,
uniform interpolants, and strongest implicates for (quasi-)normal modal logics.
Our main upper bound states that for tabular modal logics, the computation of
strongest implicates can be reduced in polynomial time to uniform interpolant
computation in classical propositional logic. Hence they are of polynomial
dag-size iff NP $\subseteq$ P$_{/\text{poly}}$. The reduction also holds for
Craig interpolants and uniform interpolants if the tabular modal logic has the
Craig interpolation property. Our main lower bound shows an unconditional
exponential lower bound on the size of Craig interpolants and strongest
implicates covering almost all non-tabular standard normal modal logics. For
normal modal logics contained in or containing S4 or GL we obtain the following
dichotomy: tabular logics have ``propositionally sized'' interpolants while for
non-tabular logics an unconditional exponential lower bound holds.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 本文提出了一种结合心理学人格特质与神经网络模型结构的新方法，通过对隐藏状态进行低秩子空间分析，实现精确操控LLM输出的个性表达，为模型对齐与行为调控提供了新的技术路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在生成文本时会表现出隐含的个性特质，但如何可靠地控制或调整这些特质以满足特定需求仍是个挑战。目前缺乏针对模型生成行为进行有效操控的机制。个性感知的LLM是值得探索的方向，但其心理学特征与模型表示之间的关系尚未深入研究。

Method: 提出了一种新的流程，通过应用Big Five人格特质理论，在transformer模型层中提取隐藏状态激活，并利用低秩子空间发现方法，识别不同模型架构中与特定人格特质对应的最佳注入层。基于此，设计了一个灵活的引导框架，能够动态选择层并精确控制模型输出中的特质表达。

Result: 发现人格特质在模型中占据低秩共享子空间，这些潜在结构可以经过精心扰动转化为可操作机制，从而有效引导模型输出中特质的表达，同时不影响模型的流畅度、多样性与通用能力。

Conclusion: 通过心理学理论与神经网络潜在结构结合，实现了LLM个性表达的可控与对齐，为模型行为操控和对齐提供了新的实用方法。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [22] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: 本文提出了TextualVerifier框架，为TextGrad增加自我验证能力，保证文本推理过程的有效性。通过链式推理、多数投票等手段，显著提升了各项基准测试的推理准确率，为文本优化领域的自动化验证提供了新方法。


<details>
  <summary>Details</summary>
Motivation: TextGrad提供了基于文本的自动微分，用于复合AI系统的优化，但在文本推理决策过程中缺乏自我验证机制，可能导致推理有效性无法保障。因此，急需一个可以自动验证推理过程的框架，以提升AI的可靠性。

Method: 提出TextualVerifier框架，利用大型语言模型（LLM）实现链式推理分解、变体生成、多数投票和共识聚合四个阶段，实现自我验证。该框架无侵入性地集成到TextGrad的损失函数和优化结果验证阶段，提升推理的有效性。

Result: 实验结果显示，TextualVerifier在PRM800K数据集独立评估中将推理有效性提升了29%，集成到TextGrad中在GPQA-Diamond、MMLU-ML和MMLU-CP基准上分别提升了2.2、8.08、10.71和3.92个百分点，且只引入了适度的LLM调用开销（平均5.9次）。

Conclusion: TextualVerifier是首个基于LLM的TextGrad自我验证框架，无需数值梯度即可提升文本优化和推理的可靠性，为基于文本的AI优化验证开辟了新的方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [23] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 作者扩展了希腊方言数据集，覆盖10个方言，显著提升了大模型对希腊多方言的处理能力，是相关研究的重要资源。


<details>
  <summary>Details</summary>
Motivation: 现有的希腊方言数据集（GRDD）数据量有限，无法覆盖多样的希腊方言需求，缺乏对少数及古老方言的充分支持。作者希望通过扩展数据集进一步推动希腊方言处理及语言模型能力。

Method: 扩充了GRDD数据集，新增克里特、塞浦路斯、庞蒂克和北部希腊方言数据，并引入6种新变体，包括格雷科-科西嘉语、南意大利希腊语(Griko)、马尼奥特、爱奥尼亚群岛方言(Heptanesian)、查科尼安(Tsakonian)和纯正希腊语(Katharevusa)。随后在多个大模型（Llama-3-8B、Llama-3.1-8B、Krikri-8B）上通过方言数据微调，并与前沿大模型对比其表现。

Result: 最终数据集总规模达到6,374,939词，涵盖10种希腊方言。基于该数据集的微调实验揭示了高质量方言数据对多种语言大模型表现的积极影响，并与国外前沿模型进行了效果对比。

Conclusion: 作者提供了领域首个大规模、多方言希腊语数据集(GRDD+)，并验证其价值，通过微调显著增强模型对多变体希腊语的理解与处理能力，对语言学研究和自然语言处理应用都有重要推动。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [24] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 论文介绍了PLLuM，一个专为波兰语打造的开源大模型，包括语料库建设、数据治理、模型训练及安全机制，并在实际任务中展示了应用效果，提升了波兰在AI领域自主创新能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流大型语言模型（LLMs）主要聚焦于英语，导致其他语言（尤其是波兰语）支持有限。波兰科研机构希望开发高质量、透明、且符合本土文化需求的波兰语大型模型，促进人工智能技术的本地化和主权发展。

Method: 构建了一个包含1400亿词的波兰语文本语料库用于预训练，收集了77,000条定制指令数据集和100,000条偏好优化数据集；采用了责任AI框架，包括严格的数据治理、输出校正与安全过滤模块；详细介绍了模型架构、训练和基线及指令微调模型的对齐技术。

Result: 推出了迄今最大的开源波兰语大型语言模型家族，通过下游公共管理任务展示了其实用性，各项技术和数据也已开放供研究和开发使用。

Conclusion: PLLuM通过面向波兰语开发大型基础模型，填补了非英语环境下开源AI的空白，推动了波兰AI主权和相关研究。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [25] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: STARS是一种新的推理期对齐算法，通过分段采样和拒绝机制提升LLM对齐效率与效果，性能显著优于主流方法且计算资源需求低。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在对齐语言模型时计算成本高且效果不佳；而推理期的Best-of-N采样虽效果好但成本极高。亟需一种高效、易于部署的新方法。

Method: 提出了一种推理期算法STARS，通过对短固定片段的逐步采样、评分和拒绝/接受，实现模型生成路径的早期修正。该方法在解码过程中引入数据驱动的奖励控制，提高了效率和对齐质量。

Result: STARS在六个大语言模型上测试，赢率比SFT高出最多14.9个百分点，优于DPO最多4.3个百分点，并与强基线Best-of-N采样竞争力接近，展现了高效性、通用性和稳定性。

Conclusion: STARS方法为对齐大语言模型与人类价值观提供了高效、鲁棒的新途径，在多个主流LLM上表现优异，是传统微调和排序方法的有力替代。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [26] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文提出将多标签文本分类任务拆分为独立是/否查询，配合前缀缓存和模型蒸馏技术，在保证高准确率同时大幅提升短文本推理效率，对小模型微调效果显著且方法具有通用性。


<details>
  <summary>Details</summary>
Motivation: 提高大型语言模型在多标签文本分类任务中的效率，尤其是针对短文本分析，实现高准确性同时降低计算资源消耗。

Method: 将多标签分类任务转化为一系列独立的二元（是/否）决策，每个目标维度单独查询，并结合前缀缓存机制提升推理效率；同时通过LLM到SLM蒸馏，由强大标注模型生成多份标注，用于微调较小模型。

Result: 微调后的小模型在训练中见过的分类维度上，相比零样本基线获得明显提升，无精度损失且推理速度显著提升。

Conclusion: 将多标签分类问题分解为二元查询，并结合蒸馏与缓存推理，可实现高效、可扩展的LLM多标签文本分类框架，且该方法具有通用性适用于不同领域。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [27] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 作者系统分析了三种低资源语言机器翻译数据集的性别分布，发现数据存在严重的性别倾斜和女性负面刻画，强调数据集建设应兼顾质量与数量，呼吁早期识别并缓解有害内容。


<details>
  <summary>Details</summary>
Motivation: 目前NLP领域对低资源语言的数据集建设更加注重数据量，忽略了数据质量，可能导致技术效果欠佳并产生社会偏见等有害内容。本文聚焦于数据集中性别表现存在的问题，尤其是在Afan Oromo、阿姆哈拉语、提格利尼亚语等三种低资源语言的机器翻译数据集。

Method: 作者对三种低资源语言的MT训练与基准数据集进行了内容分析，重点统计了词性、语域（如政治、宗教、新闻、健康、体育）分布，特别分析了数据集中性别有关的信息，比如人名、动词的语法性别和性别刻板印象。

Result: 研究发现：训练数据多源于政治、宗教领域，基准测试数据则多为新闻、健康和体育；数据中出现了明显的性别倾斜，男性相关内容占主导，无论是人名、动词性别还是刻板印象表现。此外，负面和有害刻画女性的内容也大量存在，尤其是在语料量较大的语言中更突出。这说明仅增加数据量无法改善数据质量，反而可能加剧有害内容产生。

Conclusion: 论文建议在低资源语言数据集建设中，不仅关注数据量，更应严格把控数据质量，防止性别不平等和其他潜在有害内容的问题，并呼吁研究者对数据集进行更加全面的质控和内容审查。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [28] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 本文提出了无需重训练的GRAD解码方法，通过融合语料统计证据，有效减少幻觉和提升大模型生成的准确性，显著优于现有解码和知识增强技术。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在生成内容时容易产生幻觉（即错误或不实内容），现有方法常依赖外部知识库或知识图谱，但这些方式在实际操作中脆弱且成本高。本文希望寻求一种无需重训练、能高效提升生成准确率的新方法。

Method: 提出了Graph-Retrieved Adaptive Decoding（GRAD）方法。该方法在解码阶段利用从语料中检索到的证据，通过一次前向过程累计下一个token的logits，构建稀疏的token转换图。解码过程中，检索到的logits会最大归一化，并与模型自身的logits自适应融合，从而提升有证据支撑的输出连贯性和流畅性，无需重训练或复杂数据格式转换。

Result: 在多个大型语言模型上和问答基准测试（涵盖内在、外在幻觉和事实性任务）中，GRAD方法表现出色。与贪婪解码相比，内在准确率提升最高可达9.7%，幻觉率降低8.6%，正确率提升6.9%，且达到各方法中最高的truth--informativeness得分。

Conclusion: GRAD方法是一种轻量级、即插即用的解码技术，可以有效利用语料中token级统计证据，提升大模型生成的真实性和可验证性。相比传统对比解码和知识图谱增强，成本更低且实用性强。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [29] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 该研究对比了人类与视觉-语言模型在多轮指代游戏中的表现，发现增加相关语境能大幅提升机器模型理解和推理能力，但在处理抽象和少样本情景时，模型仍不如人类。


<details>
  <summary>Details</summary>
Motivation: 多轮指代游戏考察智能体在多回合语言环境中的语境敏感的语用推理能力，旨在评估机器模型在复杂语言交流中的表现。

Method: 对人类参与者和视觉-语言模型进行多轮指代游戏实验，设定不同的语境变量，包括信息量、顺序及相关性，并对比其表现。

Result: 没有相关语境时，模型表现虽好于偶然但远低于人类；加入相关语境后，模型表现随轮次显著提升，不过在抽象指代和少样本任务上模型依然困难。

Conclusion: 当使用相关语境时，视觉-语言模型在多轮指代游戏中的表现获得显著提升，但在抽象指代和有限样本条件下仍存在困难。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [30] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 作者用26亿条美国推文训练语言模型，提出人类繁荣地理指数（HFGI），实现了以县和州为单位、月度和年度分辨率的人类繁荣测量，结果与现有指标一致，为社会福利等多学科研究提供了更精细的数据资源。


<details>
  <summary>Details</summary>
Motivation: 传统的人类繁荣测量手段（如幸福、健康、目标感、美德、人际关系和经济稳定等社会指标）通常缺乏细致的空间和时间分辨率，难以全面理解社会福祉。作者希望克服这些局限，提出新的定量方法，更精准反映社会各层面及地域、时序变化。

Method: 作者提出了“人类繁荣地理指数（HFGI）”，利用大约26亿条2013-2023年间带地理标签的美国推文，通过微调的大型语言模型，从48个符合哈佛全球繁荣研究框架的指标以及对移民和腐败感知的态度中进行自动分类和分析，生成按月、按年、按县、按州的繁荣相关舆情数据。

Result: 结果证实HFGI能准确反映繁荣及其各个维度，且与其它已有相关指标相关性良好。这一数据集可供社会福利、社会不平等与社会变迁等多学科分析，空间与时间分辨率前所未有。

Conclusion: 该研究为美国境内多角度、多尺度洞察人类繁荣状态提供了有力的基础资源，有助于更动态、细致地把握社会福祉变迁趋势。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [31] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文提出通过向量语义翻译实现多模型之间的潜在语义对齐，有效提升了模型协作的信息传递效率，并验证了跨模型直接语义交流的可行性。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境中，大语言模型（LLM）之间交流时通常以普通文本形式传递信息，忽略了大量潜在语义，限制了信息的高效传递，并产生了不必要的计算负担。本文因此致力于提升模型间的协作效率。

Method: 提出并训练一种向量翻译的潜在通路，通过双编码器结构在不同模型（如Llama-2-7B与Mistral-7B-Instruct）之间直接进行语义交换，实现语义空间的无损对齐。

Result: 在Llama-2-7B和Mistral-7B-Instruct之间训练的双编码器翻译器，经验证平均余弦对齐度为0.538。将30%强度的翻译向量注入目标模型，能够有效引导生成行为且不会导致logits不稳定。双向评测表明：通用模型的信息可迁移性显著高于指令微调模型（比例为2.01:1）。

Conclusion: 通过保守的向量注入方式，实现了不同模型之间的潜在语义交流，验证了跨模型语义通信的可行性，为AI系统协同工作开辟了新路径。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [32] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 本文提出将溯因推理整合进RAG框架，能有效改进RAG在证据不足场景下的推理与答案质量，提升了系统的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: RAG（检索增强生成）模型在知识密集型任务表现突出，但当检索证据不完整时，推理会出现中断，需要弥补证据缺口。向RAG注入“溯因推理”（生成合理的缺失前提）有助于加强系统的鲁棒性与解释性。

Method: 提出了一种将溯因推理整合到RAG中的框架。当检测到证据不足时，系统会自动生成可能的缺失前提，并通过一致性与合理性检查进行验证。

Result: 实验证明，该方法在溯因推理和多跳问答任务上提升了答案准确率和推理可信度。

Conclusion: 将溯因推理引入RAG能有效增强其鲁棒性和解释性，是提升RAG系统表现的一个有前景的方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [33] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种无需额外置信度模型或大量预训练的弱监督Transducer方法（WST），对带有大量错误的转录也表现良好，显著优于现有的弱监督语音识别方法，将公开实现。


<details>
  <summary>Details</summary>
Motivation: RNN-T语音识别模型成果显著，但对大规模高质量标注数据依赖极高，这类数据昂贵且难以获取。减轻模型对于高质量转录本的需求成为亟需解决的问题。

Method: 提出一种弱监督Transducer（WST）方法，通过引入灵活的训练图以自动适应转录文本中的错误，不依赖置信度估计或预训练辅助模型。

Result: 在合成和工业数据集上评测显示，即使转录错误率高达70%，WST依然能保持良好性能，且在各种场景下持续优于BTC、OTC等基于CTC的弱监督方法。

Conclusion: WST具有实际应用价值和较强鲁棒性，缓解了对高质量标注数据的依赖，提升了自动语音识别在真实场景下的可用性。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [34] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 论文提出用专家一致性评价LLM解释效果，并推出跨领域基准T-FIX和新指标，可更好衡量解释内容与专家直觉的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在知识密集型领域应用广泛，如外科、天文学、心理治疗等，但用户（往往为领域专家）不仅需要模型给出答案，还希望获得有意义的解释。现有的模型解释评估方法主要关注解释的合理性或模型内部的忠实性，无法有效反映解释内容是否真正符合专家直觉。

Method: 作者提出将“专家一致性”作为评估解释的新标准，并建立了T-FIX基准，覆盖七个知识密集型领域。该研究与领域专家合作，开发了新的评价指标，以衡量LLM解释与专家判断的一致性。

Result: 作者成功构建了跨领域的解释评估基准和评价指标，能够量化LLM解释与专家意见的对齐程度。

Conclusion: 当前解释质量评估存在短板，专家一致性是评价LLM解释更高质量的重要标准，T-FIX及相关指标为相关领域提供了有效参考。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [35] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 该论文通过提出PoK框架，有效结合问题分解和对比式时间知识检索，大幅提升了大型语言模型在时间知识图谱问答任务中的推理准确率和事实一致性，实验结果优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 时间知识图谱问答（TKGQA）需要对涉及时间约束的复杂问题进行推理和解答，但现有方法对时间约束的语义理解不充分，大型语言模型（LLM）虽具强语义理解和推理能力，却缺乏时间推理能力和事实准确性。

Method: 作者提出Plan of Knowledge（PoK）框架，引入对比式检索器。PoK模块将复杂的时间问题分解为多个子目标进行推理指导，并结合一个基于对比学习的时间知识存储（TKS），从TKG中检索语义与时间一致的事实，以提升推理可解释性与事实准确性。

Result: PoK方法在四个TKGQA基准数据集上进行了大量实验，显著提升了LLM的检索精度和推理准确率，效果最多超过现有最优方法56.0%。

Conclusion: 通过将结构化推理与时间知识检索结合，PoK框架显著提升了时间知识图谱问答的效果，提升了语义理解、可解释性和事实一致性。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [36] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 本文比较了人类与大语言模型对情感词语的联想效果，发现模型与人类的联想结果部分重合，但模型的联想更情绪化且缺乏创造性。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在探索大语言模型（LLM）是否能够像人类一样，对带有情感色彩的词汇产生联想，并比较人类与LLM的联想行为。

Method: 通过对人类与LLM对情感词的联想进行对比，分析其相似性以及联想特征。

Result: 人类与LLM的联想结果有一定的重叠，但LLM的联想结果通常更加强化词语原有的情感色彩，并且较为可预测、缺乏创造性。

Conclusion: LLM在词语联想上与人类有一定相似性，但其联想偏向于情感放大、模式化，创造性不及人类。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [37] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 该研究通过大规模多模态放射科报告训练，增强了transformer模型的去标识化能力，实现了跨机构泛化和隐私保护，比主流商业系统效果更佳。


<details>
  <summary>Details</summary>
Motivation: 当前自动化去标识化工具在放射科报告中的表现受限，尤其是在不同机构之间的泛化能力与数据隐私保护方面存在挑战。因此，提升基于transformer模型的去标识化能力，以适应真实临床环境且优于商业云厂商方案，成为亟需解决的问题。

Method: 本研究在已有的transformer-based去标识化流水线基础上，利用斯坦福大学的两个大型注释放射科报告语料库（涵盖X光、CT、MR等多模态报告），引入新的PHI类别（年龄），进行模型微调；并在来自斯坦福和宾大的测试集上进行token级PHI检测评估，同时用“hide-in-plain-sight”方法检验合成PHI的稳定性，并与主流商业系统作性能对比。

Result: 模型在Stanford和Penn数据集上分别取得0.996和0.973的F1分数，超越或保持了原有最优水平。合成PHI测试同样展现出高度鲁棒性（F1 0.959），在商业系统对比中（F1:0.960 vs 0.632-0.754）取得领先。

Conclusion: 基于transformer的大规模多机构放射科报告去标识化模型，在保护隐私与保持数据可用性方面达到新标杆，优于现有学术及商业方案，适合安全处理临床文本。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [38] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文扩展了极限下语言识别的经典理论，提出了学习者每步允许k个猜测的新模型，并对k-list识别的可能性给出了递归刻画。核心结论是：可k-list极限识别的语言集合=可分解为k个可单猜测极限识别的集合，并证明该类集合在统计学习下能以最佳指数速率识别。


<details>
  <summary>Details</summary>
Motivation: 经典的语言识别问题在极限下几乎对所有有趣语言集合都是不可能完成的。最近在相关的语言生成问题中有积极进展，激发了研究者重新思考在增加学习者猜测能力（每步可以给出k个猜测）的情况下，语言识别在极限下的可能性。

Method: 通过为学习者引入每步可以给出k个猜测的设定，提出对k-list识别的严密刻画。构建了一个递归版的Angluin刻画（原为k=1），进一步给出：一个语言集合可k-list极限识别当且仅当该集合可分解为k个能以单猜测识别的语言集合。统计设置下，用该刻画分析了输入为某语言集合支持的分布的独立同分布流时的识别速度。

Result: 得出了关于k-list极限识别的精确刻画：能k-list极限识别的集合可分解为k个能极限识别的集合。还得出了速率结论：如果集合可k-list极限识别，则可用指数速率达到k-list识别；反之，则无法达到任何收敛于零的识别速率。

Conclusion: 在学习者允许每步给出k个猜测的设定下，严格刻画了哪些语言集合可以在极限意义下被正确识别，并证明了这些集合在统计学习下能够以指数速率识别，无法k-list极限识别的集合则无法有有效速率。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [39] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 批量推理不仅节省计算资源，还能提升LLMs的推理效率、准确性和可靠性，带来行为规律化及模式迁移等集体效应，远超传统理解中的吞吐优化。


<details>
  <summary>Details</summary>
Motivation: 除降低计算成本外，深入探索批处理推理对LLMs多步推理行为的潜在规律化效应，发现之前被忽视的重要优势。

Method: 在13个不同基准测试上进行综合实验，同时进行详细的行为分析，包括准确率和推理token用量等多维度评估。

Result: 批处理推理能提升准确率，显著减少推理token消耗（3-5倍），减少“多想”与反复自我修正，增强模型决断力，并出现集体效应，模型能迁移早期样例的模式以解决更难问题。

Conclusion: 批处理推理不仅提高了推理吞吐量，还能作为强大的推理时正则化工具，提升LLMs的推理效率及可靠性。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [40] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 论文提出RIDE框架，通过IRT和强化学习对数学题目进行对抗性重写，实证显示能显著降低大语言模型的数学推理表现，有效评估其真实推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的数学推理成绩可能受数据泄漏和模式匹配影响，缺乏真正推理能力的评估方法，现有规则扰动方法易生成不合理问题，难以有效衡量题目难度和提升基准测试。

Method: 提出RIDE——基于项目反应理论(IRT)的对抗性问题重写框架，通过收集35个LLM对数学问题的回答，构建难度排序器，并以此引导问题重写模型生成更具挑战性的题目，利用强化学习进行优化。

Result: 将RIDE应用于数学竞赛基准，成功生成扰动版本，26个模型平均表现下降21.73%，充分表明高级LLM的数学推理鲁棒性有限。

Conclusion: RIDE框架有效揭示了大语言模型在数学推理方面的鲁棒性不足，并证实了该评价方法的有效性。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [41] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: 本文提出CantoASR，通过深度融合声学线索与大型音语模型推理，实现在粤语等低资源声调语种上的高效、准确语音识别，并且在实际测试中超越现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 粤语属于低资源语种，其语音识别（ASR）面临标注数据少、六声调、变调以及口音多样等挑战，现有如Whisper等ASR模型在粤语上词错误率高，亟须新方法提升识别准确性。

Method: 提出CantoASR框架，结合ASR与大型音频-语言模型（LALM）的协同纠错机制，包括：1. 强制对齐提取声学特征；2. LoRA微调Whisper模型以增强声调判别力；3. 指令微调Qwen-Audio用于韵律感知和后处理纠错。

Result: 在粤语自发语料上的评测结果显示，CantoASR在字错误率（CER）上较Whisper-Large-V3取得了显著提升。

Conclusion: 将声学特征与LALM推理结合，是解决低资源声调/方言语音识别问题的可扩展方案。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [42] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 作者系统评测了三种多智能体LLM流程在Text-to-SQL任务上的表现，发现多智能体方法能显著提升小型模型的SQL生成精度，其中Reasoner-Coder流程最优。该工作为高效、高性能的自然语言数据库访问开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 当前主流的大型语言模型（LLM）很难从自然语言指令生成高质量SQL，特别是在面对大型数据结构和复杂推理时表现不佳。同时，业界很少关注高效的小型模型的潜力。作者希望通过多智能体方法提升各类模型（尤其是小模型）的Text-to-SQL表现。

Method: 提出三种多智能体LLM流程：(1) 多智能体讨论流程，多个智能体迭代批判和优化SQL查询，最终由裁判合成答案；(2) 规划-编码流程，规划模型制定逐步SQL生成方案，编码模型实现查询；(3) 编码-聚合流程，多个编码智能体独立生成SQL查询，由推理智能体选出最佳查询。作者采用Bird-Bench Mini-Dev数据集进行系统性性能评测，对不同规模的开源模型进行实验。

Result: 多智能体讨论流程可显著提升小型模型的执行精度，Qwen2.5-7b-Instruct在三轮讨论后精度提升10.6%。三种流程中，LLM Reasoner-Coder流程效果最佳，DeepSeek-R1-32B和QwQ-32B规划器将Gemma 3 27B IT的准确率从52.4%提升到56.4%的最高分。

Conclusion: 多智能体方法不仅有效提升大型模型的Text-to-SQL性能，对小型、高效模型同样有显著效果。最佳的Reasoner-Coder流程为小型开源模型带来突破性提升，展现了强大的实际应用潜力。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [43] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: 当前AI内容常陷入“扩充—压缩”的无效循环，LAAC模型通过结构化对话提升真实沟通，但在信任维度上仍存在短板，需进一步研究以支持重要任务场景。


<details>
  <summary>Details</summary>
Motivation: AI生成内容由来已久，造成了沟通的荒谬场景：内容创作者用大模型把简单想法扩充成冗长文本，内容接收者再用大模型压缩成摘要，结果双方都没有真正接触到真实、有价值的内容。

Method: 提出LAAC（LLM as a Communicator）新范式，让大模型作为智能沟通中介，结构化地捕捉发送者意图，促进知识交换。通过设计多智能体架构，并对LAAC应用于不同沟通场域进行系统性实验，包括信息捕获准确性、可复现性以及回应可靠性等信任维度。

Result: 实验显示，LAAC在多个实际沟通场景下存在信任缺口，包括意图准确捕获、知识复现一致性以及对用户查询的可靠回应方面，需进一步解决其在高风险沟通领域的部署难题。

Conclusion: LAAC作为沟通中介能够提升沟通的真实性与有效性，但其信任性（信息保真、重现性与回应完整性）尚未达到高风险场景的要求。未来需重点优化LAAC的信任机制，以实现广泛可靠应用。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [44] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文建立了可扩展的计算型图灵测试体系，验证了主流开源LLM在多平台用户文本模拟上的表现。结果显示，即使优化，LLM与人类文本差异依然显著，且提升“人类性”往往损害语义准确性，为后续模型校准及社会科学应用提供重要参考。


<details>
  <summary>Details</summary>
Motivation: 在社会科学中，越来越多研究利用大语言模型(LLMs)模拟人类行为，假设其能生成真实且类似人类的文本，但该假设尚未被系统验证。目前的验证方法主要依赖人类判断，且被认为不够可靠，因此亟需更稳健的检验工具和模型校准方法。

Method: 提出了计算型图灵测试，结合聚合指标（基于BERT的可检测性与语义相似度）以及可解释的语言特征（文体标记和话题模式），用以评估LLMs是否能在给定数据集内逼近人类语言。同时，对九种开源权重的大语言模型进行了对比，涵盖五种校准策略（包括微调、文体提示、情境检索等），并以用户在X（原Twitter）、Bluesky和Reddit上的交互为基准。

Result: 即使经过校准，LLM生成的文本在情感语调和情绪表达等方面依然与人类文本有明显区别。指令微调的模型表现不如基础模型，扩大模型规模也未提升“人类性”。优化“人类性”与保持语义一致性之间存在权衡。

Conclusion: 研究提出了可扩展的验证和校准框架，系统性检验了LLM模拟人类交流的现实局限性，并警示了其在捕捉人类沟通时仍受限。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [45] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 论文实证分析多款大语言模型在波兰公共采购资格考试中的表现，发现其仅能通过知识测试而难以完成复杂判决任务，评审结果也与官方标准不符。当前LLM尚无法取代人类法官，需进一步改进与跨界合作。


<details>
  <summary>Details</summary>
Motivation: 研究当前大语言模型（LLMs）是否能够通过波兰国家申诉委员会资格考试，探讨LLMs在公共采购法律应用中的实际能力。

Method: 在多个LLM（包括GPT-4.1、Claude 4 Sonnet和Bielik-11B-v2.6）间进行实验，包括闭卷环境与检索增强生成（Retrieval-Augmented Generation）设置，并采用‘LLM作为考生’及‘LLM作为评审’两种方式，检验其对考试中选择题和判决书写作的表现。

Result: 模型在知识测试部分表现较好，但在实务判决写作上均未达到及格线，‘LLM作为评审’的结果与官方委员会的判决存在明显差异。揭示LLM存在幻觉、法律条文引用错误、逻辑论证薄弱等关键局限。

Conclusion: 当前大语言模型尚无法胜任波兰公共采购领域的审判或独立考试任务，不能取代人类法官或考官，需法律专家与技术团队密切协作提升模型能力。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [46] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: 该文提出REMIND方法，通过分析模型对小输入变化的反应，更敏锐地检测模型是否真正遗忘了指定数据，弥补了现有评估方法的不足，具有强实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 机器反遗忘（Machine Unlearning）旨在无需完全重新训练模型的情况下，移除特定训练数据对模型的影响。这对于保障隐私、安全和合规至关重要。目前的评估方法多聚焦于单点输入，可能忽视与目标数据语义相近的数据残留影响，造成隐私泄露。

Method: 提出REMIND（Residual Memorization In Neighborhood Dynamics）方法，通过分析模型在输入微小变化下的损失分布，检测未被完全遗忘数据的影响，并据此分类数据是否被有效遗忘。其只需查询模型响应，不需内部访问，适用于多种模型和数据集。

Result: REMIND方法发现被遗忘数据的损失分布更平坦，而未遗忘或无关数据则有更陡峭的损失地形。REMIND在同等条件下优于现有方法，且对于不同模型、数据集及同义重述数据都具备稳健性和实用性。

Conclusion: REMIND为评估大语言模型的反遗忘效果提供了更敏感、更易解释的检测手段，为遗忘机制的研究和实际部署奠定基础，拓展了对记忆和反遗忘本质的理解。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [47] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 作者发现，通过检索增强和增加测试算力，可以显著提升大语言模型在多项任务上的表现，说明现有预训练方法尚未能充分利用数据集的信息，暗示未来可以通过更充分地挖掘和利用已有数据带来更大性能提升。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型主要通过大规模预训练数据学习，但对于预训练过程从数据中提取知识和信息的效率，研究还不够充分。作者希望评估预训练方法从现有数据集提取价值的有效性及其潜力。

Method: 作者使用基于检索增强生成（retrieval augmented generation）的方法，并探讨在测试阶段增加计算资源作为衡量预训练遗留数据价值的手段。他们在模型中引入检索模块，比较了仅预训练和“预训练+检索”在多个任务（如MMLU、Math-500、SimpleQA）上的性能。

Result: 在标准的、开源的数据集上，预训练后再进行检索带来任务表现的明显提升。这些提升经过去污染后依然存在。例如，在MMLU测试中，检索带来的效果相当于增加了约5倍的计算力。进一步，增加测试阶段的计算资源处理检索到的内容，可使LLaMA 3.1 8B的MMLU成绩提高10个百分点。

Conclusion: 当前的预训练技术并未从已有数据集中完全挖掘信息潜力，检索增强与测试时额外计算可大幅提升模型能力，表明尚有明显改进空间。

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [48] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 作者提出了一种基于图的主题标注方法，实现了计算高效、可解释性强的主题标签分配，结果优于传统方法，与ChatGPT-3.5持平。


<details>
  <summary>Details</summary>
Motivation: 现有主题抽取与标注方法计算资源消耗高，且主题建模产生的词集合可解释性差，因此作者希望在不依赖高算力复杂模型的前提下，实现更高效且可解释的主题标注。

Method: 提出了一种基于图的主题标注方法，通过构建主题词之间的关系图，丰富词汇且挖掘语义关系，并通过分析图结构来推断主题标签。方法进行了BERTScore与余弦相似度指标下的对比实验，使用两个数据集与ChatGPT-3.5和多项传统方法对比。

Result: 该方法在两个数据集上BERTScore与余弦相似度上均显著优于传统方法，并与ChatGPT-3.5表现接近，同时保持较高的计算效率。

Conclusion: 所提出的基于图的方法能够有效地为主题建模输出的无解释词组集合进行有意义的主题标注，其性能与ChatGPT-3.5相媲美，并且计算资源消耗较低，优于传统基准方法。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [49] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 文章为大型语言模型RLVR后训练提出SSPO算法，融合句子级重要性比率与熵调节PPO，显著提升数据利用与模型性能，超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 针对现有RLVR算法GRPO和GSPO在大模型后训练中的不足，例如GRPO存在策略更新不稳定、易受异常值影响导致崩溃，GSPO则导致采样数据利用率低。

Method: 提出SSPO，将重要性比率在句子级别进行计算，兼顾GRPO与GSPO优点，并利用句子熵动态调整PPO剪切边界，使高熵（探索度高）token扩展探索空间、低熵token收窄。

Result: SSPO在五个数据集上平均分达到46.57，优于GRPO（43.01）和GSPO（44.42），并在三个数据集上取得最新最优性能。

Conclusion: SSPO综合了GSPO的精髓而摒弃其缺陷，在大模型生成数据利用率和训练稳定性等方面表现优异，验证了句子级重要性比率和熵调节机制的有效性。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [50] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 本文提出一种用于机器翻译微调的数据选择方法，结合可学习性评分和批量选择策略，有效提升了数据与计算效率以及翻译质量，在多个实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 翻译模型性能高度依赖于数据质量与有效选择，这对于实现健壮可靠的翻译系统至关重要。作者针对提升微调过程中的数据选择问题，提出新的方法以优化训练效果。

Method: 方法结合学习模型和预训练参考模型，通过定义可学习性分数，对数据点训练效用进行系统化评估，仅筛选最相关与影响力大的样本用于微调，并采用批量选择策略，考虑数据点之间的相互依赖，从而优化训练效率同时保证数据相关性。

Result: 在英到波斯语及其他多个语对的实验中，基于mBART模型并微调CCMatrix数据集，方法相比iid基线达到至多5倍的数据效率提升。利用缓存嵌入，计算效率提高24倍，并且需要更少训练点，泛化能力增强，翻译效果优于随机选择。

Conclusion: 提出的数据选择方法显著提升了机器翻译微调过程中的数据与计算效率，也增强了模型泛化与翻译性能。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [51] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 实验发现，用英文提示能提升LLM在历史性冷知识问答中的表现，且模型规模越大效果越好。这为非英语语境下LLM应用提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 探究大模型在时间推理上的能力，尤其是在回答历史性问题时，提示语言（英文或挪威语）及模型规模的影响。

Method: 利用1940年出版的挪威语冷知识问答书，设计实验，让LLMs以“1940年视角”回答问题，并分别用英文和挪威语提示。答案通过LLM自动评分，并由母语者抽查验证。测试了DeepSeek-R1、Gemma3、Qwen3和Llama3.1等主流模型，以及专为挪威语设计的最大LLM。

Result: 英文提示下LLM表现优于挪威语提示。模型规模越大，表现提升越明显。专门为挪威语设计的大型LLM未能超越其他主流模型。

Conclusion: 用英文进行提示比用挪威语效果更好，出乎意料。同时更大型的LLM模型也能提升表现。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [52] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: PTTSD框架结合神经网络与概率建模，实现了对临床抑郁严重程度的准确可解释预测，且具备良好不确定性估计和时间动态分析性能，在权威数据集上取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有模型预测抑郁程度时缺乏不确定性估计和时间建模，影响临床应用的准确性与解读性。

Method: 提出PTTSD：一种概率型文本时间序列抑郁检测框架，结合双向LSTM、自注意力、残差连接和高斯/Student-t输出。通过负对数似然进行训练，包含序列到序列与序列到单一的两种模式。

Result: 在E-DAIC和DAIC-WOZ数据集上实现了目前文本预测最佳表现（MAE分别为3.85和3.55），预测区间良好校准，消融实验证实注意力和概率建模的价值，与MentalBERT对比显示泛化性强。

Conclusion: PTTSD框架能准确并具备不确定性估计地预测抑郁严重程度，对临床决策有重要意义，其解读性强且泛化能力良好。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [53] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: ThaiOCRBench填补泰语视觉-语言模型评测空白，专有模型显著优于开源。数据集标准化、分析详尽，为提升泰语相关模型表现提供支撑。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）评测主要集中在高资源语言，对于泰语这种低资源语言尤其是在需要理解文档结构的任务上，缺乏完善的评测基准。针对这一空白，论文提出补充性基准。

Method: 构建并公布了ThaiOCRBench基准集，包含13类任务共2808个人工标注样本，并对多种最先进视觉-语言模型在零样本条件下进行系统评测，包括专有和开源模型。

Result: 专有模型（如Gemini 2.5 Pro）在各项任务上明显优于开源模型。细粒度文本识别与手写内容提取是开源模型性能下降最显著的部分。论文通过细致的错误分析，发现如语言偏差、结构不匹配、内容臆造等核心挑战。

Conclusion: ThaiOCRBench为低资源且书写系统复杂的语言（如泰语）提供了统一的模型评测框架和详实的数据集，并为后续提升泰语文档理解性能提供了数据和方法建议。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [54] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 本文针对现有基准无法全面评估大语言模型处理复杂真实表格的能力，提出了RUST-BENCH基准。该基准涵盖科学与体育两个领域的大量多样化表格和问题，实验结果揭示现有模型在表格推理上的持续弱点。RUST-BENCH为推进表格推理研究提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 现有的表格推理基准主要测试模型在小而统一的表格上，无法体现实际数据的复杂性，也不能全面评估大语言模型（LLMs）的推理能力。现实中的表格数据多样、冗长且领域专属，同时包含结构化字段和自由文本，推理过程往往需要多步并穿越海量内容。为了解决这一不足，作者设立了新的基准。

Method: 提出RUST-BENCH，一个全新基准，包括7966个问题，涵盖2031个真实世界表格，横跨两个领域：科学（NSF资助记录）与体育（NBA统计）。该基准支持大规模、异构、领域专属及复杂推理等特征的联合评估。作者基于开放源码和专有模型进行了实验。

Result: 实验结果显示，大语言模型在异构表结构和复杂多步推理方面存在明显困难，当前架构及策略仍有显著不足。

Conclusion: RUST-BENCH为表格推理领域建立了极具挑战性的新测试基准，有助于推动相关方法的进步和研究方向的探索。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [55] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 该论文针对文本可读性受控简化，分析了CEFR等级差距对简化效果的影响，并基于GPT-4o提出两种多轮简化方法。实验证明系统性能较优，后续优化显示联合规则与LLM方法进一步增强了简化效果。


<details>
  <summary>Details</summary>
Motivation: 通过分析基于Prompt的文本简化方法，发现文本简化效果与原文及目标CEFR水平之间的差距高度相关，因此激发了多轮简化方案的设计。

Method: 提出了两种基于GPT-4o的多轮简化方法：规则驱动简化（MRS-Rule）和联合规则驱动LLM简化（MRS-Joint），并将LLM生成的候选文本用作后续简化的起点。

Result: 提交的系统在20支参赛队中排名第7，随后通过以LLM简化文本为起点，MRS-Joint进一步提升了简化性能。

Conclusion: 多轮简化策略，特别是将LLM简化候选作为基础，能显著提升文本简化系统的表现。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [56] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本研究系统地利用大五人格量表评估了六种主流LLM的人格特质，揭示模型间在多个人格维度上的显著差异，证明温度设置和模型架构会影响AI表现出的人格特质，为AI系统的选择、调优和伦理治理提出了新见解。数据与代码全部开放。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在以人为中心的应用中的重要性提升，了解其类似人格的行为对于负责任的发展和部署变得愈发重要。

Method: 系统性评估了六种LLM，采用“大五人格量表-2（BFI-2）”框架，在不同采样温度下，对其人格特质表达进行测量和对比，并运用层级聚类方法揭示模型间的差异与共性。

Result: 在五个主要人格维度中，发现有四个维度存在显著差异，神经质和外向性受到温度调整影响较大。模型之间可以分为不同的特征集群，表明模型架构会影响其人格特征表达的稳定性。

Conclusion: LLM具备明显且可调节的人格特征表达，这为模型调优、选择及AI伦理治理提供了新视角。论文同时开放了相关数据与代码，促进后续研究。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [57] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 本文提出RAGalyst，一种高效自动化且与人类判断一致的RAG系统评测框架，能针对细分领域评估RAG方法优劣，结论显示领域和上下文对系统表现影响极大。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统评测依赖启发式或未与人类对齐的LLM评判方法，无法准确反映专用、关键领域的实际表现，因此需要合理、系统且能与人工标注高度一致的新型评测框架。

Method: 提出了一个自动化、与人类评价对齐的agentic评测框架RAGalyst，包括自动生成高质量QA数据集、agentic过滤确保数据质量，以及针对两大核心指标（答案正确性、可回答性）的提示优化。并在军事、网络安全和桥梁工程三大领域进行实证测试。

Result: RAGalyst实现了自动化与人类评价高度一致的RAG评测流程，揭示不同领域下RAG系统表现强烈依赖上下文，没有通用最优模型或参数设计，并分析了影响正确率的主要原因。

Conclusion: RAGalyst可以有效评估不同领域RAG系统的性能，没有单一模型或配置能适用于所有应用场景，体现评估框架对实际开发的重要性。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [58] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 提出针对影像报告显式与隐式不确定性的量化和拓展方法，并发布不确定性感知的结构化报告数据集Lunguage++，推动自动化分析和临床应用。


<details>
  <summary>Details</summary>
Motivation: 医学影像报告对临床决策至关重要，但其非结构化特性和不确定性阻碍自动化分析和量化。报告常含有两类不确定性：显式不确定性（通过模糊词表达，对传统规则系统挑战大）；隐式不确定性（遗漏部分推理，仅记录关键发现，导致信息缺失）。

Method: 提出了两部分框架：1）构建基于专家验证的大模型参考，将常用模糊词按不确定性排名，并映射发现至概率值以量化显式不确定性；2）通过扩展框架，将专家定义的14种常见诊断路径的典型子发现系统性补充至报告，以建模隐式不确定性。

Result: 生成并发布了Lunguage++，即Lunguage基准的扩展版，报告结构更细致且具备不确定性标签。该资源支持不确定性感知的影像分类、可靠的诊断推理，以及探究诊断不确定性的临床影响。

Conclusion: 整合专有大模型和诊断流程知识，显著提升了结构化影像报告对不确定性量化与表达的能力，为自动化医学影像分析和临床决策提供了更可靠的信息基础。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [59] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 通过研究语言模型在推理过程中的隐藏激活，作者发现模型的不确定性与推理路径可控性相关，激活可以用来预测未来输出，证明了模型在生成文本时内隐存在多条可能推理路径。


<details>
  <summary>Details</summary>
Motivation: 语言模型在生成文本时，每个单独的词元选择都可能导致不同的推理路径，这使得量化模型的不确定性变得困难。作者希望研究推理型语言模型在生成过程中是否隐式地表示可选的推理路径。

Method: 通过操控和预测语言模型在链式思维推理过程中的隐藏激活（hidden activations），分析其对模型不确定性的影响。具体实验包括探测不同词元下的不确定性表现，以及利用激活干预来引导模型推理路径。

Result: 实验发现，模型在不同词元上的不确定性与能否通过控制其激活进行路径引导有明显相关性。激活干预在模型尚未决定最终答案、有可选路径时效果最佳。

Conclusion: 隐藏激活不仅能预测模型未来的输出分布，还证明了模型在生成过程中隐式地表示着多种可能的推理路径。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [60] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof用LLM自动分析和评分论证性作文，通过可视化论证图和详细解释提升用户理解和体验，实现高效、结构化、可交互的作文评估。


<details>
  <summary>Details</summary>
Motivation: 目前自动化作文评分系统多关注评分本身，较少注重用户理解和人机交互。结构化、量化分析作文的论证性和连贯性，在实际教学场景中有巨大价值。

Method: 提出一种交互系统IntelliProof，将作文结构为论证图：主张为节点，证据为属性，边为支持或反对关系。每一关系由LLM自动判定并打分，通过可视化帮助用户理解，系统还提供分类理由和量化连贯性指标。

Result: 实现了IntelliProof系统，支持对论证性作文的结构化分析和交互式展示，既便于快速评估又保留人工监督。系统桥接结构语义与自然语言理解，提升用户体验。并提供线上演示。

Conclusion: IntelliProof有效提升了论证性作文分析的交互性和可解释性，将自动判定与用户理解结合，突破了传统自动作文评分系统的局限。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [61] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: LLM编码助手仍容易生成安全漏洞，现有修补措施效果有限。论文引入PE、ME指标评估模型风险，有助于促进更有效的安全优化。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型（LLM）的代码助手在软件开发中变得愈发重要，这些模型生成的漏洞也越来越影响整个网络安全格局。目前虽然已有LLM代码安全基准和提升代码生成安全性的相关方法，但这些机制对于主流编码LLM产生的具体影响仍不清楚。

Method: 作者通过在真实使用场景下测试最新的开源LLM，验证其在已报道的最早期的漏洞场景下的表现，并提出了一种衡量LLM生成漏洞风险的新指标——Prompt Exposure（PE）。PE综合考虑了漏洞严重性、生成概率以及导致漏洞生成的提示词设计。同时引入Model Exposure（ME）分数，用于表征模型生成漏洞的严重性与普遍性。

Result: 实验证明，即使是最新的开放权重模型，在早期已知漏洞场景下仍然容易受到攻击，说明安全与功能性的权衡阻碍了有效的漏洞修补。

Conclusion: 目前LLM编码助手普遍存在安全缺陷，仅靠现有安全提升手段尚难覆盖全部风险。提出PE和ME指标有助于量化模型暴露的安全风险，并推动社区关注和缓解最严重和高频的漏洞问题。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [62] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 本文针对孟加拉语医学问答的低资源问题，构建了大规模选择题数据集，提出并评测多种RAG策略，Agentic RAG方案准确率最高，显著提升了问答质量，为多语种医学AI探索提供了新基础。


<details>
  <summary>Details</summary>
Motivation: 发展低资源语言的生物医学问答系统仍面临重大挑战，影响了医疗知识的公平获取。针对孟加拉语领域缺乏相关数据集和技术，亟需提升医疗问答的准确性与可用性。

Method: 本文创建了孟加拉语生物医学选择题数据集BanglaMedQA和BanglaMMedBench，设计并评测了多种检索增强生成（RAG）策略，包括传统RAG、零样本回退、Agentic、迭代反馈及聚合RAG，结合教材和网络检索与生成推理提升事实准确率，并通过OCR集成医学教材语料，开发了动态选择检索与推理策略的Agentic RAG流程。

Result: Agentic RAG与openai/gpt-oss-120b结合实现了89.54%的最高准确率，在所有配置中表现最佳，并显著提升了推理质量。

Conclusion: 基于RAG的方法可有效提升孟加拉语医学问答的可靠性和可获取性，为多语种医学AI研究奠定了基础。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [63] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: DeReC是一种面向事实核查的轻量级密集检索分类框架，能以更低计算资源实现比大型语言模型更高的精度和效率，适合实际场景应用。


<details>
  <summary>Details</summary>
Motivation: 当前全球错误信息传播严重，需要高效且计算资源友好的事实核查系统。现有方法主要依赖大型语言模型（LLMs）生成解释性推理，但在实际应用中面临计算成本高和虚假信息风险。

Method: 提出了DeReC（Dense Retrieval Classification）框架，利用通用文本嵌入替代自回归式LLM方法，通过密集检索与专项分类相结合实现事实核查。

Result: DeReC在效率和精度上均优于生成解释的LLM方法：在RAWFC和LIAR-RAW数据集上分别减少95%和92%的运行时间；在RAWFC数据集上F1得分65.58%，优于现有最佳L-Defense方法（61.20%）。

Conclusion: 精心设计的基于检索的方法可以在特定任务中达到甚至超过LLM性能，同时大幅提升实际部署的可用性。

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [64] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: 本文提出LEASH算法，通过动态监控推理状态，自适应中止推理链生成，无需额外训练，可显著减少token消耗和延迟，但精度略有下降，适用于需要高效生成的场景。


<details>
  <summary>Details</summary>
Motivation: 目前CoT推理方法在大语言模型中能有效提升复杂推理能力，但生成完整的固定长度推理链非常耗费计算，导致token使用过多和延迟增加。

Method: 提出LEASH（Logit-Entropy Adaptive Stopping Heuristic）算法，通过监控token级熵变化的斜率和顶部logit边际的提升情况，自适应地决定何时停止推理链生成。该方法无需额外训练和监督，属于一种训练无关的解码算法。

Result: 在GSM8K和AQuA-RAT基准测试上，LEASH可减少30-35%的平均token生成数和27%的延迟，但相较普通CoT推理有约10个百分点的准确率下降。

Conclusion: LEASH是一种高效、简单、模型无关的推理链生成终止方法，能显著降低资源消耗和延迟，为CoT推理提供了有效替代，尽管会有一定准确率损失。

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [65] [Induced matching treewidth and tree-independence number, revisited](https://arxiv.org/abs/2511.03864)
*Noga Alon,Martin Milanič,Paweł Rzążewski*

Main category: cs.DM

TL;DR: 树独立数和诱导匹配树宽是两种通过树分解定义的图参数，在一般情况下关系松散。但本文发现，在排除固定二分团$K_{t,t}$的图中，这两个参数之间可以用多项式关系紧密联系起来，而不是之前复杂的指数上界。


<details>
  <summary>Details</summary>
Motivation: 此前的研究表明，对于$K_{t,t}$-free图，可以将树独立数用诱导匹配树宽上界，但这个上界是指数级的。为了解决这一松散界限，作者尝试找到更紧的、多项式关系，从而使这些参数更具实际应用价值。

Method: 论文通过利用Kövári–Sós–Turán定理分析$K_{t,t}$-free图类，并对树独立数和诱导匹配树宽的关系进行严格估计。

Result: 作者证明在$K_{t,t}$-free图类中，树独立数不超过诱导匹配树宽的某个多项式函数，显著收紧了该参数之间的关系。

Conclusion: 对于不包含特定二分团（即$K_{t,t}$-free）的图类，树独立数与诱导匹配树宽之间实际上存在线性多项式关系，而非之前的指数关系。

Abstract: We study two graph parameters defined via tree decompositions:
tree-independence number and induced matching treewidth. Both parameters are
defined similarly as treewidth, but with respect to different measures of a
tree decomposition $\mathcal{T}$ of a graph $G$: for tree-independence number,
the measure is the maximum size of an independent set in $G$ included in some
bag of $\mathcal{T}$, while for the induced matching treewidth, the measure is
the maximum size of an induced matching in $G$ such that some bag of
$\mathcal{T}$ contains at least one endpoint of every edge of the matching.
  While the induced matching treewidth of any graph is bounded from above by
its tree-independence number, the family of complete bipartite graphs shows
that small induced matching treewidth does not imply small tree-independence
number. On the other hand, Abrishami, Bria\'nski, Czy\.zewska, McCarty,
Milani\v{c}, Rz\k{a}\.zewski, and Walczak~[SIAM Journal on Discrete
Mathematics, 2025] showed that, if a fixed biclique $K_{t,t}$ is excluded as an
induced subgraph, then the tree-independence number is bounded from above by
some function of the induced matching treewidth. The function resulting from
their proof is exponential even for fixed $t$, as it relies on multiple
applications of Ramsey's theorem. In this note we show, using the
K\"ov\'ari-S\'os-Tur\'an theorem, that for any class of $K_{t,t}$-free graphs,
the two parameters are in fact polynomially related.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [66] [State Complexity of Multiple Concatenation](https://arxiv.org/abs/2511.03814)
*Jozef Jirásek,Galina Jirásková*

Main category: cs.FL

TL;DR: 该论文通过构造新见证语言和简化证明，优化了多重连接正规语言状态复杂度的相关结果，部分情况下字母表规模从$k+1$降到$k$，同时解决了一个公开问题，并首次获得了一元语言的紧确上界。


<details>
  <summary>Details</summary>
Motivation: 简化关于正规语言多重连接状态复杂度上界见证的证明，优化字母表规模，并解决现有研究中的部分公开问题。

Method: 通过构造见证语言和简化证明，讨论了不同字母表规模下多重连接的状态复杂度，并解决了文献中的一个公开问题。

Result: 构造了新的见证语言，简化了证明过程，在保证最优状态复杂度的情况下将字母表需求从$k+1$降至$k$（部分情形），并首次解决了相关的公开问题，还对一元语言的状态复杂度给出了紧确上界。

Conclusion: 证明了在多重连接三个语言时，三元字母表是最优的，并获得了对一元环语言以及一元自动机（尾部无终态）语言的状态复杂度紧确上界。

Abstract: We describe witness languages meeting the upper bound on the state complexity
of the multiple concatenation of $k$ regular languages over an alphabet of size
$k+1$ with a significantly simpler proof than that in the literature. We also
consider the case where some languages may be recognized by two-state automata.
Then we show that one symbol can be saved, and we define witnesses for the
multiple concatenation of $k$ languages over a $k$-letter alphabet. This solves
an open problem stated by Caron et al. [2018, Fundam. Inform. 160, 255--279].
We prove that for the concatenation of three languages, the ternary alphabet is
optimal. We also show that a trivial upper bound on the state complexity of
multiple concatenation is asymptotically tight for ternary languages, and that
a lower bound remains exponential in the binary case. Finally, we obtain a
tight upper bound for unary cyclic languages and languages recognized by unary
automata that do not have final states in their tails.

</details>


### [67] [Explorability in Pushdown Automata](https://arxiv.org/abs/2511.04048)
*Ayaan Bedi,Karoliina Lehtinen*

Main category: cs.FL

TL;DR: 提出、刻画并层次化分析下推自动机的可探索性，作为精细度高且有理论意义的非确定性度量，揭示其表达能力、与上下文无关语言关系及极大简洁性优势。


<details>
  <summary>Details</summary>
Motivation: 现有的下推自动机形式中，历史确定性和完全非确定性两端之间缺乏精细刻画非确定性度量的理论工具，因此需要提出介于两者之间的度量方法。

Method: 定义k-可探索自动机（k-explorable），分析其表达能力与简洁性，建立不同可探索性水平之间的严格层级，并推广到参数化（输入长度相关）情形。同时，证明其与上下文无关语言等价性，以及与历史确定性和2-可探索自动机的表达与简洁性差距。

Result: 证明了k-可探索下推自动机系列严格插入在历史确定与完全非确定自动机之间，层级无限递增；参数化可探索性精确刻画了上下文无关语言；可探索自动机在描述简洁性方面可比历史确定自动机高出双指数，2-可探索自动机与确定性自动机简洁性差异不可递归枚举。

Conclusion: 可探索性为下推自动机提供了介于历史确定性与完全非确定性之间的新的非确定性度量，并且形成了严格的表达能力和简洁性层级。其参数化版本正好刻画了上下文无关语言，因此是一个操作性强且有意义的理论工具。

Abstract: We study explorability, a measure of nondeterminism in pushdown automata,
which generalises history-determinism. An automaton is k-explorable if, while
reading the input, it suffices to follow k concurrent runs, built step-by-step
based only on the input seen so far, to construct an accepting one, if it
exists. We show that the class of explorable PDAs lies strictly between
history-deterministic and fully nondeterministic PDAs in terms of both
expressiveness and succinctness. In fact increasing explorability induces an
infinite hierarchy: each level k defines a strictly more expressive class than
level k-1, yet the entire class remains less expressive than general
nondeterministic PDAs. We then introduce a parameterized notion of
explorability, where the number of runs may depend on input length, and show
that exponential explorability precisely captures the context-free languages.
Finally, we prove that explorable PDAs can be doubly exponentially more
succinct than history-deterministic ones, and that the succinctness gap between
deterministic and 2-explorable PDAs is not recursively enumerable. These
results position explorability as a robust and operationally meaningful measure
of nondeterminism for pushdown systems.

</details>
